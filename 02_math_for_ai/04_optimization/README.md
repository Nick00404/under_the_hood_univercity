# 04 Optimization

- [01 intro to optimization](./01_intro_to_optimization.ipynb)
- [02 gradient descent](./02_gradient_descent.ipynb)
- [03 stochastic gradient descent](./03_stochastic_gradient_descent.ipynb)
- [04 convexity and minima](./04_convexity_and_minima.ipynb)
- [05 momentum rmsprop adam](./05_momentum_rmsprop_adam.ipynb)
- [06 loss functions in depth](./06_loss_functions_in_depth.ipynb)

---

## ðŸ“š Optimization

1. **Intro to Optimization**
   - **What is Optimization?**: Definition and importance in machine learning.
   - **Optimization in ML**: Objective functions, decision variables, and constraints.
   - **Types of Optimization**: Unconstrained vs. constrained optimization.
   - **Global vs Local Minima**: Understanding global and local optimal solutions.

---

2. **Gradient Descent**
   - **Gradient Descent Overview**: The basic principle of gradient descent as an optimization algorithm.
   - **Derivatives and Gradients**: How gradients guide the optimization process.
   - **Gradient Descent in ML**: Minimizing a cost function, step sizes (learning rate).
   - **Convergence**: How and when gradient descent converges to a local minimum.
   - **Learning Rate**: The role of step size in gradient descent.

---

3. **Stochastic Gradient Descent (SGD)**
   - **Stochastic vs Batch Gradient Descent**: Difference between batch and stochastic gradient descent.
   - **SGD Process**: Iterative update of parameters with a subset of data.
   - **Advantages of SGD**: Faster convergence, lower computational cost.
   - **Challenges of SGD**: High variance, noise in updates.
   - **Mini-batch Gradient Descent**: Combines the benefits of batch and stochastic approaches.

---

4. **Convexity and Minima**
   - **Convex Functions**: Characteristics of convex functions and their importance in optimization.
   - **Convex vs Non-Convex Optimization**: Differences, challenges, and implications for ML.
   - **Types of Minima**: Local minima, global minima, saddle points.
   - **Convex Optimization**: Why convex functions are easier to optimize and their role in ML algorithms.
   - **Non-convex Optimization**: Handling non-convexity in deep learning and neural networks.

---

5. **Momentum, RMSprop, Adam**
   - **Momentum**: Accelerating gradient descent using a memory of past gradients.
   - **RMSprop**: A method for adjusting the learning rate based on recent gradient magnitudes.
   - **Adam Optimizer**: Combination of momentum and RMSprop for adaptive learning rates.
   - **Comparing Optimizers**: Strengths and weaknesses of momentum, RMSprop, and Adam.
   - **Choosing the Right Optimizer**: How to select optimizers based on your model and dataset.

---

6. **Loss Functions in Depth**
   - **What is a Loss Function?**: Definition and importance in ML models.
   - **Types of Loss Functions**:
     - **Mean Squared Error (MSE)**: Commonly used in regression tasks.
     - **Cross-Entropy Loss**: Common for classification tasks.
     - **Hinge Loss**: Used in support vector machines (SVM).
     - **Huber Loss**: A combination of squared loss and absolute loss.
   - **Choosing the Right Loss Function**: How to choose the right loss function based on the task.
   - **Loss Functionâ€™s Role in Optimization**: How different loss functions affect the learning process.

---
