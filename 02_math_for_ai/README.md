# 📘 Math for AI: The Foundations of Machine Learning

Welcome to the **Math for AI** module of **UTHU** — Under The Hood University.  
This book grounds you in the essential mathematical tools every ML/DL/LLM system is built upon.

📌 **Why this matters:**  
Forget memorizing formulas. This is about building an **intuition-driven mathematical toolkit**  
that lets you design models, debug training, and understand what's *really* happening under the hood.

---

## 🧱 Structure of This Book

This book is split into **4 major subjects** — each its own mini-book.  
You’ll move from **algebra to calculus to optimization** in a story-like arc.

### 1️⃣ Linear Algebra

> _Vectors are directions. Matrices are machines.  
You’re learning how data moves and models transform it._

| Notebook File | Topic |
|---------------|-------|
| `01_vectors_and_scalars.ipynb` | Intro to vectors, scalar ops, visual demos |
| `02_matrix_operations.ipynb` | Matrix multiplication, identity, inverse |
| `03_dot_and_cross_products.ipynb` | Geometry of inner/outer products |
| `04_identity_inverse_determinant.ipynb` | Matrix algebra deep dive |
| `05_linear_transformations.ipynb` | Data warping, projections, PCA |
| `06_svd_and_eigendecomposition.ipynb` | Dimensionality reduction, spectral insights |

---

### 2️⃣ Statistics & Probability

> _Distributions, uncertainty, and probabilistic thinking —  
so your models can understand the world, not just memorize it._

| Notebook File | Topic |
|---------------|-------|
| `01_descriptive_statistics.ipynb` | Mean, median, std, histograms |
| `02_probability_basics.ipynb` | Independence, conditional prob |
| `03_distributions.ipynb` | Normal, binomial, Poisson & friends |
| `04_bayes_theorem.ipynb` | Updating beliefs with evidence |
| `05_statistical_tests.ipynb` | T-tests, p-values, null hypothesis |
| `06_sampling_and_bootstrapping.ipynb` | Estimating from small samples |

---

### 3️⃣ Calculus

> _You’re not learning calculus. You’re learning to see gradients,  
loss surfaces, and how models change as they learn._

| Notebook File | Topic |
|---------------|-------|
| `01_limits_and_continuity.ipynb` | Core concepts + intuition |
| `02_derivatives_rules.ipynb` | Product, quotient, chain rules |
| `03_chain_product_rule.ipynb` | Backprop’s best friend |
| `04_partial_derivatives.ipynb` | Multivariable functions |
| `05_integrals_basics.ipynb` | Area under the curve |
| `06_calculus_in_ml.ipynb` | Gradient descent, softmax, activation curves |

---

### 4️⃣ Optimization

> _The math of learning. From “how do we minimize loss?” to  
“why does Adam sometimes explode?”_

| Notebook File | Topic |
|---------------|-------|
| `01_intro_to_optimization.ipynb` | Problem setup, convexity |
| `02_gradient_descent.ipynb` | Derivatives in action |
| `03_stochastic_gradient_descent.ipynb` | Online learning setups |
| `04_convexity_and_minima.ipynb` | Visualizing saddle points and traps |
| `05_momentum_rmsprop_adam.ipynb` | Optimizer zoo: tradeoffs explained |
| `06_loss_functions_in_depth.ipynb` | MSE, Cross-Entropy, Hinge, Contrastive |

---

## 🧪 Bonus Labs (Coming Soon)

> 🔬 Intuition Builders — interactive widgets, 3D plots, demos.
- PCA visualizer
- Gradient descent on real surfaces
- Central Limit Theorem simulator

---

## 📘 Prereqs

- Python basics
- Basic plots with `matplotlib`
- Motivation to understand what every AI paper takes for granted

---

## ✅ Outcome

By the end of this track, you'll be able to:
- Understand ML papers without skipping the math section
- Derive gradients by hand and debug backprop
- Design new loss functions for your own custom tasks
- Visually explain eigenvectors, gradient flow, and sampling

---

> ✨ Welcome to UTHU. Math is not scary — it’s your co-pilot.