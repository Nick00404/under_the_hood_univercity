{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c74dcc",
   "metadata": {},
   "source": [
    "🔥 Locked in. Time to peek inside the mind of a transformer.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `09_lab_attention_visualization.ipynb`  \n",
    "### 📁 `03_natural_language_processing`  \n",
    "> Visualize **self-attention heads** in real transformer models like BERT or GPT-2.  \n",
    "See which words attend to which — layer-by-layer, head-by-head.  \n",
    "**Intuition meets interpretability** in this lab.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Goals\n",
    "\n",
    "- Understand **self-attention weights** as visual heatmaps  \n",
    "- Use tools like **`bertviz`** to see where attention flows  \n",
    "- Compare **different heads/layers**  \n",
    "- Analyze attention patterns: positional, syntactic, semantic\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Runtime Design\n",
    "\n",
    "| Feature          | Spec             |\n",
    "|------------------|------------------|\n",
    "| Platform         | ✅ Colab (recommended)  \n",
    "| Model            | ✅ `bert-base-uncased` (or `gpt2`)  \n",
    "| Tooling          | ✅ `bertviz` or `transformer-vis`  \n",
    "| Hardware         | ✅ CPU/GPU (minimal VRAM)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Section 1: Install & Import\n",
    "\n",
    "```python\n",
    "!pip install transformers bertviz\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from bertviz import head_view, model_view\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 Section 2: Load Model & Tokenizer\n",
    "\n",
    "```python\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "model = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Section 3: Prepare Input Sentence\n",
    "\n",
    "```python\n",
    "sentence = \"The cat sat on the mat and looked at the dog.\"\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention = model(**inputs).attentions  # Tuple of layers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔬 Section 4: Visualize Attention — Model View\n",
    "\n",
    "```python\n",
    "# Full attention structure across layers and heads\n",
    "model_view(attention, tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "```\n",
    "\n",
    "> 🧠 Shows attention across **all layers + heads**  \n",
    "> Hover over tokens to see which ones they focus on  \n",
    "> Great for analyzing **which layers attend to what**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Section 5: Visualize Specific Head — Head View\n",
    "\n",
    "```python\n",
    "head_view(attention, tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "```\n",
    "\n",
    "> 🔍 Lets you isolate specific **layer/head pairs**  \n",
    "> See if early layers attend **positionally**, while later layers attend **semantically**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Optional: Use GPT-2 Instead\n",
    "\n",
    "```python\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "gpt2 = GPT2Model.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "sentence = \"Transformers are changing the world.\"\n",
    "\n",
    "gpt2_inputs = gpt2_tokenizer(sentence, return_tensors='pt')\n",
    "attn = gpt2(**gpt2_inputs).attentions\n",
    "\n",
    "head_view(attn, gpt2_tokenizer.convert_ids_to_tokens(gpt2_inputs['input_ids'][0]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Section 6: What to Look For\n",
    "\n",
    "| Layer | Pattern            | Meaning |\n",
    "|-------|--------------------|---------|\n",
    "| 0–3   | Positional          | Looks left/right like positional encodings  \n",
    "| 4–8   | Phrase-based        | Attention spans across phrase boundaries  \n",
    "| 9–12  | Semantic & summary  | Focuses on nouns, verbs, sentence ends  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Wrap-Up\n",
    "\n",
    "| What You Did             | ✅ |\n",
    "|--------------------------|----|\n",
    "| Visualized self-attention| ✅ |\n",
    "| Understood head structure| ✅ |\n",
    "| Interpreted patterns     | ✅ |\n",
    "| Used BERT and GPT2       | ✅ |\n",
    "| Colab safe               | ✅ |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- **Attention = context mapping**  \n",
    "- Each head has its own **linguistic role**  \n",
    "- Transformers **don’t see linearly** — they jump, link, and focus dynamically  \n",
    "- You can now **debug models visually**, not just by numbers\n",
    "\n",
    "---\n",
    "\n",
    "✅ That's the final NLP lab in this batch.\n",
    "\n",
    "Next up: `04_advanced_architectures` →  \n",
    "Want to move to `07_lab_gnn_node_classification_with_cora.ipynb` and get graphy with it?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
