{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e528920",
   "metadata": {},
   "source": [
    "📚 Let’s crack open the next NLP lab and put GPT-2 on a leash of your own words.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `07_lab_finetuning_gpt2_text_generation.ipynb`  \n",
    "### 📁 `03_natural_language_processing`  \n",
    "> Fine-tune **GPT-2** on **custom text** (movie quotes, poetry, code, anything).  \n",
    "Compare pretraining vs post-finetuning generations.  \n",
    "🚀 Lightweight. **Colab-ready. Laptop-tolerant. Fun guaranteed.**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Goals\n",
    "\n",
    "- Understand how to fine-tune GPT-2 using Hugging Face  \n",
    "- Train on your own dataset (tiny corpus = fast iterations)  \n",
    "- See **how generation style changes** post-tuning  \n",
    "- Learn to **use Trainer API + Tokenizer + Dataset**\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Hardware Target\n",
    "\n",
    "| Spec           | Design Target     |\n",
    "|----------------|------------------|\n",
    "| Device         | ✅ Colab / T4 / Laptop CPU  \n",
    "| Dataset Size   | ✅ <10k lines text (e.g., quotes, poems)  \n",
    "| Model          | ✅ `distilgpt2` (lighter than full GPT-2)  \n",
    "| Epochs         | 🔁 3–5 max  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Section 1: Install & Import\n",
    "\n",
    "```python\n",
    "!pip install transformers datasets\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Section 2: Load or Create Dataset\n",
    "\n",
    "```python\n",
    "# 👇 Paste your custom corpus here\n",
    "sample_text = \"\"\"\n",
    "There is a light that never goes out.\n",
    "To infinity and beyond.\n",
    "The cake is a lie.\n",
    "All your base are belong to us.\n",
    "Live long and prosper.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"my_corpus.txt\", \"w\") as f:\n",
    "    f.write(sample_text * 100)  # repeat to enlarge\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Section 3: Tokenizer + Dataset Setup\n",
    "\n",
    "```python\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"my_corpus.txt\",\n",
    "    block_size=64\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 Section 4: Load Model\n",
    "\n",
    "```python\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🏃‍♀️ Section 5: Define Trainer\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Section 6: Fine-Tune GPT-2\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Section 7: Compare Text Generation (Before/After)\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(\"\\n📦 Post-Finetune Sample:\")\n",
    "print(gen(\"To infinity\", max_length=30, num_return_sequences=1)[0]['generated_text'])\n",
    "\n",
    "print(\"\\n📦 Raw GPT2 Sample:\")\n",
    "gpt2_base = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "raw_gen = pipeline(\"text-generation\", model=gpt2_base, tokenizer=tokenizer)\n",
    "\n",
    "print(raw_gen(\"To infinity\", max_length=30, num_return_sequences=1)[0]['generated_text'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Wrap-Up: Lab Outcome\n",
    "\n",
    "| What You Did              | ✅ |\n",
    "|---------------------------|----|\n",
    "| Custom corpus prep        | ✅ |\n",
    "| GPT-2 tokenizer usage     | ✅ |\n",
    "| Dataset / DataCollator    | ✅ |\n",
    "| Hugging Face Trainer API  | ✅ |\n",
    "| Side-by-side generation   | ✅ |\n",
    "| Small-scale, fast run     | ✅ |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- Fine-tuning = steering style, not changing core logic  \n",
    "- Even small datasets **alter tone and phrasing**  \n",
    "- You now understand Hugging Face training loop 💪  \n",
    "- **Prompt + tokenized text + LM = controllable generation**\n",
    "\n",
    "---\n",
    "\n",
    "Ready to build `08_lab_masked_language_modeling_from_scratch.ipynb` next — a baby BERT pretraining pipeline on your own dataset?  \n",
    "Let’s train a toy MLM from zero!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
