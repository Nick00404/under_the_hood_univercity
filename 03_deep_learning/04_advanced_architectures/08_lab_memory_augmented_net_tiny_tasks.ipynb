{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4f6c89b",
   "metadata": {},
   "source": [
    "👁️ Memory time. Let's build a brain that **remembers**, not just computes.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `08_lab_memory_augmented_net_tiny_tasks.ipynb`  \n",
    "### 📁 `04_advanced_architectures`  \n",
    "> Implement a **tiny Neural Turing Machine (NTM)** or **Memory-Augmented Neural Network** to solve classic tasks like **copy**, **repeat**, or **pattern recall**.  \n",
    "Understand how **external memory + attention** creates **differentiable memory systems**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Goals\n",
    "\n",
    "- Build a **differentiable memory controller**  \n",
    "- Learn how NTMs solve **copy/repeat tasks**  \n",
    "- Train on **simple toy tasks** (low memory/GPU needs)  \n",
    "- Visualize **read/write attention over time**\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Runtime Design\n",
    "\n",
    "| Spec                | Setting             |\n",
    "|---------------------|---------------------|\n",
    "| Dataset             | Synthetic copy task  \n",
    "| Device              | ✅ Colab / CPU / GPU  \n",
    "| Memory              | ✅ <1GB  \n",
    "| Duration            | 🏃 Fast (1–2 mins)  \n",
    "| Architecture        | RNN + External Mem  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Section 1: Setup & Imports\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Section 2: Create Toy Copy Task Dataset\n",
    "\n",
    "```python\n",
    "class CopyDataset(Dataset):\n",
    "    def __init__(self, seq_len=5, num_samples=1000):\n",
    "        self.seq_len = seq_len\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            seq = torch.randint(0, 8, (seq_len,))\n",
    "            self.samples.append(seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.samples[idx]\n",
    "        # Input = seq + zeros + delimiter token (9)\n",
    "        inp = torch.cat([seq, torch.tensor([9]), torch.zeros(len(seq), dtype=torch.long)])\n",
    "        target = torch.cat([torch.zeros(len(seq)+1, dtype=torch.long), seq])\n",
    "        return inp, target\n",
    "\n",
    "dataset = CopyDataset()\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Section 3: Define Memory-Augmented Model (Simplified NTM)\n",
    "\n",
    "```python\n",
    "class TinyNTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=32, memory_size=16, memory_width=16):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size + memory_width, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size + memory_width, vocab_size)\n",
    "\n",
    "        # Memory\n",
    "        self.memory = torch.randn(1, memory_size, memory_width)\n",
    "\n",
    "        # Read/Write attention\n",
    "        self.read_head = nn.Linear(hidden_size, memory_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        x = self.embed(x)\n",
    "\n",
    "        memory = self.memory.repeat(B, 1, 1)  # [B, N, W]\n",
    "        read = torch.zeros(B, 1, memory.size(2)).to(x.device)  # initial read vector\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        h = torch.zeros(1, B, x.size(2)).to(x.device)\n",
    "        for t in range(T):\n",
    "            xt = x[:, t].unsqueeze(1)\n",
    "            rnn_in = torch.cat([xt, read], dim=2)\n",
    "            out, h = self.rnn(rnn_in, h)\n",
    "\n",
    "            # Read head: attention weights\n",
    "            attn = torch.softmax(self.read_head(out), dim=2)\n",
    "            read = torch.bmm(attn, memory)\n",
    "\n",
    "            # Output\n",
    "            out = torch.cat([out, read], dim=2)\n",
    "            logits = self.fc(out)\n",
    "            outputs.append(logits)\n",
    "\n",
    "        return torch.cat(outputs, dim=1)  # [B, T, V]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🏋️ Section 4: Train on Copy Task\n",
    "\n",
    "```python\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TinyNTM(vocab_size=10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inp, tgt in loader:\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        out = model(inp)\n",
    "        loss = criterion(out.view(-1, 10), tgt.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 👀 Section 5: Visualize Prediction\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "sample = dataset[0][0].unsqueeze(0).to(device)\n",
    "preds = model(sample).argmax(dim=-1).cpu().squeeze()\n",
    "\n",
    "print(\"Input:     \", sample.cpu().squeeze().tolist())\n",
    "print(\"Predicted: \", preds.tolist())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Wrap-Up Recap\n",
    "\n",
    "| Feature                         | ✅ |\n",
    "|----------------------------------|----|\n",
    "| External memory used             | ✅ |\n",
    "| Differentiable attention access  | ✅ |\n",
    "| Copy task solved via memory      | ✅ |\n",
    "| CPU/Colab safe, fast training    | ✅ |\n",
    "| Easy to scale to complex tasks   | ✅ |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- Memory-Augmented networks **extend RNNs** with read/write capacity  \n",
    "- Attention = soft pointer to memory slots  \n",
    "- Simple NTM can **copy sequences**, **recall patterns**, even simulate Turing-style behavior  \n",
    "- This is the **foundation of differentiable memory agents**\n",
    "\n",
    "---\n",
    "\n",
    "Next lab up is `09_lab_diffusion_model_toy_image_gen.ipynb` —  \n",
    "Shall we build a **tiny denoising diffusion pipeline** and visualize how it generates MNIST digits from pure noise?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
