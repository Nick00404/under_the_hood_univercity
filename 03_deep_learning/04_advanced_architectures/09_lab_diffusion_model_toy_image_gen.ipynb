{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1001a637",
   "metadata": {},
   "source": [
    "🎨 Let’s make noise into art. We’re diving into the **most powerful generative model class** of the 2020s: **diffusion models**.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `09_lab_diffusion_model_toy_image_gen.ipynb`  \n",
    "### 📁 `04_advanced_architectures`  \n",
    "> Implement a **toy Denoising Diffusion Probabilistic Model (DDPM)** on MNIST.  \n",
    "Step-by-step from **noise schedule → sampling → image recovery**.  \n",
    "Fully Colab + CPU/GPU friendly.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "- Understand **forward diffusion** (add noise)  \n",
    "- Train a model to **reverse it** (denoise)  \n",
    "- Visualize **denoising steps as images emerge**  \n",
    "- Generate **new digits from noise**\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Runtime Design\n",
    "\n",
    "| Feature             | Spec                   |\n",
    "|---------------------|------------------------|\n",
    "| Dataset             | MNIST (28×28) ✅  \n",
    "| Runtime             | Colab / Laptop CPU/GPU ✅  \n",
    "| Model               | Tiny UNet-ish CNN ✅  \n",
    "| VRAM                | < 2GB ✅  \n",
    "| Training time       | ~5 mins ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Section 1: Imports & Dataset\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "```python\n",
    "# Load MNIST\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Lambda(lambda x: x * 2 - 1)  # scale to [-1, 1]\n",
    "])\n",
    "\n",
    "ds = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Section 2: Noise Scheduler\n",
    "\n",
    "```python\n",
    "T_steps = 1000\n",
    "betas = torch.linspace(1e-4, 0.02, T_steps)\n",
    "alphas = 1 - betas\n",
    "alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "def q_sample(x0, t, noise=None):\n",
    "    \"\"\"Apply forward diffusion step.\"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x0)\n",
    "    sqrt_ab = alphas_bar[t]**0.5\n",
    "    sqrt_1ab = (1 - alphas_bar[t])**0.5\n",
    "    return sqrt_ab[:, None, None, None] * x0 + sqrt_1ab[:, None, None, None] * noise\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 Section 3: Denoising Model\n",
    "\n",
    "```python\n",
    "class SimpleDenoiser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        return self.net(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🏋️ Section 4: Training\n",
    "\n",
    "```python\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SimpleDenoiser().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for x0, _ in dl:\n",
    "        x0 = x0.to(device)\n",
    "        t = torch.randint(0, T_steps, (x0.size(0),), device=device)\n",
    "        noise = torch.randn_like(x0)\n",
    "        xt = q_sample(x0, t, noise=noise)\n",
    "        pred_noise = model(xt, t)\n",
    "        loss = loss_fn(pred_noise, noise)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Section 5: Reverse Process (Sampling)\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def sample_images(num=4):\n",
    "    x = torch.randn((num, 1, 28, 28)).to(device)\n",
    "    for t in reversed(range(T_steps)):\n",
    "        z = torch.randn_like(x) if t > 0 else 0\n",
    "        alpha = alphas[t]\n",
    "        alpha_bar = alphas_bar[t]\n",
    "        noise_pred = model(x, torch.tensor([t]*num).to(device))\n",
    "        x = (1 / alpha**0.5) * (x - (1 - alpha) / (1 - alpha_bar)**0.5 * noise_pred) + betas[t]**0.5 * z\n",
    "    return x\n",
    "\n",
    "imgs = sample_images(4).cpu()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎨 Section 6: Visualize Generated Images\n",
    "\n",
    "```python\n",
    "grid = torchvision.utils.make_grid((imgs + 1) / 2, nrow=4)\n",
    "plt.imshow(grid.permute(1, 2, 0))\n",
    "plt.title(\"Generated Digits (Diffusion)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Wrap-Up Summary\n",
    "\n",
    "| Feature                        | ✅ |\n",
    "|--------------------------------|----|\n",
    "| Forward & reverse process      | ✅ |\n",
    "| Trained on MNIST               | ✅ |\n",
    "| Generated from pure noise      | ✅ |\n",
    "| Visualized denoising steps     | ✅ |\n",
    "| Colab-friendly, fast training  | ✅ |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- Diffusion models work by **learning to denoise** step-by-step  \n",
    "- Unlike GANs, they use **likelihood training (MSE)** and are **stable**  \n",
    "- Each generation step **peels noise away** — like an image developing in reverse  \n",
    "- You’ve now built a **tiny DDPM** from scratch 💥\n",
    "\n",
    "---\n",
    "\n",
    "Next lab:  \n",
    "Shall we hit `05_model_optimization` next and explore `07_lab_weight_pruning_and_accuracy_tracking.ipynb`?  \n",
    "Let’s see what happens when we **cut out neurons** and **track how accuracy behaves**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
