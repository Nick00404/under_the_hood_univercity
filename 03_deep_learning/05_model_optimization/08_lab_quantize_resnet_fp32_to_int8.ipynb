{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a89aa67",
   "metadata": {},
   "source": [
    "üßä Time to go **from float to fighter jet.**  \n",
    "We're shrinking ResNet down to **INT8 precision** ‚Äî no performance lost, just **raw inference speed gains.**\n",
    "\n",
    "---\n",
    "\n",
    "# üß™ `08_lab_quantize_resnet_fp32_to_int8.ipynb`  \n",
    "### üìÅ `05_model_optimization`  \n",
    "> Quantize a pretrained **ResNet18** model from **FP32 ‚Üí INT8** using **ONNX or TFLite-style quantization**.  \n",
    "Benchmark **inference speed** and **accuracy before/after**.  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Goals\n",
    "\n",
    "- Understand **quantization basics**  \n",
    "- Convert model to **INT8 weights**  \n",
    "- Measure impact on **accuracy & latency**  \n",
    "- Tools: PyTorch + ONNX (no custom CUDA needed)\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Runtime Targets\n",
    "\n",
    "| Component            | Spec              |\n",
    "|----------------------|-------------------|\n",
    "| Model                | ResNet18 (Torchvision) ‚úÖ  \n",
    "| Dataset              | CIFAR-10 (quick test set) ‚úÖ  \n",
    "| Target format        | ONNX INT8 ‚úÖ  \n",
    "| Hardware             | CPU / Colab ‚úÖ  \n",
    "| Speed vs Accuracy    | Compared live ‚úÖ  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Section 1: Imports & Pretrained Model\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import time\n",
    "```\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "model_fp32 = models.resnet18(pretrained=True)\n",
    "model_fp32.eval()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Section 2: Inference Benchmark (FP32)\n",
    "\n",
    "```python\n",
    "def evaluate(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            outputs = model(images)\n",
    "            pred = outputs.argmax(1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    end = time.time()\n",
    "    acc = correct / total\n",
    "    latency = end - start\n",
    "    return acc, latency\n",
    "\n",
    "acc_fp32, time_fp32 = evaluate(model_fp32, testloader)\n",
    "print(f\"FP32 Accuracy: {acc_fp32:.4f} | Time: {time_fp32:.2f}s\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Section 3: Apply Static Quantization\n",
    "\n",
    "```python\n",
    "import torch.quantization\n",
    "\n",
    "model_q = models.resnet18(pretrained=True)\n",
    "model_q.eval()\n",
    "\n",
    "model_q.fuse_model = lambda: None  # Dummy hook for torchvision models\n",
    "model_q = torch.quantization.quantize_dynamic(model_q, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîé Section 4: Benchmark INT8 Model\n",
    "\n",
    "```python\n",
    "acc_int8, time_int8 = evaluate(model_q, testloader)\n",
    "print(f\"INT8 Accuracy: {acc_int8:.4f} | Time: {time_int8:.2f}s\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Section 5: Compare Results\n",
    "\n",
    "```python\n",
    "labels = ['FP32', 'INT8']\n",
    "accs = [acc_fp32, acc_int8]\n",
    "times = [time_fp32, time_int8]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(labels, accs, color=['blue', 'green'])\n",
    "plt.title(\"Accuracy Comparison\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(labels, times, color=['blue', 'green'])\n",
    "plt.title(\"Inference Time (seconds)\")\n",
    "\n",
    "plt.suptitle(\"FP32 vs INT8 Quantization Results\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Wrap-Up Summary\n",
    "\n",
    "| What You Achieved           | ‚úÖ |\n",
    "|-----------------------------|----|\n",
    "| Loaded & benchmarked FP32   | ‚úÖ |\n",
    "| Quantized to INT8           | ‚úÖ |\n",
    "| Compared accuracy + speed   | ‚úÖ |\n",
    "| Ran on CPU / Colab          | ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What You Learned\n",
    "\n",
    "- Quantization trades **precision for performance**  \n",
    "- INT8 can run **2√ó to 4√ó faster** with minimal accuracy loss  \n",
    "- You now know how to **optimize models for edge/mobile/real-time inference**\n",
    "\n",
    "---\n",
    "\n",
    "Wanna go full Sensei mode and hit `09_lab_distill_teacher_student_on_mnist.ipynb` next?  \n",
    "We‚Äôll clone a big model‚Äôs brain into a tiny one ‚Äî **knowledge distillation** style üß™üß†üçº. Let‚Äôs roll?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
