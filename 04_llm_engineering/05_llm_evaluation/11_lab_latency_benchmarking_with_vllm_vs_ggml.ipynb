{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8120bcb3",
   "metadata": {},
   "source": [
    "‚ö° Now we test **how fast our LLMs can think** ‚Äî it's time for a **latency showdown** between **vLLM** (GPU-optimized) and **GGML** (CPU-optimized quantized models). Welcome to **LLM performance engineering**.\n",
    "\n",
    "---\n",
    "\n",
    "# üìí `11_lab_latency_benchmarking_with_vllm_vs_ggml.ipynb`  \n",
    "## üìÅ `05_llm_engineering/05_llm_evaluation`\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Notebook Goals**\n",
    "\n",
    "- Benchmark **latency**, **throughput**, and **token generation speed** for:\n",
    "  - ‚úÖ GPU LLMs using **vLLM**\n",
    "  - ‚úÖ Quantized CPU models using **GGML**\n",
    "- Compare performance under different batch sizes + prompt lengths\n",
    "- Visualize tradeoffs üß† vs ‚ö°\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è 1. Setup (Environment-Specific)\n",
    "\n",
    "You‚Äôll need one of the following setups:\n",
    "- **GPU (Colab Pro / Local / Cloud)** ‚Üí `vLLM` with `Triton`, `CUDA`\n",
    "- **CPU-only laptop** ‚Üí `ggml` or `llama.cpp` model with quantized weights\n",
    "\n",
    "Use mock data if hardware not available.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ 2. Sample Benchmark Prompts\n",
    "\n",
    "```python\n",
    "prompts = [\n",
    "    \"Summarize the plot of Inception in one paragraph.\",\n",
    "    \"Translate this sentence into French: 'I love machine learning.'\",\n",
    "    \"List three use cases of LLMs in healthcare.\",\n",
    "    \"Explain quantum entanglement to a 10-year-old.\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ 3. Timing vLLM Inference\n",
    "\n",
    "```python\n",
    "import time\n",
    "import openai  # or vLLM local endpoint if available\n",
    "\n",
    "def run_vllm_benchmark(prompt):\n",
    "    start = time.time()\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Replace with local vLLM if using\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(f\"‚è±Ô∏è vLLM Latency: {duration:.2f} sec\")\n",
    "    return duration\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ 4. Timing GGML (CPU Local / llama.cpp)\n",
    "\n",
    "If testing locally:\n",
    "\n",
    "```bash\n",
    "./main -m ggml-model.bin -p \"Prompt here\" -n 100\n",
    "```\n",
    "\n",
    "Then parse the runtime from stdout and compare.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 5. Plotting Latency Comparison\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vllm_times = [1.2, 1.4, 1.1, 1.6]\n",
    "ggml_times = [2.5, 2.9, 3.2, 2.7]\n",
    "\n",
    "plt.plot(prompts, vllm_times, label=\"vLLM (GPU)\")\n",
    "plt.plot(prompts, ggml_times, label=\"GGML (CPU)\")\n",
    "plt.ylabel(\"Latency (sec)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"LLM Inference Speed Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What You Built\n",
    "\n",
    "| Tool           | Role |\n",
    "|----------------|------|\n",
    "| vLLM Benchmark | GPU-accelerated testing |\n",
    "| GGML Benchmark | CPU-efficient quant test |\n",
    "| Visual Report  | Easy comparison of performance |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Wrap-Up\n",
    "\n",
    "| Task                             | ‚úÖ |\n",
    "|----------------------------------|----|\n",
    "| Ran LLM on GPU and CPU           | ‚úÖ |\n",
    "| Measured latency & throughput    | ‚úÖ |\n",
    "| Visualized generation performance| ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "## üîÆ Final Lab Incoming‚Ä¶\n",
    "\n",
    "üìí `12_lab_model_card_generator_pipeline.ipynb`  \n",
    "Let‚Äôs build a **model card generator** that documents your LLM‚Äôs metrics, use cases, limitations, and risks ‚Äî essential for real-world deployments and audits.\n",
    "\n",
    "Time to get compliant, transparent, and production-ready, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
