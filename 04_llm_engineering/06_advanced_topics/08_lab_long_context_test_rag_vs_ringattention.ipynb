{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a400b7d1",
   "metadata": {},
   "source": [
    "📜🧠 **Professor, we’re going long.**\n",
    "\n",
    "You're entering the realm where **most models forget**...  \n",
    "But you? You’re about to compare **retrieval** vs **native long attention** over **8K+ token contexts**.\n",
    "\n",
    "This lab is all about **context resilience** — who holds memory, who forgets, and when to RAG instead.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `08_lab_long_context_test_rag_vs_ringattention.ipynb`  \n",
    "### 📁 `05_llm_engineering/06_advanced_topics`  \n",
    "> Feed LLMs **long documents (4K–32K tokens)**  \n",
    "→ See how well **Ring Attention, FlashAttention**, and **RAG** methods  \n",
    "→ Retain key info from **early, middle, and late** parts of the prompt\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Goals\n",
    "\n",
    "- Understand **why long-context matters** in real-world use (contracts, codebases, papers)  \n",
    "- Test model recall across different **prompt positions**  \n",
    "- Compare **Flash/Ring attention** vs **RAG-based refresh**  \n",
    "- Visualize attention decay and token importance maps\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Runtime Spec\n",
    "\n",
    "| Feature         | Setup                            |\n",
    "|------------------|----------------------------------|\n",
    "| Models           | Claude-style (e.g. Longformer, Mistral, RAG-ready GPT2) ✅  \n",
    "| Dataset          | Long news articles, stories, code ✅  \n",
    "| Metrics          | Recall@position, cosine match ✅  \n",
    "| Visuals          | Heatmaps, token scoring ✅  \n",
    "| Runtime          | GPU recommended (Colab Pro ok) ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Section 1: Create a Long Document\n",
    "\n",
    "```python\n",
    "section_intro = \"Topic: Machine Learning\\n\\n\"\n",
    "middle_blob = \"Noise filler. \" * 3000  # ~6K tokens\n",
    "needle = \"The secret keyword is PROFESSOR_MOOOAAHHH\"\n",
    "long_context = section_intro + middle_blob + needle + \"\\n\" + middle_blob\n",
    "\n",
    "print(\"Length (tokens est):\", len(long_context.split()))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 Section 2: Define Retrieval Baseline (RAG)\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "retriever = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "needle_embedding = retriever.encode(\"PROFESSOR_MOOOAAHHH\")\n",
    "\n",
    "# Simulate chunks\n",
    "chunks = [long_context[i:i+500] for i in range(0, len(long_context), 500)]\n",
    "\n",
    "scored = []\n",
    "for chunk in chunks:\n",
    "    score = util.cos_sim(needle_embedding, retriever.encode(chunk))[0][0]\n",
    "    scored.append((score.item(), chunk[:60]))\n",
    "\n",
    "top_hits = sorted(scored, reverse=True)[:3]\n",
    "for i, (score, snip) in enumerate(top_hits):\n",
    "    print(f\"Hit {i+1}: {score:.4f} → {snip}...\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Section 3: Feed Full Prompt to Long-Attention Model\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "input_ids = tokenizer(long_context, return_tensors=\"pt\", truncation=True).input_ids.to(model.device)\n",
    "\n",
    "out = model.generate(input_ids, max_new_tokens=30)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Section 4: Position Recall Test\n",
    "\n",
    "Split `long_context` into 3 positions:\n",
    "\n",
    "- 🟢 Beginning (0–500 tokens)  \n",
    "- 🟡 Middle (5K–6K)  \n",
    "- 🔴 End (final 500)\n",
    "\n",
    "Inject **similar questions** referencing each and measure output quality.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Lab Wrap-Up\n",
    "\n",
    "| What You Tested                  | ✅ |\n",
    "|----------------------------------|----|\n",
    "| Long document fed into LLM       | ✅  \n",
    "| RAG retrieved key chunk          | ✅  \n",
    "| Native attention held long token | ✅  \n",
    "| Recall mapped across positions   | ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- Most models degrade after **3–4K tokens** unless specially trained  \n",
    "- RAG helps with **targeted refresh** — especially when full prompt is too long  \n",
    "- Long-attention models like **Ring / Flash** retain position but still struggle on low-RAM GPUs  \n",
    "- Hybrid = use **long attention + fallback to RAG**\n",
    "\n",
    "---\n",
    "\n",
    "Only one lab remains, Professor…  \n",
    "> `09_lab_multi_agent_llm_scratchpad_protocol.ipynb`  \n",
    "We simulate **multi-agent LLM workflows** with memory, tools, scratchpads, and message passing —  \n",
    "building the skeleton of **AutoGPT** from scratch.\n",
    "\n",
    "Wanna see your models **talk, think, and solve as a team**?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
