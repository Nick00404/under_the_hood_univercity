{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b93d963",
   "metadata": {},
   "source": [
    "🎮 Welcome to the **RLHF dojo**, Professor.\n",
    "\n",
    "We’re about to simulate the magic behind **ChatGPT tuning** —  \n",
    "where thumbs up/down from humans train a **reward model**…  \n",
    "and that reward model helps refine responses using **Reinforcement Learning with Human Feedback (RLHF)**.\n",
    "\n",
    "This is how OpenAI made GPT helpful, harmless, and honest (well... mostly).\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `09_lab_rlhf_reward_model_mock_demo.ipynb`  \n",
    "### 📁 `05_llm_engineering/02_pretraining_and_finetuning`  \n",
    "> Simulate **RLHF pipeline** with a simple reward model and PPO loop  \n",
    "→ Fine-tune model outputs based on **feedback scores**  \n",
    "→ Learn how ChatGPT got its alignment crown 👑\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Goals\n",
    "\n",
    "- Understand RLHF’s three-part process:  \n",
    "  1. Supervised Finetuning (SFT)  \n",
    "  2. Reward Model Training  \n",
    "  3. PPO / Policy Optimization  \n",
    "- Train a **mock reward model** on simple preference data  \n",
    "- Simulate a PPO-like **reward-guided optimization loop**\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Runtime Spec\n",
    "\n",
    "| Component      | Spec                          |\n",
    "|----------------|-------------------------------|\n",
    "| Base Model     | GPT2 (or distilled variant) ✅  \n",
    "| Reward Model   | Classifier on prompt + reply ✅  \n",
    "| Feedback Data  | Simulated preference pairs ✅  \n",
    "| Optimizer      | Simplified PPO loop ✅  \n",
    "| Platform       | Colab / Laptop ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Section 1: Install Dependencies\n",
    "\n",
    "```bash\n",
    "!pip install transformers peft datasets accelerate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Section 2: Simulate Feedback Dataset\n",
    "\n",
    "```python\n",
    "examples = [\n",
    "    {\"prompt\": \"Tell me a joke\", \"good\": \"Why did the cat sit on the computer? To keep an eye on the mouse!\",\n",
    "     \"bad\": \"I don’t know.\"},\n",
    "    {\"prompt\": \"What is AI?\", \"good\": \"AI is the simulation of human intelligence in machines.\",\n",
    "     \"bad\": \"Just a robot.\"},\n",
    "    {\"prompt\": \"Define overfitting\", \"good\": \"Overfitting is when a model performs well on training data but poorly on new data.\",\n",
    "     \"bad\": \"It fits too much.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Section 3: Reward Model as Classifier\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def make_dataset(examples):\n",
    "    data = []\n",
    "    for e in examples:\n",
    "        data.append({\"text\": e[\"prompt\"] + \" \" + e[\"good\"], \"label\": 1})\n",
    "        data.append({\"text\": e[\"prompt\"] + \" \" + e[\"bad\"], \"label\": 0})\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "reward_data = make_dataset(examples)\n",
    "reward_data = reward_data.map(lambda x: tokenizer(x[\"text\"], padding=\"max_length\", truncation=True), batched=True)\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🏋️ Section 4: Train Reward Model\n",
    "\n",
    "```python\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./reward_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=reward_model,\n",
    "    args=args,\n",
    "    train_dataset=reward_data\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Section 5: Simulate PPO Loop (Mini)\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "gen_model = pipeline(\"text-generation\", model=\"gpt2\", tokenizer=\"gpt2\")\n",
    "\n",
    "def score_output(prompt, response):\n",
    "    inputs = tokenizer(prompt + \" \" + response, return_tensors=\"pt\")\n",
    "    logits = reward_model(**inputs).logits\n",
    "    return torch.softmax(logits, dim=-1)[0][1].item()  # probability of label 1 (good)\n",
    "\n",
    "prompt = \"Tell me a joke\"\n",
    "outputs = [gen_model(prompt, max_length=30)[0]['generated_text'] for _ in range(3)]\n",
    "for o in outputs:\n",
    "    print(f\"Output: {o.strip()}\")\n",
    "    print(f\"Reward: {score_output(prompt, o):.3f}\\n\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Lab Wrap-Up\n",
    "\n",
    "| Feature                             | ✅ |\n",
    "|-------------------------------------|----|\n",
    "| Simulated SFT feedback dataset      | ✅  \n",
    "| Reward model trained from feedback  | ✅  \n",
    "| PPO-like reward scoring loop        | ✅  \n",
    "| Output optimization by reward       | ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- RLHF aligns LLMs to **human preference**  \n",
    "- Reward models are just **text classifiers with attitude**  \n",
    "- Even **toy PPO loops can show alignment behavior**  \n",
    "- This is the basis of **modern dialogue tuning** for LLMs\n",
    "\n",
    "---\n",
    "\n",
    "That wraps up your RLHF lab. You’ve officially touched the core of **ChatGPT-style fine-tuning**.\n",
    "\n",
    "Next up?  \n",
    "> `07_lab_chunking_and_embedding_evaluation.ipynb`  \n",
    "Let’s shift into **RAG systems**: chunk text, embed, retrieve intelligently —  \n",
    "and **build the brains** behind AI assistants that cite sources.\n",
    "\n",
    "Ready to chunk and retrieve like a pro?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
