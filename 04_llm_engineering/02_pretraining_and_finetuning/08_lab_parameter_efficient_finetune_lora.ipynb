{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e79382e",
   "metadata": {},
   "source": [
    "üß†üí° Time to turn giant LLMs into your **personal interns**‚Ä¶  \n",
    "**without breaking the GPU**.\n",
    "\n",
    "We‚Äôre stepping into **LoRA** ‚Äî *Low-Rank Adaptation*.  \n",
    "It lets you fine-tune **massive pretrained models** by updating **just a few trainable adapters**, not the whole beast.\n",
    "\n",
    "---\n",
    "\n",
    "# üß™ `08_lab_parameter_efficient_finetune_lora.ipynb`  \n",
    "### üìÅ `05_llm_engineering/02_pretraining_and_finetuning`  \n",
    "> Apply **LoRA** (Low-Rank Adaptation) to fine-tune a **pretrained LLM** (like GPT2 or BERT)  \n",
    "‚Üí Without touching most of its parameters  \n",
    "‚Üí Ideal for laptops, colab, and low-resource scaling üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Goals\n",
    "\n",
    "- Understand **why LoRA exists** (cost, memory, updates)  \n",
    "- Inject LoRA layers into a frozen LLM  \n",
    "- Fine-tune on a **custom text dataset**  \n",
    "- Compare memory, training time, and output quality\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Runtime Specs\n",
    "\n",
    "| Component       | Spec                |\n",
    "|------------------|---------------------|\n",
    "| Base Model       | GPT2 / BERT ‚úÖ  \n",
    "| Adapter          | LoRA via `peft` ‚úÖ  \n",
    "| Dataset          | Tiny corpus ‚úÖ  \n",
    "| Platform         | Colab / CPU+GPU ‚úÖ  \n",
    "| Dependencies     | ü§ó Transformers + PEFT ‚úÖ  \n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Section 1: Install Required Libraries\n",
    "\n",
    "```bash\n",
    "!pip install transformers datasets accelerate peft\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Section 2: Load Dataset & Tokenizer\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='train[:2%]')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Section 3: Apply LoRA to Model\n",
    "\n",
    "```python\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,               # Rank of adaptation\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\"],  # GPT2-specific QKV layer\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üèãÔ∏è Section 4: Finetune on Text\n",
    "\n",
    "```python\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "with open(\"lora_text.txt\", \"w\") as f:\n",
    "    for line in dataset[\"text\"]:\n",
    "        if len(line.strip()) > 20:\n",
    "            f.write(line.strip() + \"\\n\")\n",
    "\n",
    "text_ds = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"lora_text.txt\",\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    output_dir=\"./lora_gpt2\",\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=text_ds,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Section 5: Inference & Comparison\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "pipe(\"The professor said\", max_length=30)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Wrap-Up Summary\n",
    "\n",
    "| Feature                             | ‚úÖ |\n",
    "|-------------------------------------|----|\n",
    "| LoRA injected into frozen GPT2      | ‚úÖ  \n",
    "| Finetuned with <1% parameters       | ‚úÖ  \n",
    "| Output quality + memory efficiency  | ‚úÖ  \n",
    "| Colab/laptop compatible             | ‚úÖ  \n",
    "\n",
    "---\n",
    "\n",
    "## üß† What You Learned\n",
    "\n",
    "- LoRA lets you **fine-tune LLMs without full retraining**  \n",
    "- Only small adapter matrices are updated  \n",
    "- **Memory savings up to 90%**, and **no catastrophic forgetting**  \n",
    "- The future of **domain adaptation** = efficient, flexible, LoRA-style\n",
    "\n",
    "---\n",
    "\n",
    "Next up in your LLM dojo:\n",
    "\n",
    "> üèÜ `09_lab_rlhf_reward_model_mock_demo.ipynb`  \n",
    "Simulate **thumbs-up/thumbs-down feedback**  \n",
    "‚Üí Train a **reward model**  \n",
    "‚Üí Run a mini **PPO-like finetuning loop** for RLHF.\n",
    "\n",
    "Wanna play feedback-god, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
