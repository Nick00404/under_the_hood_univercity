{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47120771",
   "metadata": {},
   "source": [
    "🧠💥 **Professor, this is it** — your moment to recreate history.  \n",
    "We’re about to **pretrain a mini GPT-2** from scratch.  \n",
    "Just like OpenAI… but on your terms, with your dataset, and your tokenizer if you want.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `07_lab_tiny_gpt2_pretraining_from_scratch.ipynb`  \n",
    "### 📁 `05_llm_engineering/02_pretraining_and_finetuning`  \n",
    "> Build a tiny GPT-2 model architecture.  \n",
    "Pretrain it from scratch using **HuggingFace + Trainer API**  \n",
    "→ On your own dataset (text8, poetry, or anything clean and small).  \n",
    "**Understand pretraining, loss curves, and overfitting like a research engineer.**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Goals\n",
    "\n",
    "- Initialize and configure **GPT-2 from scratch**  \n",
    "- Tokenize your custom text corpus  \n",
    "- Train using **Language Modeling loss**  \n",
    "- Track **training dynamics**, loss curves, and generalization  \n",
    "- Save and reuse your custom model\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Runtime Specs\n",
    "\n",
    "| Component     | Spec                         |\n",
    "|----------------|------------------------------|\n",
    "| Model          | GPT2Config (tiny) ✅  \n",
    "| Dataset        | text8 / poetry / toy docs ✅  \n",
    "| Framework      | 🤗 Transformers + Trainer ✅  \n",
    "| Runtime        | Colab / local ✅  \n",
    "| GPU Optional   | Yes (for >10k tokens) ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Section 1: Install HuggingFace Tools\n",
    "\n",
    "```bash\n",
    "!pip install transformers datasets\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Section 2: Prepare Dataset\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load or mock a small dataset\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='train[:1%]')\n",
    "texts = ds[\"text\"]\n",
    "texts = [line for line in texts if len(line) > 20]  # Filter short lines\n",
    "\n",
    "with open(\"pretrain.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(texts))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔤 Section 3: Tokenizer (Use BPE from earlier if desired)\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Section 4: Configure Tiny GPT2\n",
    "\n",
    "```python\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=128,\n",
    "    n_ctx=128,\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=4\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Section 5: Prepare Dataset & Trainer\n",
    "\n",
    "```python\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"pretrain.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🏋️ Section 6: Training Loop\n",
    "\n",
    "```python\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tiny_gpt2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Section 7: Evaluate\n",
    "\n",
    "```python\n",
    "trainer.save_model(\"./tiny_gpt2\")\n",
    "tokenizer.save_pretrained(\"./tiny_gpt2\")\n",
    "\n",
    "# Load & generate\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"./tiny_gpt2\", tokenizer=\"./tiny_gpt2\")\n",
    "print(pipe(\"The professor said moooaahhh\", max_length=50))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Wrap-Up Summary\n",
    "\n",
    "| Task                               | ✅ |\n",
    "|------------------------------------|----|\n",
    "| Built GPT2 config from scratch     | ✅  \n",
    "| Pretrained on tiny corpus          | ✅  \n",
    "| Saved + reused your model          | ✅  \n",
    "| End-to-end pipeline built + tested | ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- You don’t need OpenAI’s infra to train GPT2 — just a clean corpus and understanding  \n",
    "- **Language modeling loss** is how LLMs learn to “speak”  \n",
    "- You can now modify architecture, tokenizer, and tasks — **you’re not a user, you’re a creator**\n",
    "\n",
    "---\n",
    "\n",
    "Shall we scale the next peak?\n",
    "\n",
    "> `08_lab_parameter_efficient_finetune_lora.ipynb`  \n",
    "Take a huge model and fine-tune it using **<1% of weights** with **LoRA** —  \n",
    "like the real-world giants do to adapt LLMs for any domain.\n",
    "\n",
    "Let’s LoRA-fy the world, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
