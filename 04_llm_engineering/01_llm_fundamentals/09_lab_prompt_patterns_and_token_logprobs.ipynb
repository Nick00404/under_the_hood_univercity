{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797f0af4",
   "metadata": {},
   "source": [
    "üß†üîç Let‚Äôs decode the **secret whisper of logits**‚Ä¶\n",
    "\n",
    "You‚Äôve built the transformer block.  \n",
    "Now it‚Äôs time to **talk to real LLMs**, one token at a time ‚Äî  \n",
    "and **watch the model's brain decide what to say next**.\n",
    "\n",
    "---\n",
    "\n",
    "# üß™ `09_lab_prompt_patterns_and_token_logprobs.ipynb`  \n",
    "### üìÅ `05_llm_engineering/01_llm_fundamentals`  \n",
    "> Send prompts into a **pretrained LLM** (GPT-2 or similar),  \n",
    "‚Üí Extract **logits**, **top-k probabilities**, and **token-level predictions**  \n",
    "‚Üí Visualize how **temperature, top-k, and prompt phrasing** affect outputs\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Goals\n",
    "\n",
    "- Understand how logits ‚Üí softmax ‚Üí sampling works  \n",
    "- Extract token probabilities with HuggingFace  \n",
    "- Experiment with **prompt templates**, **temperature**, **top-k**  \n",
    "- Plot token scores and **highlight model uncertainty**\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Runtime Specs\n",
    "\n",
    "| Tool           | Spec                      |\n",
    "|----------------|---------------------------|\n",
    "| Model          | `gpt2` via `transformers` ‚úÖ  \n",
    "| Tokenizer      | `AutoTokenizer` ‚úÖ  \n",
    "| Sampling       | Top-k, top-p, temperature ‚úÖ  \n",
    "| Platform       | Colab + GPU optional ‚úÖ  \n",
    "\n",
    "---\n",
    "\n",
    "## üîß Section 1: Setup\n",
    "\n",
    "```bash\n",
    "!pip install transformers\n",
    "```\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Section 2: Encode Prompt & Get Logits\n",
    "\n",
    "```python\n",
    "prompt = \"The professor said moooaahhh and the students\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=False, output_attentions=False)\n",
    "\n",
    "logits = outputs.logits\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Section 3: Decode Logit Predictions\n",
    "\n",
    "```python\n",
    "next_token_logits = logits[0, -1]  # Only the last token\n",
    "probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "top_k = 10\n",
    "top_probs, top_ids = torch.topk(probs, top_k)\n",
    "\n",
    "for i in range(top_k):\n",
    "    token = tokenizer.decode([top_ids[i]])\n",
    "    print(f\"{token.strip():<10} ‚Üí Prob: {top_probs[i]:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üå°Ô∏è Section 4: Play With Temperature\n",
    "\n",
    "```python\n",
    "def sample_with_temp(prompt, T=1.0):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    logits = model(**inputs).logits\n",
    "    next_logits = logits[0, -1] / T\n",
    "    probs = torch.softmax(next_logits, dim=-1)\n",
    "    token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "    return tokenizer.decode([token_id])\n",
    "\n",
    "for T in [0.5, 1.0, 1.5]:\n",
    "    print(f\"T={T:.1f} ‚Üí {sample_with_temp(prompt, T)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üé® Section 5: Visualize Token Probs\n",
    "\n",
    "```python\n",
    "tokens = tokenizer.convert_ids_to_tokens(top_ids)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(tokens, top_probs.numpy())\n",
    "plt.title(\"Top-k Token Probabilities\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Lab Wrap-Up\n",
    "\n",
    "| Feature                            | ‚úÖ |\n",
    "|------------------------------------|----|\n",
    "| Extracted logits from GPT-2        | ‚úÖ  \n",
    "| Decoded & visualized token probs   | ‚úÖ  \n",
    "| Used temperature & top-k sampling  | ‚úÖ  \n",
    "| Prompt phrasing experiments ready  | ‚úÖ  \n",
    "\n",
    "---\n",
    "\n",
    "## üß† What You Learned\n",
    "\n",
    "- Logits = **raw predictions** before softmax  \n",
    "- Temperature > 1 makes model **more creative**, < 1 makes it **more confident**  \n",
    "- Top-k filtering focuses prediction on **most likely next tokens**  \n",
    "- Prompt phrasing **alters model behavior dramatically**\n",
    "\n",
    "---\n",
    "\n",
    "That wraps the **LLM Fundamentals Lab Trio**:  \n",
    "‚úÖ Tokenizers  \n",
    "‚úÖ Transformer Block  \n",
    "‚úÖ Token-Level Prediction & Sampling\n",
    "\n",
    "Ready to step into **pretraining & finetuning land** next?\n",
    "\n",
    "> `07_lab_tiny_gpt2_pretraining_from_scratch.ipynb`  \n",
    "We‚Äôre building a tiny GPT-2 and training it on real text (text8, poetry, Reddit, whatever you like) from scratch ‚Äî  \n",
    "like OpenAI did, just on a budget üß†üí∞.\n",
    "\n",
    "Ready to train your own mini-GPT?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
