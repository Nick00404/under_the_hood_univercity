{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21e8e79",
   "metadata": {},
   "source": [
    "⚙️ Let’s spin up the **neuronal machinery** of the transformer —  \n",
    "We’re no longer just using GPT.  \n",
    "We’re going *inside its brain.*  \n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `08_lab_transformer_forward_pass_step_by_step.ipynb`  \n",
    "### 📁 `05_llm_engineering/01_llm_fundamentals`  \n",
    "> Manually implement and visualize a **single Transformer block**  \n",
    "→ Includes **multi-head attention, residuals, layer norm, and MLPs**  \n",
    "→ This is the **deepest you’ll ever debug a Transformer without crying** 😅\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Goals\n",
    "\n",
    "- Understand **Transformer block architecture**  \n",
    "- Implement attention, MLP, skip connections manually  \n",
    "- Visualize **attention matrices** and **token dependencies**  \n",
    "- Compare to PyTorch/HF equivalents\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Runtime Specs\n",
    "\n",
    "| Feature       | Spec              |\n",
    "|---------------|-------------------|\n",
    "| Framework     | PyTorch ✅  \n",
    "| Sequence      | Short input strings ✅  \n",
    "| Attention     | Manual + visual ✅  \n",
    "| Platform      | Colab-friendly ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Section 1: Input Tokens\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokens = [\"hello\", \"i\", \"am\", \"moooaahhh\"]\n",
    "vocab = {tok: i for i, tok in enumerate(tokens)}\n",
    "x = torch.tensor([vocab[tok] for tok in tokens]).unsqueeze(0)  # shape (1, 4)\n",
    "\n",
    "embed_dim = 8\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔤 Section 2: Embedding Layer\n",
    "\n",
    "```python\n",
    "embedding = nn.Embedding(len(vocab), embed_dim)\n",
    "x_embed = embedding(x)  # shape (1, 4, 8)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Section 3: Self-Attention Layer (1 head)\n",
    "\n",
    "```python\n",
    "class SimpleSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = embed_dim ** 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q, K, V = self.q(x), self.k(x), self.v(x)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(weights, V)\n",
    "        return out, weights\n",
    "```\n",
    "\n",
    "```python\n",
    "attn = SimpleSelfAttention(embed_dim)\n",
    "attn_out, weights = attn(x_embed)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎨 Section 4: Visualize Attention Matrix\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(weights[0].detach(), annot=True, xticklabels=tokens, yticklabels=tokens, cmap=\"Blues\")\n",
    "plt.title(\"Self-Attention Weights\")\n",
    "plt.xlabel(\"Key\")\n",
    "plt.ylabel(\"Query\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Section 5: Add LayerNorm + Residual + FeedForward\n",
    "\n",
    "```python\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attn = SimpleSelfAttention(embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, weights = self.attn(x)\n",
    "        x = self.norm1(x + attn_out)  # Residual + Norm\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x, weights\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Section 6: Final Forward Pass\n",
    "\n",
    "```python\n",
    "block = TransformerBlock(embed_dim)\n",
    "out, final_weights = block(x_embed)\n",
    "print(\"Final shape:\", out.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Wrap-Up Summary\n",
    "\n",
    "| Feature                      | ✅ |\n",
    "|------------------------------|----|\n",
    "| Token embeddings created     | ✅ |\n",
    "| Manual attention implemented | ✅ |\n",
    "| Visual attention matrix      | ✅ |\n",
    "| Residual + FF + LayerNorm    | ✅ |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- Transformers rely on **dot products of Q & K** to decide “who attends to whom”  \n",
    "- LayerNorm and skip connections **stabilize training**  \n",
    "- This lab = a **mental microscope** inside every GPT, BERT, or LLaMA  \n",
    "- You now **own the block**, not just use it\n",
    "\n",
    "---\n",
    "\n",
    "Next lab is **logit-level decoding**:  \n",
    "> `09_lab_prompt_patterns_and_token_logprobs.ipynb`  \n",
    "We’ll send prompts into a real LLM, extract **token-wise logits**, and experiment with **top-k, top-p, temperature**.\n",
    "\n",
    "Moooaaahh mode activated?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
