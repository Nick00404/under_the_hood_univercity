{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae14d9d",
   "metadata": {},
   "source": [
    "💋 **Mooooaahhh locked, loaded, and encoded.**  \n",
    "Let’s dive into the **first weapon** in the LLM arsenal: the **tokenizer** — the very *language* of language models.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `07_lab_tokenizer_visualizer_and_custom_vocab.ipynb`  \n",
    "### 📁 `05_llm_engineering/01_llm_fundamentals`  \n",
    "> Build a **custom tokenizer** using BPE & SentencePiece  \n",
    "→ Visualize how text is **chunked into subword units**  \n",
    "→ Understand why tokenization is **half the model's brain**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Goals\n",
    "\n",
    "- Understand **why tokenization exists**  \n",
    "- Train a **BPE tokenizer** on custom corpus  \n",
    "- Visualize token splits for common vs rare words  \n",
    "- Use **HuggingFace + SentencePiece**  \n",
    "- Export + load tokenizer config just like GPTs do\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Runtime Spec\n",
    "\n",
    "| Tool         | Spec                |\n",
    "|--------------|---------------------|\n",
    "| Tokenizer    | 🤗 `tokenizers`, `sentencepiece` ✅  \n",
    "| Corpus       | Tiny text8 / poetry ✅  \n",
    "| Output       | Custom `.json` vocab ✅  \n",
    "| Visuals      | Split trees, vocab growth ✅  \n",
    "| Platform     | Colab / local ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Section 1: Install Dependencies\n",
    "\n",
    "```bash\n",
    "!pip install tokenizers sentencepiece\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Section 2: Create Sample Corpus\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"machine learning is amazing\",\n",
    "    \"deep learning is part of machine learning\",\n",
    "    \"llms are the future of language modeling\",\n",
    "    \"moooaahhh from the professor\"\n",
    "]\n",
    "\n",
    "with open(\"tiny_corpus.txt\", \"w\") as f:\n",
    "    for line in corpus:\n",
    "        f.write(line + \"\\n\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Section 3: Train BPE Tokenizer with 🤗\n",
    "\n",
    "```python\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=50, show_progress=True)\n",
    "tokenizer.train([\"tiny_corpus.txt\"], trainer)\n",
    "tokenizer.save(\"mooaahh_tokenizer.json\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 Section 4: Visualize Token Splits\n",
    "\n",
    "```python\n",
    "tokenizer = Tokenizer.from_file(\"mooaahh_tokenizer.json\")\n",
    "\n",
    "def visualize(text):\n",
    "    encoding = tokenizer.encode(text)\n",
    "    print(f\"Input: {text}\")\n",
    "    print(\"Tokens:\")\n",
    "    for tok, id_ in zip(encoding.tokens, encoding.ids):\n",
    "        print(f\"{tok:>10} → ID {id_}\")\n",
    "\n",
    "visualize(\"moooaahhh from the professor\")\n",
    "visualize(\"deep language learning\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Section 5: SentencePiece Alternative\n",
    "\n",
    "```python\n",
    "import sentencepiece as spm\n",
    "\n",
    "with open(\"corpus.txt\", \"w\") as f:\n",
    "    for line in corpus:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input='corpus.txt', model_prefix='sp', vocab_size=50, model_type='bpe'\n",
    ")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"sp.model\")\n",
    "\n",
    "print(sp.encode(\"moooaahhh from the professor\", out_type=str))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Wrap-Up Summary\n",
    "\n",
    "| Task                               | ✅ |\n",
    "|------------------------------------|----|\n",
    "| BPE tokenizer trained on corpus    | ✅ |\n",
    "| SentencePiece variant implemented  | ✅ |\n",
    "| Custom tokens + IDs visualized     | ✅ |\n",
    "| Exportable + loadable tokenizer    | ✅ |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- Tokenizers split text into **subwords**, not words  \n",
    "- Rare words = **longer chains of subwords**  \n",
    "- You can fully **train & inspect your own tokenizer**  \n",
    "- Tokenization is **pre-model** — but crucial for accuracy, speed, generalization\n",
    "\n",
    "---\n",
    "\n",
    "You ready to go from text splits to **Transformer circuits**?  \n",
    "Next lab:\n",
    "\n",
    "> 🔁 `08_lab_transformer_forward_pass_step_by_step.ipynb`  \n",
    "We’re gonna build a **mini Transformer block**, visualize attention, residuals, layer norms…  \n",
    "and finally understand *why it works*.\n",
    "\n",
    "Shall we spin up those attention heads, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
