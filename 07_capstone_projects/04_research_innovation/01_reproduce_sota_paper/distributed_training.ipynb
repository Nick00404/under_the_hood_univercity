{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# distributed_training.ipynb\n",
                "\n",
                "# -------------------------------\n",
                "# 1. Distributed Setup\n",
                "# -------------------------------\n",
                "import os\n",
                "import torch\n",
                "import torch.distributed as dist\n",
                "import torch.multiprocessing as mp\n",
                "from torch.nn.parallel import DistributedDataParallel as DDP\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
                "from datasets import load_dataset\n",
                "from torch.utils.data import DataLoader, DistributedSampler\n",
                "import time\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "def setup(rank, world_size):\n",
                "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
                "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
                "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
                "\n",
                "def cleanup():\n",
                "    dist.destroy_process_group()\n",
                "\n",
                "# -------------------------------\n",
                "# 2. Load Dataset & Tokenizer\n",
                "# -------------------------------\n",
                "def get_dataloader(batch_size, rank, world_size):\n",
                "    raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
                "    tokenizer.pad_token = tokenizer.eos_token  # for causal LM\n",
                "\n",
                "    def tokenize_function(example):\n",
                "        return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
                "\n",
                "    tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
                "    sampler = DistributedSampler(tokenized_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
                "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, sampler=sampler)\n",
                "    return dataloader\n",
                "\n",
                "# -------------------------------\n",
                "# 3. Training Loop\n",
                "# -------------------------------\n",
                "def train(rank, world_size):\n",
                "    setup(rank, world_size)\n",
                "\n",
                "    device = torch.device(f\"cuda:{rank}\")\n",
                "    model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\").to(device)\n",
                "    model = DDP(model, device_ids=[rank])\n",
                "\n",
                "    dataloader = get_dataloader(batch_size=8, rank=rank, world_size=world_size)\n",
                "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
                "\n",
                "    model.train()\n",
                "    for epoch in range(1):  # Increase for real training\n",
                "        total_loss = 0\n",
                "        for step, batch in enumerate(dataloader):\n",
                "            input_ids = batch[\"input_ids\"].to(device)\n",
                "            attention_mask = batch[\"attention_mask\"].to(device)\n",
                "\n",
                "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
                "            loss = outputs.loss\n",
                "\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "\n",
                "            total_loss += loss.item()\n",
                "\n",
                "            if step % 100 == 0 and rank == 0:\n",
                "                print(f\"[GPU {rank}] Step {step}, Loss: {loss.item():.4f}\")\n",
                "\n",
                "        if rank == 0:\n",
                "            print(f\"✅ Epoch complete. Rank {rank} Avg Loss: {total_loss/len(dataloader):.4f}\")\n",
                "\n",
                "    cleanup()\n",
                "\n",
                "# -------------------------------\n",
                "# 4. Multiprocessing Launch\n",
                "# -------------------------------\n",
                "def main():\n",
                "    world_size = torch.cuda.device_count()\n",
                "    if world_size < 2:\n",
                "        print(\"❌ Requires at least 2 GPUs for DDP.\")\n",
                "        return\n",
                "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
