{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# reimplementation_pytorch.ipynb\n",
                "\n",
                "# -----------------------------\n",
                "# 1. Imports & Config\n",
                "# -----------------------------\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(\"Using device:\", device)\n",
                "\n",
                "# -----------------------------\n",
                "# 2. Positional Encoding (Sinusoidal)\n",
                "# -----------------------------\n",
                "class PositionalEncoding(nn.Module):\n",
                "    def __init__(self, dim, max_len=2048):\n",
                "        super().__init__()\n",
                "        pe = torch.zeros(max_len, dim)\n",
                "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
                "        div = torch.exp(torch.arange(0, dim, 2) * (-math.log(10000.0) / dim))\n",
                "        pe[:, 0::2] = torch.sin(pos * div)\n",
                "        pe[:, 1::2] = torch.cos(pos * div)\n",
                "        pe = pe.unsqueeze(0)  # (1, max_len, dim)\n",
                "        self.register_buffer('pe', pe)\n",
                "\n",
                "    def forward(self, x):\n",
                "        return x + self.pe[:, :x.size(1)]\n",
                "\n",
                "# -----------------------------\n",
                "# 3. Multi-Head Attention (no FlashAttention for now)\n",
                "# -----------------------------\n",
                "class MultiHeadSelfAttention(nn.Module):\n",
                "    def __init__(self, dim, heads):\n",
                "        super().__init__()\n",
                "        self.heads = heads\n",
                "        self.scale = (dim // heads) ** -0.5\n",
                "\n",
                "        self.qkv = nn.Linear(dim, dim * 3)\n",
                "        self.out_proj = nn.Linear(dim, dim)\n",
                "\n",
                "    def forward(self, x):\n",
                "        B, T, C = x.size()\n",
                "        qkv = self.qkv(x)  # (B, T, 3C)\n",
                "        q, k, v = qkv.chunk(3, dim=-1)\n",
                "\n",
                "        q = q.view(B, T, self.heads, C // self.heads).transpose(1, 2)\n",
                "        k = k.view(B, T, self.heads, C // self.heads).transpose(1, 2)\n",
                "        v = v.view(B, T, self.heads, C // self.heads).transpose(1, 2)\n",
                "\n",
                "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
                "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
                "        out = attn_weights @ v\n",
                "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
                "\n",
                "        return self.out_proj(out)\n",
                "\n",
                "# -----------------------------\n",
                "# 4. Transformer Block\n",
                "# -----------------------------\n",
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, dim, heads, ff_dim, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.ln1 = nn.LayerNorm(dim)\n",
                "        self.attn = MultiHeadSelfAttention(dim, heads)\n",
                "        self.ln2 = nn.LayerNorm(dim)\n",
                "        self.ff = nn.Sequential(\n",
                "            nn.Linear(dim, ff_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(ff_dim, dim),\n",
                "        )\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = x + self.dropout(self.attn(self.ln1(x)))\n",
                "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
                "        return x\n",
                "\n",
                "# -----------------------------\n",
                "# 5. Transformer Language Model\n",
                "# -----------------------------\n",
                "class MiniLM(nn.Module):\n",
                "    def __init__(self, vocab_size, dim=512, depth=6, heads=8, ff_dim=2048, max_len=512):\n",
                "        super().__init__()\n",
                "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
                "        self.pos_enc = PositionalEncoding(dim, max_len=max_len)\n",
                "        self.blocks = nn.Sequential(*[\n",
                "            TransformerBlock(dim, heads, ff_dim) for _ in range(depth)\n",
                "        ])\n",
                "        self.ln_f = nn.LayerNorm(dim)\n",
                "        self.head = nn.Linear(dim, vocab_size)\n",
                "\n",
                "    def forward(self, idx):\n",
                "        x = self.token_emb(idx)\n",
                "        x = self.pos_enc(x)\n",
                "        x = self.blocks(x)\n",
                "        x = self.ln_f(x)\n",
                "        logits = self.head(x)\n",
                "        return logits\n",
                "\n",
                "# -----------------------------\n",
                "# 6. Inference / Sample Usage\n",
                "# -----------------------------\n",
                "vocab_size = 50257\n",
                "model = MiniLM(vocab_size).to(device)\n",
                "\n",
                "sample_input = torch.randint(0, vocab_size, (1, 32)).to(device)\n",
                "logits = model(sample_input)\n",
                "print(\"Output shape:\", logits.shape)  # (1, 32, vocab_size)\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
