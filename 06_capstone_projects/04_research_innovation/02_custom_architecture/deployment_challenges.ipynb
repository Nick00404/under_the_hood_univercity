{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# deployment_challenges.ipynb\n",
                "\n",
                "# -------------------------------\n",
                "# 1. Setup & Imports\n",
                "# -------------------------------\n",
                "!pip install transformers onnx onnxruntime -q\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
                "import onnx\n",
                "import onnxruntime as ort\n",
                "import time\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(\"Running on:\", device)\n",
                "\n",
                "# -------------------------------\n",
                "# 2. Load & Prepare Model\n",
                "# -------------------------------\n",
                "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
                "model.eval()\n",
                "\n",
                "text = \"Artificial intelligence is\"\n",
                "inputs = tokenizer(text, return_tensors=\"pt\")\n",
                "input_ids = inputs[\"input_ids\"].to(device)\n",
                "\n",
                "# -------------------------------\n",
                "# 3. Export to ONNX\n",
                "# -------------------------------\n",
                "onnx_path = \"gpt2_export.onnx\"\n",
                "\n",
                "torch.onnx.export(\n",
                "    model,\n",
                "    (input_ids,),\n",
                "    onnx_path,\n",
                "    input_names=[\"input_ids\"],\n",
                "    output_names=[\"logits\"],\n",
                "    dynamic_axes={\"input_ids\": {0: \"batch\", 1: \"sequence\"}},\n",
                "    do_constant_folding=True,\n",
                "    opset_version=13\n",
                ")\n",
                "\n",
                "print(f\"âœ… Model exported to {onnx_path}\")\n",
                "\n",
                "# -------------------------------\n",
                "# 4. Inference with ONNX Runtime\n",
                "# -------------------------------\n",
                "def onnx_infer(onnx_path, input_ids):\n",
                "    session = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
                "    ort_inputs = {\"input_ids\": input_ids.cpu().numpy()}\n",
                "    start = time.time()\n",
                "    ort_outs = session.run(None, ort_inputs)\n",
                "    end = time.time()\n",
                "    return ort_outs[0], end - start\n",
                "\n",
                "onnx_logits, onnx_time = onnx_infer(onnx_path, input_ids)\n",
                "print(\"âš¡ ONNX Inference Time (CPU): {:.4f}s\".format(onnx_time))\n",
                "\n",
                "# -------------------------------\n",
                "# 5. Compare with PyTorch Inference\n",
                "# -------------------------------\n",
                "start = time.time()\n",
                "with torch.no_grad():\n",
                "    torch_logits = model(input_ids).logits\n",
                "end = time.time()\n",
                "print(\"âš¡ PyTorch Inference Time (GPU): {:.4f}s\".format(end - start))\n",
                "\n",
                "# -------------------------------\n",
                "# 6. Deployment Challenges Summary\n",
                "# -------------------------------\n",
                "from IPython.display import Markdown\n",
                "\n",
                "Markdown(\"\"\"\n",
                "### ðŸ§  Deployment Challenges Overview\n",
                "\n",
                "| Challenge            | Notes |\n",
                "|----------------------|-------|\n",
                "| **Model Size**       | GPT-2 (500MB), difficult for edge |\n",
                "| **ONNX Accuracy**    | Matches FP32, but slow without GPU EP |\n",
                "| **Quantization**     | ONNX supports INT8 â€” needs calibration |\n",
                "| **Tokenization I/O** | Slowest part often not the model, but token I/O |\n",
                "| **Runtime Support**  | ONNX > ONNX Runtime > Edge device support |\n",
                "\"\"\")\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
