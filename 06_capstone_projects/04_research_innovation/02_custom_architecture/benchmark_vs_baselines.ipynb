{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# benchmark_vs_baselines.ipynb\n",
                "\n",
                "# -------------------------------\n",
                "# 1. Setup and Imports\n",
                "# -------------------------------\n",
                "!pip install transformers datasets evaluate accelerate -q\n",
                "\n",
                "import torch\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM, AutoTokenizer, \n",
                "    GPT2LMHeadModel, GPT2TokenizerFast\n",
                ")\n",
                "import time\n",
                "import evaluate\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(\"Using device:\", device)\n",
                "\n",
                "# -------------------------------\n",
                "# 2. Load Models\n",
                "# -------------------------------\n",
                "\n",
                "# Baseline model: GPT-2\n",
                "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
                "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
                "\n",
                "# Custom model: use MiniLM or plug in your own\n",
                "from torch import nn\n",
                "\n",
                "class CustomMiniTransformer(nn.Module):\n",
                "    def __init__(self, vocab_size=50257, dim=256, depth=4, heads=4, ff_dim=1024):\n",
                "        super().__init__()\n",
                "        self.embed = nn.Embedding(vocab_size, dim)\n",
                "        self.blocks = nn.Sequential(*[\n",
                "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=ff_dim, batch_first=True)\n",
                "            for _ in range(depth)\n",
                "        ])\n",
                "        self.ln = nn.LayerNorm(dim)\n",
                "        self.head = nn.Linear(dim, vocab_size)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.embed(x)\n",
                "        x = self.blocks(x)\n",
                "        x = self.ln(x)\n",
                "        return self.head(x)\n",
                "\n",
                "custom_model = CustomMiniTransformer().to(device)\n",
                "print(\"âœ… Models loaded.\")\n",
                "\n",
                "# -------------------------------\n",
                "# 3. Inference Benchmark\n",
                "# -------------------------------\n",
                "def benchmark_model(model, tokenizer, prompt, max_new_tokens=50):\n",
                "    model.eval()\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                "    with torch.no_grad():\n",
                "        start = time.time()\n",
                "        output = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
                "        end = time.time()\n",
                "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
                "    return decoded, end - start\n",
                "\n",
                "prompt = \"The rise of artificial general intelligence will\"\n",
                "print(\"ðŸ§ª Running GPT-2...\")\n",
                "gpt2_out, gpt2_time = benchmark_model(gpt2_model, gpt2_tokenizer, prompt)\n",
                "\n",
                "print(\"ðŸ§ª Running Custom Transformer...\")\n",
                "tokenizer = gpt2_tokenizer  # Reuse GPT-2 tokenizer\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
                "with torch.no_grad():\n",
                "    start = time.time()\n",
                "    logits = custom_model(inputs)\n",
                "    end = time.time()\n",
                "\n",
                "print(\"ðŸ“‰ GPT-2 Time: {:.3f}s\".format(gpt2_time))\n",
                "print(\"ðŸ“‰ Custom Model Time: {:.3f}s\".format(end - start))\n",
                "\n",
                "# -------------------------------\n",
                "# 4. Evaluation (Perplexity)\n",
                "# -------------------------------\n",
                "ppl_eval = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
                "\n",
                "def calc_ppl(model, tokenizer):\n",
                "    model.eval()\n",
                "    return ppl_eval.compute(model_id=model, add_start_token=True, batch_size=4)\n",
                "\n",
                "# This works only with HuggingFace models\n",
                "gpt2_ppl = calc_ppl(\"gpt2\", gpt2_tokenizer)\n",
                "print(\"ðŸ§  GPT-2 Perplexity:\", gpt2_ppl)\n",
                "\n",
                "# -------------------------------\n",
                "# 5. Summary Table\n",
                "# -------------------------------\n",
                "import pandas as pd\n",
                "\n",
                "results = pd.DataFrame([\n",
                "    {\"Model\": \"GPT-2\", \"Time (s)\": gpt2_time, \"Perplexity\": gpt2_ppl[\"perplexity\"]},\n",
                "    {\"Model\": \"Custom\", \"Time (s)\": end - start, \"Perplexity\": \"N/A\"},\n",
                "])\n",
                "\n",
                "results\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
