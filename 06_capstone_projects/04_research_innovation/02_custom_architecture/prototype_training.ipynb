{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# prototype_training.ipynb\n",
                "\n",
                "# -------------------------------\n",
                "# 1. Setup & Install\n",
                "# -------------------------------\n",
                "!pip install transformers datasets accelerate -q\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import DataLoader\n",
                "from datasets import load_dataset\n",
                "from transformers import GPT2TokenizerFast\n",
                "from torch.optim import AdamW\n",
                "from tqdm import tqdm\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(\"Running on:\", device)\n",
                "\n",
                "# -------------------------------\n",
                "# 2. Define Custom Transformer\n",
                "# -------------------------------\n",
                "class MiniTransformer(nn.Module):\n",
                "    def __init__(self, vocab_size=50257, dim=256, depth=4, heads=4, ff_dim=1024):\n",
                "        super().__init__()\n",
                "        self.embed = nn.Embedding(vocab_size, dim)\n",
                "        self.blocks = nn.Sequential(*[\n",
                "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=ff_dim, batch_first=True)\n",
                "            for _ in range(depth)\n",
                "        ])\n",
                "        self.ln = nn.LayerNorm(dim)\n",
                "        self.head = nn.Linear(dim, vocab_size)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.embed(x)\n",
                "        x = self.blocks(x)\n",
                "        x = self.ln(x)\n",
                "        return self.head(x)\n",
                "\n",
                "model = MiniTransformer().to(device)\n",
                "\n",
                "# -------------------------------\n",
                "# 3. Load Dataset & Tokenizer\n",
                "# -------------------------------\n",
                "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
                "\n",
                "def tokenize_fn(example):\n",
                "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
                "\n",
                "tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
                "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
                "loader = DataLoader(tokenized_dataset, batch_size=8, shuffle=True)\n",
                "\n",
                "# -------------------------------\n",
                "# 4. Training Loop\n",
                "# -------------------------------\n",
                "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
                "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
                "\n",
                "model.train()\n",
                "epochs = 1\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    total_loss = 0\n",
                "    for batch in tqdm(loader):\n",
                "        input_ids = batch[\"input_ids\"].to(device)\n",
                "        labels = input_ids.clone()\n",
                "\n",
                "        logits = model(input_ids)\n",
                "        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    print(f\"ðŸ§  Epoch {epoch+1} complete. Avg loss: {total_loss / len(loader):.4f}\")\n",
                "\n",
                "# -------------------------------\n",
                "# 5. Save Model (Optional)\n",
                "# -------------------------------\n",
                "torch.save(model.state_dict(), \"custom_transformer.pt\")\n",
                "print(\"âœ… Model saved.\")\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
