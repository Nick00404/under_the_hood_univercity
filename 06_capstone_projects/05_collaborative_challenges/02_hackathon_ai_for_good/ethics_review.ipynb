{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ethics_review.ipynb\n",
                "\n",
                "# -------------------------------\n",
                "# 1. Setup\n",
                "# -------------------------------\n",
                "!pip install transformers datasets -q\n",
                "\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "from datasets import load_dataset\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Load your fine-tuned or baseline model\n",
                "model_name = \"distilbert-base-multilingual-cased\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(model_name).eval()\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "model.to(device)\n",
                "\n",
                "# -------------------------------\n",
                "# 2. Load & Analyze Dataset\n",
                "# -------------------------------\n",
                "dataset = load_dataset(\"civil_comments\", split=\"validation[:1000]\")\n",
                "labels = dataset[\"toxicity\"]\n",
                "\n",
                "def classify(texts):\n",
                "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "        probs = torch.softmax(outputs.logits, dim=-1)\n",
                "    return probs[:, 1].cpu().numpy()\n",
                "\n",
                "dataset = dataset.map(lambda x: {\"model_score\": classify([x[\"text\"]])[0]})\n",
                "print(\"✅ Model scores added\")\n",
                "\n",
                "# -------------------------------\n",
                "# 3. Bias Audit by Demographic\n",
                "# -------------------------------\n",
                "import pandas as pd\n",
                "\n",
                "df = dataset.to_pandas()\n",
                "sensitive_groups = [\"female\", \"male\", \"black\", \"white\", \"LGBTQ\", \"muslim\", \"christian\"]\n",
                "\n",
                "bias_scores = {}\n",
                "for group in sensitive_groups:\n",
                "    mask = df[group] > 0.5\n",
                "    bias_scores[group] = df[mask][\"model_score\"].mean()\n",
                "\n",
                "bias_df = pd.DataFrame.from_dict(bias_scores, orient=\"index\", columns=[\"Avg Toxicity Score\"])\n",
                "bias_df.sort_values(\"Avg Toxicity Score\", ascending=False, inplace=True)\n",
                "\n",
                "# -------------------------------\n",
                "# 4. Visualize Bias Scores\n",
                "# -------------------------------\n",
                "sns.barplot(x=bias_df.index, y=\"Avg Toxicity Score\", data=bias_df)\n",
                "plt.title(\"⚠️ Bias Detection – Avg Toxicity Score by Demographic\")\n",
                "plt.xticks(rotation=45)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# -------------------------------\n",
                "# 5. Mitigation Plan (Markdown)\n",
                "# -------------------------------\n",
                "from IPython.display import Markdown\n",
                "\n",
                "Markdown(\"\"\"\n",
                "### ✅ Ethical Risk Review Summary\n",
                "\n",
                "**Bias Detected**: The model shows higher toxicity scores for certain groups.  \n",
                "**Action**:\n",
                "- Fine-tune on balanced, inclusive datasets (e.g., Jigsaw Demographic Balancer)\n",
                "- Apply post-processing thresholds per demographic\n",
                "- Flag high-risk responses for human review\n",
                "\n",
                "**Fairness Goals**:\n",
                "- Maintain demographic parity in false positive rates\n",
                "- Use explainability (e.g., SHAP, LIME) for edge-case analysis\n",
                "- Document data sources, annotation criteria\n",
                "\n",
                "**Harm Prevention**:\n",
                "- Model runs behind a safety filter\n",
                "- No storage of personal data\n",
                "- Explainability UI offered for human review\n",
                "\n",
                "\"\"\")\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
