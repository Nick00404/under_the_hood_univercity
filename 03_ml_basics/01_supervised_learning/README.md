# 01 Supervised Learning

- [01 linear regression and cost functions](./01_linear_regression_and_cost_functions.ipynb)
- [02 logistic regression and classification metrics](./02_logistic_regression_and_classification_metrics.ipynb)
- [03 decision trees and ensemble methods](./03_decision_trees_and_ensemble_methods.ipynb)
- [04 svm and kernel tricks for nonlinear data](./04_svm_and_kernel_tricks_for_nonlinear_data.ipynb)
- [05 regularization l1 l2 elasticnet](./05_regularization_l1_l2_elasticnet.ipynb)
- [06 bayesian models and naive bayes](./06_bayesian_models_and_naive_bayes.ipynb)



## ðŸ“˜ `01_linear_regression_and_cost_functions.ipynb`

### ðŸ§© **1. Building Blocks of Linear Regression**
- Hypothesis Function
- Line Fitting (Geometric intuition)
- Assumptions of Linear Models

### ðŸ§© **2. Cost Function & Optimization**
- Squared Error / MSE
- Gradient Descent (Single variable)
- Gradient Descent (Multivariable)
- Vectorization for Speedup

### ðŸ§© **3. Evaluation & Interpretation**
- RÂ² Score
- Underfitting & Model Diagnostics
- Visualizing Cost Surface

---

## ðŸ“˜ `02_logistic_regression_and_classification_metrics.ipynb`

### ðŸ§© **1. Understanding Logistic Regression**
- Binary Classification Motivation
- Sigmoid Function & Probability Output
- Decision Boundary Interpretation

### ðŸ§© **2. Training the Model**
- Cost Function for Logistic Regression
- Gradient Descent for Logistic Regression
- Feature Scaling

### ðŸ§© **3. Evaluation & Performance**
- Accuracy, Precision, Recall, F1
- Confusion Matrix
- ROC Curve & AUC
- Overfitting in Classification Models

---

## ðŸ“˜ `03_decision_trees_and_ensemble_methods.ipynb`

### ðŸ§© **1. Decision Trees Explained**
- Splitting Criteria: Gini vs Entropy
- Tree Depth & Pruning
- Overfitting in Trees

### ðŸ§© **2. Ensemble Techniques**
- Bagging & Random Forests
- Feature Importance
- Boosting Basics (GBM, XGBoost)

### ðŸ§© **3. Model Tuning & Comparison**
- Hyperparameters for Forests & Boosters
- When to Use Trees vs Linear Models
- Bias-Variance Tradeoff Visualized

---

## ðŸ“˜ `04_svm_and_kernel_tricks_for_nonlinear_data.ipynb`

### ðŸ§© **1. Core Concepts of SVM**
- Max-Margin Intuition
- Hard vs Soft Margins
- Hinge Loss Function

### ðŸ§© **2. Going Nonlinear with Kernels**
- Polynomial & RBF Kernels
- Visualizing Transformations
- Kernelized Decision Boundaries

### ðŸ§© **3. Practical Usage**
- Parameter Tuning: C and Gamma
- Linear vs Non-linear SVM
- Comparison with Logistic Regression

---

## ðŸ“˜ `05_regularization_l1_l2_elasticnet.ipynb`

### ðŸ§© **1. Motivation & Math of Regularization**
- Overfitting Intuition
- Regularized Cost Functions
- Effect of Î» on Loss

### ðŸ§© **2. Types of Regularization**
- L2 (Ridge)
- L1 (Lasso)
- ElasticNet (Combining L1 + L2)

### ðŸ§© **3. Practical Model Fitting**
- Regularization in Scikit-Learn
- Cross-Validation for Î» Selection
- Visual Demos: Shrinkage of Weights

---

## ðŸ“˜ `06_bayesian_models_and_naive_bayes.ipynb`

### ðŸ§© **1. Foundations of Bayesian Thinking**
- Bayes Theorem Refresher
- Likelihood vs Prior vs Posterior
- Probabilistic Classification Intuition

### ðŸ§© **2. Naive Bayes Classifiers**
- Gaussian, Multinomial, Bernoulli
- Conditional Independence Assumption
- When Naive Bayes Works Well

### ðŸ§© **3. Evaluation & Usage**
- Use Cases (Spam, Sentiment, etc.)
- Comparison to Logistic Regression
- Performance on Imbalanced Data

---
