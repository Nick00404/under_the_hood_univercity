{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Absolutely. Let‚Äôs enter the realm of anomaly detection, where the unusual is what we want. Here's your UTHU-style breakdown of:\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Isolation Forest Algorithm**  \n",
                "üå≤ *Random Partitioning, Path Length, and Scoring Outliers via Isolation Depth*\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "Most ML models try to **predict common patterns** ‚Äî but in anomaly detection, we want the opposite:  \n",
                "Find the **weird**, **rare**, and **unexpected**.\n",
                "\n",
                "**Isolation Forest** is built for this. It doesn't cluster or model data ‚Äî it literally tries to **isolate** each point using random decision trees. Outliers get isolated **faster** than normal points.\n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine finding a celebrity in a crowd. You don‚Äôt need a whole biography ‚Äî just a few quick questions (‚ÄúIs he 7 feet tall?‚Äù ‚ÄúIs she wearing a red cape?‚Äù).  \n",
                "> **Weird traits = short path to isolation.**\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman Explanation |\n",
                "|------|---------------------|\n",
                "| **Isolation Forest** | A collection of trees that randomly split data to isolate points |\n",
                "| **Path Length** | How many splits it takes to isolate a point |\n",
                "| **Anomaly Score** | Shorter path ‚Üí more likely to be an outlier |\n",
                "| **Random Subsampling** | Randomly choosing feature + split value at each tree node |\n",
                "| **Tree Ensemble** | Using many trees to get a reliable average path length |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- Financial fraud detection (rare transactions)\n",
                "- Intrusion detection in networks\n",
                "- Industrial monitoring (e.g., sensor drift)\n",
                "- Outlier rejection before training supervised models\n",
                "\n",
                "```plaintext\n",
                "    Have unlabeled data?\n",
                "            ‚Üì\n",
                "   Want to flag rare points?\n",
                "            ‚Üì\n",
                "     ‚Üí Use Isolation Forest\n",
                "            ‚Üì\n",
                "     Fast isolation ‚Üí likely outlier\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Core Equation\n",
                "\n",
                "Let \\( h(x) \\) be the average path length to isolate point \\( x \\), and \\( c(n) \\) be the average path length in a random BST of \\( n \\) samples:\n",
                "\n",
                "- **Anomaly Score**:\n",
                "  $$\n",
                "  s(x, n) = 2^{-\\frac{h(x)}{c(n)}}\n",
                "  $$\n",
                "\n",
                "Where:\n",
                "- \\( s(x, n) \\to 1 \\) ‚Üí strong anomaly (isolated quickly)  \n",
                "- \\( s(x, n) \\to 0.5 \\) ‚Üí normal instance\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "Random splits isolate extreme values fast:\n",
                "- Outliers ‚Üí isolated in **few splits** ‚Üí **short path**  \n",
                "- Normal points ‚Üí take **more splits** ‚Üí **longer path**\n",
                "\n",
                "This is different from density methods (like LOF) ‚Äî Isolation Forest doesn't estimate distance or density, just *splittability*.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                            | Pitfalls                             |\n",
                "|--------------------------------------|--------------------------------------|\n",
                "| Outliers are few and separable       | Fails if anomalies cluster together  |\n",
                "| Features can be split meaningfully   | Poor performance on flat/noisy data  |\n",
                "| Input features are **independent**   | Correlated features may confuse splits |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Feature               | Isolation Forest                 | Other Methods (LOF, One-Class SVM)  |\n",
                "|-----------------------|----------------------------------|--------------------------------------|\n",
                "| Speed                 | Extremely fast (random trees)    | Slower (distance or kernel based)    |\n",
                "| Interpretability      | Medium (path length logic)       | Low                                  |\n",
                "| Scaling to Big Data   | Excellent                        | Poor                                 |\n",
                "| Non-Euclidean Spaces  | Good                             | Poor                                 |\n",
                "| Handles High Dimensions| Yes                             | No (SVMs fail with curse of dim.)    |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- **False positives** can occur in underrepresented groups if their behavior appears ‚Äúunusual‚Äù to the model\n",
                "- Always validate flagged anomalies with **human review**, especially in finance or health contexts\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **SCiForest**: Stream-compatible Isolation Forest  \n",
                "- **Hybrid models**: Combine Isolation Forest with LSTM for time-series anomaly detection  \n",
                "- **Explainable Anomaly Detection**: Feature attribution for outlier score (e.g., SHAP + iForest)\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: Why do outliers tend to have shorter path lengths in Isolation Forests?**\n",
                "\n",
                "A. They are always smaller in value  \n",
                "B. They get split later in trees  \n",
                "C. They‚Äôre far from dense clusters, so isolated faster  \n",
                "D. They have higher reconstruction error\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: Outliers lie on the fringes and get split off quickly ‚Äî so they have short paths in many trees.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Debug Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: incorrect scoring method\n",
                "scores = model.predict(X_test)  # returns labels, not scores\n",
                "```\n",
                "\n",
                "**Fix:**\n",
                "\n",
                "```python\n",
                "scores = model.decision_function(X_test)  # Higher = more normal\n",
                "anomaly_score = -scores  # Invert for anomaly interpretation\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Meaning |\n",
                "|------|--------|\n",
                "| **Isolation Forest** | Ensemble of trees built to isolate points |\n",
                "| **Path Length** | Number of splits needed to isolate a data point |\n",
                "| **Anomaly Score** | Normalized value showing how quickly a point was isolated |\n",
                "| **Decision Function** | Outputs raw anomaly scores |\n",
                "| **Random Partitioning** | Splitting data randomly instead of optimizing like in decision trees |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Hyperparameters\n",
                "\n",
                "| Param             | Description                          | Tip                              |\n",
                "|------------------|--------------------------------------|----------------------------------|\n",
                "| `n_estimators`   | Number of trees                      | 100‚Äì200 usually enough           |\n",
                "| `max_samples`    | Size of subsample per tree           | 256 is default and efficient     |\n",
                "| `contamination`  | % of expected outliers               | Set manually if known (e.g., 0.01) |\n",
                "\n",
                "```python\n",
                "model = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Precision, Recall, F1** (if labeled anomalies available)  \n",
                "- **ROC-AUC** (binary classification)  \n",
                "- **Visual threshold tuning** on score distribution\n",
                "\n",
                "```python\n",
                "from sklearn.metrics import roc_auc_score\n",
                "roc_auc_score(y_true, -model.decision_function(X_test))\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Always **normalize** data before fitting (e.g., `StandardScaler`)\n",
                "- Visualize **score distributions** to tune decision threshold\n",
                "- Combine with **domain rules** or **time-based filtering** in deployment\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.ensemble import IsolationForest\n",
                "from sklearn.datasets import make_blobs\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Generate synthetic dataset with outliers\n",
                "X, _ = make_blobs(n_samples=300, centers=1, cluster_std=1.0, random_state=42)\n",
                "X_outliers = np.random.uniform(low=-6, high=6, size=(20, 2))\n",
                "X_combined = np.vstack([X, X_outliers])\n",
                "\n",
                "# Normalize\n",
                "X_scaled = StandardScaler().fit_transform(X_combined)\n",
                "\n",
                "# Fit Isolation Forest\n",
                "model = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
                "model.fit(X_scaled)\n",
                "\n",
                "# Get anomaly scores\n",
                "scores = -model.decision_function(X_scaled)\n",
                "labels = model.predict(X_scaled)\n",
                "\n",
                "# Plot results\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=scores, cmap='coolwarm', s=30, edgecolors='k')\n",
                "plt.colorbar(label=\"Anomaly Score (Higher = More Outlier)\")\n",
                "plt.title(\"Anomaly Detection with Isolation Forest\")\n",
                "plt.xlabel(\"Feature 1\")\n",
                "plt.ylabel(\"Feature 2\")\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "All done: Isolation Forest, explained and deployed üî¨  \n",
                "Next: Want to compare it with One-Class SVM or LOF? Or jump into use cases and real-data examples like **fraud detection**?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Perfect. Time to **compare Isolation Forest** with its two most notable anomaly detection cousins:  \n",
                "üîç **One-Class SVM** and **Local Outlier Factor (LOF)**.\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Comparison with Other Methods**  \n",
                "üéØ *Isolation Forest vs One-Class SVM vs LOF*  \n",
                "(UTHU-style structured breakdown)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "Isolation Forest is **fast and tree-based**, but it‚Äôs not the only way to find anomalies.\n",
                "\n",
                "Let‚Äôs contrast it with:\n",
                "- **One-Class SVM (OCSVM)** ‚Äî a kernel-based method that learns a decision boundary  \n",
                "- **Local Outlier Factor (LOF)** ‚Äî a density-based method that compares how crowded your neighborhood is\n",
                "\n",
                "> **Analogy**:  \n",
                "> - Isolation Forest: ‚ÄúHow fast can I isolate you with random questions?‚Äù  \n",
                "> - One-Class SVM: ‚ÄúDo you fall inside or outside the normal bubble?‚Äù  \n",
                "> - LOF: ‚ÄúAre your neighbors unusually far away?‚Äù\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term              | Feynman-style Explanation |\n",
                "|-------------------|---------------------------|\n",
                "| **One-Class SVM** | Learns a boundary around ‚Äúnormal‚Äù ‚Äî flags anything outside it |\n",
                "| **LOF**           | Compares how far you are from your neighbors vs how far they are from theirs |\n",
                "| **Kernel Method** | Projects data to higher space to make it separable |\n",
                "| **Density Score** | LOF score indicating isolation in sparse areas |\n",
                "| **Decision Boundary** | SVM's invisible line separating normal from abnormal |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "| Task Type                     | Best Method        |\n",
                "|-------------------------------|--------------------|\n",
                "| Huge datasets (millions)      | ‚úÖ Isolation Forest |\n",
                "| Small, complex feature sets   | ‚úÖ One-Class SVM    |\n",
                "| Detecting local group outliers| ‚úÖ LOF              |\n",
                "| Interpretability needed       | ‚úÖ LOF (neighborhood logic) |\n",
                "\n",
                "```plaintext\n",
                "     Need to detect outliers?\n",
                "             ‚Üì\n",
                "    +----------------------------+\n",
                "    | High-dimensional, big data ‚Üí Isolation Forest\n",
                "    | Small dataset, tight clusters ‚Üí One-Class SVM\n",
                "    | Neighborhood-based outliers ‚Üí LOF\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Key Equations\n",
                "\n",
                "#### **One-Class SVM**:\n",
                "Finds a function \\( f(x) \\) such that:\n",
                "$$\n",
                "f(x) > 0 \\Rightarrow \\text{inlier}, \\quad f(x) < 0 \\Rightarrow \\text{outlier}\n",
                "$$\n",
                "\n",
                "Uses kernel trick \\( K(x, x') \\) to define separation boundary.\n",
                "\n",
                "#### **LOF**:\n",
                "Outlier score is the **ratio of average local density** of neighbors vs self:\n",
                "$$\n",
                "\\text{LOF}(x) = \\frac{\\sum_{i \\in N_k(x)} \\frac{\\text{density}(i)}{\\text{density}(x)}}{|N_k(x)|}\n",
                "$$\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "| Model           | ‚ÄúHow it thinks‚Äù |\n",
                "|----------------|------------------|\n",
                "| **Isolation Forest** | Outliers split off quickly ‚Üí low path length |\n",
                "| **OCSVM**            | Fit a tight hypersphere ‚Üí everything outside is weird |\n",
                "| **LOF**              | Compare local density ‚Üí if your area is empty, you‚Äôre weird |\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Method            | Works Best When...                 | Pitfalls                             |\n",
                "|-------------------|------------------------------------|--------------------------------------|\n",
                "| Isolation Forest  | High-dimensional, unlabeled, large data | Fails with correlated, subtle anomalies |\n",
                "| One-Class SVM     | Few features, smooth boundaries    | Doesn‚Äôt scale, fails in high dims     |\n",
                "| LOF               | Local context is meaningful        | Sensitive to `k`, hard to parallelize |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Feature               | Isolation Forest       | One-Class SVM       | LOF                   |\n",
                "|------------------------|------------------------|----------------------|------------------------|\n",
                "| Speed & Scalability    | ‚úÖ Fast, parallelizable | ‚ùå Slow, scales poorly| ‚ùå Medium              |\n",
                "| Handles High Dim Data  | ‚úÖ Yes                 | ‚ùå Poorly             | ‚ùå Poorly              |\n",
                "| Local Outliers         | ‚ùå Not ideal           | ‚ùå Weak               | ‚úÖ Excellent           |\n",
                "| Interpretable Logic    | Medium                | Low                  | ‚úÖ High (neighborhood) |\n",
                "| Hyperparam Sensitivity | Low                   | ‚úÖ High (nu, gamma)   | ‚úÖ High (k neighbors)  |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- LOF and OCSVM may **miss global anomalies** that look normal locally  \n",
                "- Isolation Forest might **flag legitimate minority behaviors** as anomalies  \n",
                "- Always validate anomaly results with **domain experts or post-hoc interpretable tools**\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **Deep SVDD**: One-Class SVM generalized with deep networks  \n",
                "- **LOF + Embeddings**: Works better when applied to **latent space** from an autoencoder  \n",
                "- **Isolation Forest + SHAP**: Used to explain why a point was marked anomalous  \n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: Which method is best for detecting anomalies within dense clusters?**\n",
                "\n",
                "A. Isolation Forest  \n",
                "B. One-Class SVM  \n",
                "C. Local Outlier Factor  \n",
                "D. PCA\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: LOF compares local densities, making it ideal for spotting points that are unusual *within* a cluster.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Debug\n",
                "\n",
                "```python\n",
                "# Buggy: using One-Class SVM without scaling\n",
                "from sklearn.svm import OneClassSVM\n",
                "model = OneClassSVM(kernel='rbf').fit(X)\n",
                "```\n",
                "\n",
                "**Fix:**\n",
                "\n",
                "```python\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "X_scaled = StandardScaler().fit_transform(X)\n",
                "model = OneClassSVM(kernel='rbf').fit(X_scaled)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **One-Class SVM** | Finds boundary around ‚Äúnormal‚Äù data using support vectors |\n",
                "| **LOF (Local Outlier Factor)** | Flags points with low local density |\n",
                "| **Isolation Forest** | Detects anomalies by random splitting |\n",
                "| **Kernel Trick** | Projects data into higher dimensions for separation |\n",
                "| **Contamination** | Assumed percentage of anomalies in data |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Hyperparameters\n",
                "\n",
                "| Model | Key Params         | Heuristics                                |\n",
                "|--------|--------------------|-------------------------------------------|\n",
                "| iForest | `n_estimators`, `contamination` | Use 100 trees, set contamination if known |\n",
                "| OCSVM  | `nu`, `gamma`       | `nu=0.05`, `gamma=1/n_features` works well |\n",
                "| LOF    | `n_neighbors`       | `k=20` is common; tune based on data shape |\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Precision/Recall** for flagged outliers  \n",
                "- **ROC AUC** if true labels are available  \n",
                "- Use **score distributions** to set thresholds\n",
                "\n",
                "```python\n",
                "from sklearn.metrics import roc_auc_score\n",
                "roc_auc_score(y_true, -lof.negative_outlier_factor_)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç  \n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_blobs\n",
                "from sklearn.ensemble import IsolationForest\n",
                "from sklearn.neighbors import LocalOutlierFactor\n",
                "from sklearn.svm import OneClassSVM\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Generate synthetic dataset\n",
                "X, _ = make_blobs(n_samples=300, centers=1, cluster_std=1.0, random_state=42)\n",
                "X_outliers = np.random.uniform(low=-6, high=6, size=(20, 2))\n",
                "X_combined = np.vstack([X, X_outliers])\n",
                "X_scaled = StandardScaler().fit_transform(X_combined)\n",
                "\n",
                "# Fit models\n",
                "models = {\n",
                "    \"Isolation Forest\": IsolationForest(contamination=0.05).fit(X_scaled),\n",
                "    \"One-Class SVM\": OneClassSVM(nu=0.05, kernel='rbf').fit(X_scaled),\n",
                "    \"Local Outlier Factor\": LocalOutlierFactor(n_neighbors=20)\n",
                "}\n",
                "\n",
                "# Get predictions\n",
                "labels = {\n",
                "    \"Isolation Forest\": models[\"Isolation Forest\"].predict(X_scaled),\n",
                "    \"One-Class SVM\": models[\"One-Class SVM\"].predict(X_scaled),\n",
                "    \"Local Outlier Factor\": models[\"Local Outlier Factor\"].fit_predict(X_scaled)\n",
                "}\n",
                "\n",
                "# Plot comparisons\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "for i, (name, lbls) in enumerate(labels.items()):\n",
                "    axes[i].scatter(X_scaled[:, 0], X_scaled[:, 1], c=lbls, cmap='coolwarm', s=30, edgecolors='k')\n",
                "    axes[i].set_title(f\"{name}\")\n",
                "    axes[i].grid(True)\n",
                "\n",
                "plt.suptitle(\"Anomaly Detection: IF vs OCSVM vs LOF\", fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You now have a comparative framework to choose the **right anomaly detector** for the job.  \n",
                "Next in queue: Want to roll into **real-world use cases** (fraud/system anomalies), or explore interpretability tools like **SHAP for outliers**?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let‚Äôs land this anomaly detection plane with the **why-it-matters** piece:  \n",
                "üõ°Ô∏è **Real-World Use Cases of Isolation Forest and Anomaly Detection**\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Use Cases**  \n",
                "üîç *Fraud Detection, System Monitoring, and Beyond*  \n",
                "(*UTHU-structured summary*)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "Anomalies may be **rare**, but they‚Äôre often **the most important events** in your data:\n",
                "- A single fraudulent transaction\n",
                "- One failing machine\n",
                "- A strange login from a foreign country\n",
                "\n",
                "Isolation Forest and its anomaly detection cousins exist **specifically to find these needles in haystacks** ‚Äî and do it **without needing labels**.\n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine a security guard trained not on every criminal ever, but simply on spotting anyone who acts just a bit too‚Ä¶ different.  \n",
                "> That's what unsupervised anomaly detection does.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Explanation |\n",
                "|------|-------------|\n",
                "| **Fraud Detection** | Spotting financial or behavioral events that deviate from norms |\n",
                "| **System Monitoring** | Tracking sensors, servers, or users for unusual activity |\n",
                "| **Concept Drift** | When normal behavior changes over time (e.g., seasonality) |\n",
                "| **Outlier** | A data point that doesn‚Äôt conform to the expected pattern |\n",
                "| **Online Detection** | Real-time anomaly flagging during data stream processing |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Primary Use Cases\n",
                "\n",
                "---\n",
                "\n",
                "### üè¶ 1. **Fraud Detection**\n",
                "\n",
                "**Why it fits**:\n",
                "- Fraud is usually **rare**, **subtle**, and **evolving**\n",
                "- Isolation Forest works well with **high-dimensional**, unlabeled transaction logs\n",
                "\n",
                "**Common Targets**:\n",
                "- Credit card fraud (small % of transactions are fake)\n",
                "- Insurance fraud (anomalous claims)\n",
                "- E-commerce bots (abnormal browsing + purchases)\n",
                "\n",
                "```plaintext\n",
                "Transaction Logs ‚Üí Feature Vectors ‚Üí Isolation Forest ‚Üí Outlier Scores ‚Üí Flag frauds\n",
                "```\n",
                "\n",
                "**Note**: Labels are often delayed (i.e., fraud confirmed days later), so unsupervised detection is key.\n",
                "\n",
                "---\n",
                "\n",
                "### üõ†Ô∏è 2. **System Monitoring**\n",
                "\n",
                "**Why it fits**:\n",
                "- Systems are mostly stable ‚Äî anomalies often indicate **failures**, **attacks**, or **deviations**\n",
                "- Isolation Forest is fast and **stream-friendly**\n",
                "\n",
                "**Common Targets**:\n",
                "- Server CPU/memory spikes  \n",
                "- Sensor drift or hardware degradation  \n",
                "- Network intrusions (e.g., abnormal IP behavior)\n",
                "\n",
                "```plaintext\n",
                "Sensor Logs or API Events ‚Üí Sliding Windows ‚Üí Anomaly Scores ‚Üí Alert or Auto-response\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### üë§ 3. **User Behavior Analytics (UBA)**\n",
                "\n",
                "- Find **insider threats** (employees accessing strange files)\n",
                "- Detect **credential theft** via login patterns\n",
                "- Spot **outlier usage** in SaaS products (unusual click paths)\n",
                "\n",
                "---\n",
                "\n",
                "### üö® 4. **Security & Intrusion Detection**\n",
                "\n",
                "- Real-time detection of **network anomalies**  \n",
                "- Complement to rule-based firewalls (more adaptive)\n",
                "- Works in **zero-day attacks** where no labeled data exists\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ 5. **Preprocessing for Supervised Models**\n",
                "\n",
                "- Clean training sets by removing **label noise**\n",
                "- Filter outliers from regression datasets\n",
                "- Identify mislabeled samples for review\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "The **core insight** for all these use cases:\n",
                "$$\n",
                "s(x) = 2^{-\\frac{h(x)}{c(n)}} \\Rightarrow \\text{Lower } h(x) \\text{ ‚Üí Higher anomaly score}\n",
                "$$\n",
                "\n",
                "You can:\n",
                "- Set thresholds (e.g., top 1% of scores)\n",
                "- Use scores for **ranking**, **visualization**, or **alerting**\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Use Case              | Why Isolation Forest Works       | Caveats / Challenges             |\n",
                "|------------------------|----------------------------------|----------------------------------|\n",
                "| Fraud Detection        | Handles high-dim, sparse data    | Fraud adapts ‚Äî retrain often     |\n",
                "| System Monitoring      | Works in real-time, fast scoring | Static model may miss drift      |\n",
                "| Insider Threats        | No need for labeled ‚Äúbad users‚Äù  | False positives if behavior varies |\n",
                "| Sensor Outlier Cleaning| Captures rare, irregular spikes | Edge case anomalies may be lost  |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- False positives can **lock out legit users** (security) or **delay payouts** (insurance)\n",
                "- Models may flag behavior from **minority groups** as abnormal simply due to underrepresentation\n",
                "- Always pair anomaly detection with:\n",
                "  - Human review  \n",
                "  - Clear audit trail  \n",
                "  - Fairness checks\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **Anomaly explanation**: Using SHAP or LIME to explain outlier scores  \n",
                "- **Drift-aware detectors**: Combine Isolation Forest with concept drift monitoring  \n",
                "- **Hybrid pipelines**: Use anomaly scores as inputs to supervised fraud classifiers\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: Why is Isolation Forest effective in fraud detection tasks?**\n",
                "\n",
                "A. It uses labels to detect rare cases  \n",
                "B. It models every user behavior directly  \n",
                "C. It isolates rare behaviors quickly without labels  \n",
                "D. It requires deep neural networks\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: Isolation Forest doesn‚Äôt need labels and is built to quickly split off unusual data ‚Äî perfect for rare-event detection like fraud.\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Anomaly Detection** | Finding rare, unusual, or suspicious data points |\n",
                "| **Fraud** | Intentionally deceptive behavior in finance, insurance, or systems |\n",
                "| **System Monitoring** | Watching for performance or behavioral failures |\n",
                "| **Concept Drift** | When ‚Äúnormal‚Äù behavior changes over time |\n",
                "| **Outlier Score** | Value indicating how ‚Äúweird‚Äù a point is, based on model logic |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Hyperparameters for Deployment\n",
                "\n",
                "- `contamination`: Based on expected fraud/anomaly rate (e.g., `0.01`)\n",
                "- Use **retraining** on rolling windows (daily/weekly)\n",
                "- Always log **score + timestamp + context** for forensic review\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Precision / Recall** (for fraud or incident tickets)  \n",
                "- **Time to detect** (real-time system monitoring)  \n",
                "- **Manual review accuracy** (if human audit included)\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Use Isolation Forest in **streaming mode** with mini-batches  \n",
                "- For dynamic data, consider **online retraining or rolling windows**\n",
                "- Flag outliers, but **don‚Äôt auto-punish** ‚Äî route to analyst workflow\n",
                "\n",
                "---\n",
                "\n",
                "All done ‚Äî you‚Äôve now covered the **full loop** of Isolation Forest: theory ‚Üí math ‚Üí code ‚Üí comparison ‚Üí use cases.\n",
                "\n",
                "Next move: want to wrap this with a mini project / case study (e.g., synthetic fraud dataset), or move into **Manifold Learning** with t-SNE + UMAP?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let‚Äôs close the Isolation Forest chapter with a full **hands-on walkthrough**:  \n",
                "üí∏ **Detecting Rare Transactions in Financial Data** ‚Äî an end-to-end example using Isolation Forest.\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Example ‚Äì Detecting Rare Transactions in Financial Data**  \n",
                "(*UTHU-structured summary*)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "In real-world financial data:\n",
                "- Fraudulent transactions are **extremely rare** (e.g., <1%)  \n",
                "- We often don‚Äôt have labels in advance  \n",
                "- Outliers often come from new fraud patterns ‚Üí supervised models fail\n",
                "\n",
                "Isolation Forest is **ideal** here:\n",
                "- Doesn‚Äôt need labels  \n",
                "- Fast on large transaction logs  \n",
                "- Finds anomalies by **splitting off the weird**\n",
                "\n",
                "> **Analogy**:  \n",
                "> Picture thousands of credit card swipes.  \n",
                "> Most are normal.  \n",
                "> But one person just bought 6 Rolexes at 2AM from Tokyo.  \n",
                "> Isolation Forest raises its hand and says, ‚ÄúThat‚Äôs... different.‚Äù\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman-style Explanation |\n",
                "|------|---------------------------|\n",
                "| **Transaction Vector** | All features of a transaction (amount, time, device, etc.) |\n",
                "| **Anomaly Score** | Measure of how quickly a transaction was isolated |\n",
                "| **Contamination Rate** | Fraction of data expected to be fraud |\n",
                "| **Threshold** | Score cutoff to decide fraud or not |\n",
                "| **False Positive** | Legit transaction wrongly flagged as fraud |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Typical Features in Transaction Data\n",
                "\n",
                "- `transaction_amount`\n",
                "- `transaction_hour`\n",
                "- `merchant_type` (encoded)\n",
                "- `location_distance` from home\n",
                "- `device_trust_score`\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Core Logic\n",
                "\n",
                "Isolation Forest estimates anomaly score using:\n",
                "$$\n",
                "s(x) = 2^{-\\frac{h(x)}{c(n)}}\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( h(x) \\): Average path length in trees\n",
                "- \\( c(n) \\): Expected path length in random tree\n",
                "- \\( s(x) \\to 1 \\) ‚Üí Very anomalous transaction\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "- **High transaction amount + untrusted device + foreign country** = isolated quickly in trees  \n",
                "- **Low score** = looks like other transactions  \n",
                "- **High score** = odd, rare, suspect\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "- Fraud ‚â† clustered ‚Üí IF works better than LOF  \n",
                "- Needs normalized input features  \n",
                "- Will have **false positives** ‚Äî use with analyst workflow\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Hyperparameters\n",
                "\n",
                "```python\n",
                "model = IsolationForest(\n",
                "    n_estimators=100,\n",
                "    contamination=0.01,  # Assume ~1% fraud\n",
                "    random_state=42\n",
                ")\n",
                "```\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- If labels available: **precision, recall, ROC AUC**\n",
                "- If not: sample flagged transactions for **manual review**\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Critical Analysis** üîç\n",
                "\n",
                "| Benefit                        | Risk                                |\n",
                "|-------------------------------|--------------------------------------|\n",
                "| Fast, scalable to big data    | Can flag legitimate large purchases |\n",
                "| No labels required            | May miss subtle frauds in dense regions |\n",
                "| Easy to plug into workflows   | Needs regular retraining as fraud evolves |\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Fraud Score** | Model's confidence that a transaction is an anomaly |\n",
                "| **Manual Review** | Analyst checks flagged transactions |\n",
                "| **Path Length** | How many splits to isolate a point in a tree |\n",
                "| **False Alarm** | Legit transaction marked as fraud |\n",
                "| **Rolling Retrain** | Model is updated regularly to track new fraud patterns |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.ensemble import IsolationForest\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Simulated transaction dataset\n",
                "np.random.seed(42)\n",
                "n_legit = 1000\n",
                "n_fraud = 15\n",
                "\n",
                "# Legit transactions\n",
                "legit = pd.DataFrame({\n",
                "    'amount': np.random.normal(50, 10, n_legit),\n",
                "    'hour': np.random.normal(13, 3, n_legit),\n",
                "    'distance': np.random.normal(5, 2, n_legit),\n",
                "    'device_trust': np.random.normal(0.9, 0.05, n_legit)\n",
                "})\n",
                "\n",
                "# Fraudulent transactions (outliers)\n",
                "fraud = pd.DataFrame({\n",
                "    'amount': np.random.normal(300, 50, n_fraud),\n",
                "    'hour': np.random.normal(3, 1, n_fraud),\n",
                "    'distance': np.random.normal(50, 10, n_fraud),\n",
                "    'device_trust': np.random.normal(0.3, 0.1, n_fraud)\n",
                "})\n",
                "\n",
                "# Combine and scale\n",
                "data = pd.concat([legit, fraud], ignore_index=True)\n",
                "labels = np.array([0]*n_legit + [1]*n_fraud)  # 1 = fraud\n",
                "X_scaled = StandardScaler().fit_transform(data)\n",
                "\n",
                "# Fit Isolation Forest\n",
                "model = IsolationForest(contamination=0.015, random_state=42)\n",
                "model.fit(X_scaled)\n",
                "scores = -model.decision_function(X_scaled)\n",
                "preds = model.predict(X_scaled)  # -1 = anomaly, 1 = normal\n",
                "\n",
                "# Plot results\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.hist(scores[:n_legit], bins=50, alpha=0.6, label='Legit')\n",
                "plt.hist(scores[n_legit:], bins=10, alpha=0.9, label='Fraud')\n",
                "plt.axvline(np.percentile(scores, 98.5), color='red', linestyle='--', label='Threshold')\n",
                "plt.xlabel(\"Anomaly Score\")\n",
                "plt.ylabel(\"Number of Transactions\")\n",
                "plt.title(\"Anomaly Scores: Legit vs Fraud\")\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "# Print performance\n",
                "from sklearn.metrics import classification_report\n",
                "mapped_preds = np.where(preds == -1, 1, 0)  # Map to 1 = fraud\n",
                "print(classification_report(labels, mapped_preds, target_names=[\"Legit\", \"Fraud\"]))\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You‚Äôve just deployed an **unsupervised fraud detection model** ‚Äî from feature simulation to anomaly scoring, thresholding, and evaluation.\n",
                "\n",
                "Ready to head into the next UTHU chapter:  \n",
                "üß† **Manifold Learning with UMAP + t-SNE**?"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
