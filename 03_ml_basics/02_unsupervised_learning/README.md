# 02 Unsupervised Learning

- [01 kmeans clustering and elbow method](./01_kmeans_clustering_and_elbow_method.ipynb)
- [02 hierarchical clustering and dendrograms](./02_hierarchical_clustering_and_dendrograms.ipynb)
- [03 pca for dimensionality reduction](./03_pca_for_dimensionality_reduction.ipynb)
- [04 autoencoders for feature learning](./04_autoencoders_for_feature_learning.ipynb)
- [05 anomaly detection with isolation forests](./05_anomaly_detection_with_isolation_forests.ipynb)
- [06 manifold learning with umap tsne](./06_manifold_learning_with_umap_tsne.ipynb)

---

## ðŸ“˜ **02. Unsupervised Learning â€“ Structured Index**

---

### ðŸ§© **01. K-Means Clustering and Elbow Method**

#### ðŸ“Œ Subtopics:
- Introduction to K-Means  
  - Centroid-based clustering  
  - Distance metrics (Euclidean, cosine, etc.)  
  - Random initialization and k-means++  
- Elbow Method for Optimal `k`  
  - Within-cluster sum of squares (WCSS)  
  - Scree plot for determining the best number of clusters  
- Example: Segmenting customer data with K-Means

---

### ðŸ§© **02. Hierarchical Clustering and Dendrograms**

#### ðŸ“Œ Subtopics:
- Introduction to Hierarchical Clustering  
  - Agglomerative vs Divisive methods  
- Linkage Criteria  
  - Single, complete, average, Wardâ€™s method  
- Dendrogram Interpretation  
  - Cutting the tree to form clusters  
- Example: Visualizing clustering hierarchy on Iris dataset

---

### ðŸ§© **03. PCA for Dimensionality Reduction**

#### ðŸ“Œ Subtopics:
- What is PCA?  
  - Covariance matrix, eigenvectors, and eigenvalues  
- Explained Variance  
  - Choosing the number of components  
  - Scree plot analysis  
- PCA vs Feature Selection  
  - When to reduce dimensions vs selecting features  
- Example: Applying PCA on high-dimensional image data

---

### ðŸ§© **04. Autoencoders for Feature Learning**

#### ðŸ“Œ Subtopics:
- Autoencoder Architecture  
  - Encoder-decoder structure  
  - Bottleneck layers and latent space  
- Loss Functions  
  - Reconstruction loss (MSE, BCE)  
- Use Cases  
  - Dimensionality reduction, denoising  
- Example: Learning compressed representations of MNIST digits

---

### ðŸ§© **05. Anomaly Detection with Isolation Forests**

#### ðŸ“Œ Subtopics:
- Isolation Forest Algorithm  
  - Random partitioning, path length  
  - Scoring outliers via isolation depth  
- Comparison with Other Methods  
  - One-Class SVM, LOF  
- Use Cases  
  - Fraud detection, system monitoring  
- Example: Detecting rare transactions in financial data

---

### ðŸ§© **06. Manifold Learning with UMAP and t-SNE**

#### ðŸ“Œ Subtopics:
- What is Manifold Learning?  
  - Non-linear dimensionality reduction  
- t-SNE  
  - Pairwise similarity and perplexity  
  - High-dimensional to 2D embeddings  
- UMAP  
  - Topological structure preservation  
  - Performance and interpretability  
- Example: Visualizing complex embeddings from NLP models

---
