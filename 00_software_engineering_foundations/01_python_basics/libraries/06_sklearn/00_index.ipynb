{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527e9852",
   "metadata": {},
   "source": [
    "\n",
    "### **01_classification_basics.ipynb**\n",
    "\n",
    "1. **Introduction to Classification**\n",
    "   - What is classification?  \n",
    "   - Binary vs multiclass problems  \n",
    "   - Overview of the Scikit-learn API\n",
    "\n",
    "2. **Dataset Preparation**\n",
    "   - Loading datasets (`load_iris`, `make_classification`)  \n",
    "   - Splitting data (`train_test_split`)  \n",
    "   - Feature scaling (`StandardScaler`)\n",
    "\n",
    "3. **Training a Basic Classifier**\n",
    "   - Fitting models: `LogisticRegression`, `KNeighborsClassifier`, etc.  \n",
    "   - Predicting and understanding outputs  \n",
    "   - Class probability predictions (`predict_proba()`)\n",
    "\n",
    "4. **Decision Boundaries**\n",
    "   - Plotting classification regions  \n",
    "   - Visualizing decision surfaces\n",
    "\n",
    "5. **Basic Evaluation Metrics**\n",
    "   - Accuracy score  \n",
    "   - Confusion matrix  \n",
    "   - Classification report (precision, recall, F1-score)\n",
    "\n",
    "6. **Handling Imbalanced Datasets**\n",
    "   - Class weights  \n",
    "   - Stratified splitting  \n",
    "   - Resampling (optional intro)\n",
    "\n",
    "---\n",
    "\n",
    "### **02_regression_pipeline.ipynb**\n",
    "\n",
    "1. **Regression Problem Setup**\n",
    "   - What is regression?  \n",
    "   - Loading regression datasets (e.g., `load_boston`, synthetic)\n",
    "\n",
    "2. **Linear Regression Models**\n",
    "   - `LinearRegression`  \n",
    "   - `Ridge`, `Lasso` for regularization  \n",
    "   - Polynomial regression with `PolynomialFeatures`\n",
    "\n",
    "3. **Scikit-learn Pipelines**\n",
    "   - Creating a preprocessing + modeling pipeline  \n",
    "   - Scaling, polynomial features, and model in one step  \n",
    "   - `Pipeline()` and `make_pipeline()`\n",
    "\n",
    "4. **Pipeline Advantages**\n",
    "   - Clean code structure  \n",
    "   - Prevention of data leakage  \n",
    "   - Grid search compatibility\n",
    "\n",
    "5. **Model Evaluation**\n",
    "   - Regression metrics: MSE, RMSE, MAE, RÂ²  \n",
    "   - Train vs test performance comparison  \n",
    "   - Residual plots\n",
    "\n",
    "---\n",
    "\n",
    "### **03_cross_validation.ipynb**\n",
    "\n",
    "1. **Why Cross-Validation Matters**\n",
    "   - Bias-variance trade-off  \n",
    "   - Overfitting detection  \n",
    "   - Evaluation beyond train/test split\n",
    "\n",
    "2. **Cross-Validation Methods**\n",
    "   - `KFold`, `StratifiedKFold`, `ShuffleSplit`  \n",
    "   - `cross_val_score()` and `cross_validate()`  \n",
    "   - Scoring options\n",
    "\n",
    "3. **Using CV in Pipelines**\n",
    "   - Integrating with `Pipeline`  \n",
    "   - `cross_val_score()` on a pipeline  \n",
    "\n",
    "4. **Nested Cross-Validation**\n",
    "   - Hyperparameter tuning inside CV  \n",
    "   - Avoiding leakage during tuning\n",
    "\n",
    "5. **Visualization of CV Results**\n",
    "   - Boxplots of scores  \n",
    "   - Mean and std visualization  \n",
    "   - Fold-wise performance breakdown\n",
    "\n",
    "---\n",
    "\n",
    "### **04_metrics_visualization.ipynb**\n",
    "\n",
    "1. **Classification Metrics and Visualization**\n",
    "   - Confusion matrix (heatmap)  \n",
    "   - ROC curve & AUC  \n",
    "   - Precision-Recall curve  \n",
    "   - Visualizing thresholds\n",
    "\n",
    "2. **Regression Metrics and Visualization**\n",
    "   - Residual plot  \n",
    "   - Prediction error plot  \n",
    "   - Actual vs predicted scatter plot\n",
    "\n",
    "3. **Advanced Evaluation Tools**\n",
    "   - `classification_report` as a DataFrame  \n",
    "   - `sklearn.metrics.plot_*()` utilities  \n",
    "   - Visualizing overfitting with learning curves\n",
    "\n",
    "4. **Custom Metric Functions**\n",
    "   - Creating custom scoring functions for CV  \n",
    "   - Using `make_scorer()`\n",
    "\n",
    "---\n",
    "\n",
    "### **05_model_tuning.ipynb**\n",
    "\n",
    "1. **Overview of Hyperparameter Tuning**\n",
    "   - What are hyperparameters?  \n",
    "   - Search strategies overview  \n",
    "\n",
    "2. **Grid Search**\n",
    "   - Using `GridSearchCV`  \n",
    "   - Param grid definition  \n",
    "   - Best estimator extraction\n",
    "\n",
    "3. **Randomized Search**\n",
    "   - `RandomizedSearchCV` for faster exploration  \n",
    "   - Defining distributions and iteration control  \n",
    "\n",
    "4. **Tuning with Pipelines**\n",
    "   - Tuning preprocessing and model together  \n",
    "   - Nested parameter names in `param_grid`\n",
    "\n",
    "5. **Evaluation during Tuning**\n",
    "   - Cross-validation during search  \n",
    "   - Scoring metrics selection  \n",
    "   - Analyzing `cv_results_`\n",
    "\n",
    "6. **Visualization of Tuning Results**\n",
    "   - Heatmaps for parameter scores  \n",
    "   - Line plots and validation curves  \n",
    "   - Best parameter vs performance trade-offs\n",
    "\n",
    "7. **Final Model Deployment**\n",
    "   - Refitting on full data  \n",
    "   - Exporting model with `joblib`  \n",
    "   - Prediction on new/test data\n",
    "\n",
    "\n",
    "## **06_advanced_sklearn.ipynb**\n",
    "\n",
    "1. **Introduction to Advanced Scikit-Learn**\n",
    "   - Recap of core algorithms and pipelines  \n",
    "   - Motivation for deepening model and feature engineering techniques  \n",
    "\n",
    "2. **Ensemble Methods Deep Dive**\n",
    "   - Advanced ensemble techniques: bagging, boosting, stacking  \n",
    "   - Hyperparameter considerations for ensemble models (e.g., Random Forests, Gradient Boosting)  \n",
    "   - Best practices in ensemble design and interpretation\n",
    "\n",
    "3. **Advanced Hyperparameter Optimization**\n",
    "   - Beyond grid and randomized search: Bayesian optimization and meta-learning  \n",
    "   - Automated hyperparameter tuning frameworks  \n",
    "   - Cross-validation strategies for robust tuning  \n",
    "\n",
    "4. **Feature Selection and Dimensionality Reduction**\n",
    "   - Techniques for feature importance estimation (permutation importance, LASSO-based methods)  \n",
    "   - Advanced feature selection: Recursive Feature Elimination (RFE), SelectFromModel  \n",
    "   - Dimensionality reduction methods (PCA, t-SNE, UMAP) for preprocessing\n",
    "\n",
    "5. **Model Interpretability and Diagnostics**\n",
    "   - Tools for model interpretability: Partial Dependence Plots, SHAP, and LIME in-depth  \n",
    "   - Error analysis and residual diagnostics  \n",
    "   - Strategies for detecting and handling overfitting/underfitting\n",
    "\n",
    "6. **Advanced Pipeline Integration**\n",
    "   - Creating custom transformers and estimators  \n",
    "   - Building multi-stage pipelines for complex workflows  \n",
    "   - Strategies for pipeline automation and reproducibility\n",
    "\n",
    "7. **Case Studies and Best Practices**\n",
    "   - End-to-end projects showcasing advanced model building  \n",
    "   - Discussion of trade-offs in model complexity versus interpretability  \n",
    "   - Lessons learned from real-world applications\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd9b2c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
