{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6680c1f",
   "metadata": {},
   "source": [
    "\n",
    "### **01_tensors_basics.ipynb**\n",
    "\n",
    "1. **Introduction to PyTorch**\n",
    "   - What is PyTorch and why is it popular?  \n",
    "   - Overview of tensor operations  \n",
    "   - PyTorch vs TensorFlow: Strengths of PyTorch\n",
    "\n",
    "2. **Tensor Basics**\n",
    "   - What is a tensor?  \n",
    "   - Creating tensors from lists, NumPy arrays, and random distributions  \n",
    "   - Tensor types and their differences (CPU vs GPU tensors)  \n",
    "   - Tensor properties: `dtype`, `shape`, `size`\n",
    "\n",
    "3. **Tensor Operations**\n",
    "   - Basic operations: addition, subtraction, multiplication  \n",
    "   - Indexing and slicing tensors  \n",
    "   - Reshaping and flattening tensors  \n",
    "   - Element-wise operations and broadcasting\n",
    "\n",
    "4. **CUDA Tensors**\n",
    "   - Moving tensors to GPU: `tensor.cuda()`  \n",
    "   - Checking GPU availability: `torch.cuda.is_available()`  \n",
    "   - Performing operations on GPU tensors  \n",
    "   - Transferring tensors between CPU and GPU\n",
    "\n",
    "5. **Tensor Manipulations and Aggregation**\n",
    "   - `torch.cat()`, `torch.stack()` for combining tensors  \n",
    "   - Sum, mean, min, max: `torch.sum()`, `torch.mean()`, etc.  \n",
    "   - Sorting and unique operations\n",
    "\n",
    "---\n",
    "\n",
    "### **02_autograd_and_backprop.ipynb**\n",
    "\n",
    "1. **Autograd in PyTorch**\n",
    "   - Understanding automatic differentiation  \n",
    "   - `torch.autograd` overview  \n",
    "   - Computing gradients automatically for tensor operations\n",
    "\n",
    "2. **Setting Up Tensors for Autograd**\n",
    "   - Creating tensors with `requires_grad=True`  \n",
    "   - Basic example of gradient computation  \n",
    "   - Viewing gradients: `.grad` attribute\n",
    "\n",
    "3. **Backpropagation in PyTorch**\n",
    "   - The process of backpropagation  \n",
    "   - Calling `backward()` to compute gradients  \n",
    "   - The role of `.grad` in gradient storage  \n",
    "   - Zeroing gradients with `optimizer.zero_grad()`\n",
    "\n",
    "4. **Gradient Accumulation**\n",
    "   - Avoiding gradient accumulation across iterations  \n",
    "   - Manually controlling gradient accumulation\n",
    "\n",
    "5. **Custom Gradient Computation**\n",
    "   - Using `torch.autograd.Function` for custom gradient calculations  \n",
    "   - Implementing forward and backward passes for custom operations\n",
    "\n",
    "6. **Practical Example**\n",
    "   - Defining a simple neural network from scratch  \n",
    "   - Running forward and backward passes manually\n",
    "\n",
    "---\n",
    "\n",
    "### **03_nn_training_loop.ipynb**\n",
    "\n",
    "1. **Neural Networks in PyTorch**\n",
    "   - Introduction to `torch.nn.Module`  \n",
    "   - Understanding layers, forward pass, and parameters\n",
    "\n",
    "2. **Building a Simple Neural Network**\n",
    "   - Creating a custom model class inheriting from `nn.Module`  \n",
    "   - Defining layers: `nn.Linear()`, `nn.ReLU()`, `nn.Softmax()`  \n",
    "   - Forward pass method and output\n",
    "\n",
    "3. **Loss Functions**\n",
    "   - Common loss functions: `nn.CrossEntropyLoss()`, `nn.MSELoss()`  \n",
    "   - Using loss functions with model outputs  \n",
    "   - Loss reduction methods (mean, sum)\n",
    "\n",
    "4. **Optimizers**\n",
    "   - Using optimizers: `torch.optim.SGD()`, `torch.optim.Adam()`  \n",
    "   - Optimizer setup and parameter updates  \n",
    "   - Learning rate and scheduler adjustments\n",
    "\n",
    "5. **Training a Model**\n",
    "   - Looping over epochs and batches  \n",
    "   - Forward pass, loss computation, backward pass, optimizer step  \n",
    "   - Printing and tracking loss during training\n",
    "\n",
    "6. **Model Evaluation**\n",
    "   - Evaluating model on validation/test data  \n",
    "   - Tracking accuracy and other metrics  \n",
    "   - Overfitting and early stopping techniques\n",
    "\n",
    "7. **Saving and Loading Models**\n",
    "   - Saving model weights with `torch.save()`  \n",
    "   - Loading saved models with `torch.load()`  \n",
    "   - Model checkpoints during training\n",
    "\n",
    "---\n",
    "\n",
    "### **04_cnn_example.ipynb**\n",
    "\n",
    "1. **Convolutional Neural Networks (CNNs) Overview**\n",
    "   - What is a CNN and its importance in computer vision?  \n",
    "   - Difference between CNNs and fully connected networks\n",
    "\n",
    "2. **Building a Simple CNN**\n",
    "   - Layers involved in CNNs: `nn.Conv2d()`, `nn.MaxPool2d()`, `nn.ReLU()`, `nn.Flatten()`, `nn.Linear()`  \n",
    "   - Creating custom CNN class inheriting from `nn.Module`  \n",
    "   - Structuring convolutional layers and pooling layers\n",
    "\n",
    "3. **Understanding Kernels, Strides, Padding**\n",
    "   - Explanation of convolution operations  \n",
    "   - How kernel size, stride, and padding affect feature maps\n",
    "\n",
    "4. **Training CNNs**\n",
    "   - Applying CNN training loop principles (same as NN)  \n",
    "   - Special considerations for image data processing  \n",
    "   - Data augmentation for better generalization\n",
    "\n",
    "5. **Evaluating CNNs**\n",
    "   - Using confusion matrices and accuracy metrics for CNNs  \n",
    "   - Analyzing model performance on unseen data  \n",
    "   - Overfitting and regularization strategies\n",
    "\n",
    "6. **Transfer Learning with CNNs**\n",
    "   - Using pre-trained CNN models (e.g., VGG16, ResNet)  \n",
    "   - Freezing layers and fine-tuning for custom tasks  \n",
    "   - Implementing custom output layers for classification\n",
    "\n",
    "---\n",
    "\n",
    "### **05_custom_datasets.ipynb**\n",
    "\n",
    "1. **Introduction to Custom Datasets in PyTorch**\n",
    "   - Understanding `torch.utils.data.Dataset` and `DataLoader`  \n",
    "   - When to use custom datasets\n",
    "\n",
    "2. **Creating a Custom Dataset Class**\n",
    "   - Subclassing `torch.utils.data.Dataset`  \n",
    "   - Implementing `__len__()` and `__getitem__()` methods  \n",
    "   - Reading from images, CSV files, or custom data formats\n",
    "\n",
    "3. **Loading Data with DataLoader**\n",
    "   - Using `DataLoader` for batching and shuffling  \n",
    "   - `collate_fn` for custom batch creation  \n",
    "   - Parallel data loading with `num_workers`\n",
    "\n",
    "4. **Handling Data Transformations**\n",
    "   - Applying transformations on data with `torchvision.transforms`  \n",
    "   - Normalization, augmentation, and other image operations\n",
    "\n",
    "5. **Working with Different Data Types**\n",
    "   - Handling text, image, and tabular data with custom datasets  \n",
    "   - Tokenizing text data for NLP tasks  \n",
    "   - Preprocessing images for CNN models\n",
    "\n",
    "6. **Efficient Data Management**\n",
    "   - Caching datasets for faster access  \n",
    "   - Using `torchvision.datasets` as templates  \n",
    "   - Handling large datasets that don't fit in memory\n",
    "\n",
    "\n",
    "## **06_advanced_pytorch.ipynb**\n",
    "\n",
    "1. **Introduction to Advanced PyTorch**\n",
    "   - Recap of core tensor operations, autograd, and basic network training  \n",
    "   - Objectives for advancing toward more complex scenarios\n",
    "\n",
    "2. **Custom Autograd and Dynamic Computation Graphs**\n",
    "   - Developing custom autograd Functions for unique operations  \n",
    "   - Debugging gradient flow and troubleshooting backpropagation issues  \n",
    "   - Dynamic computation graphs and control flow in PyTorch\n",
    "\n",
    "3. **Advanced Model Architectures and Custom Layers**\n",
    "   - Creating complex custom layers and modules  \n",
    "   - Architectures for non-standard tasks (e.g., attention mechanisms, graph neural networks)  \n",
    "   - Integrating custom loss functions and metrics\n",
    "\n",
    "4. **Distributed and Parallel Training Techniques**\n",
    "   - Overview of distributed training with `torch.distributed`  \n",
    "   - Multi-GPU and multi-node training strategies  \n",
    "   - Techniques for synchronizing and scaling training processes\n",
    "\n",
    "5. **Optimization Techniques and Mixed Precision Training**\n",
    "   - Advanced optimizer strategies (learning rate schedulers, weight decay, etc.)  \n",
    "   - Mixed precision training and performance considerations  \n",
    "   - Memory optimization and profiling tools (e.g., `torch.utils.bottleneck`)\n",
    "\n",
    "6. **Debugging, Profiling, and Interpretability**\n",
    "   - Tools and methods for debugging PyTorch models  \n",
    "   - Profiling and performance analysis  \n",
    "   - Techniques for model interpretability (e.g., integrated gradients, feature visualization)\n",
    "\n",
    "7. **Practical Projects and Best Practices**\n",
    "   - Real-world case studies applying advanced PyTorch techniques  \n",
    "   - Lessons on fine-tuning and managing model complexity  \n",
    "   - Best practices in deploying advanced models\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b4b2b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
