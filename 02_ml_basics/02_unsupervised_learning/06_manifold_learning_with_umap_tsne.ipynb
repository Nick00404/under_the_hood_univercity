{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Absolutely ‚Äî let‚Äôs open the doors to one of the most mind-bending, shape-shifting ideas in modern ML:  \n",
                "üß† **Manifold Learning** ‚Äî the foundation of t-SNE, UMAP, and much more.\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **What is Manifold Learning?**  \n",
                "üåå *Nonlinear Dimensionality Reduction Explained*  \n",
                "(UTHU-structured summary)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "In many real-world datasets (like images, speech, or embeddings), the **data doesn‚Äôt really live in a full-dimensional space** ‚Äî it lives on a **curved, lower-dimensional shape** inside that space. That shape is a **manifold**.\n",
                "\n",
                "**Manifold Learning** helps us:\n",
                "- **Flatten** this curved space to something we can **visualize**\n",
                "- Keep the **structure** intact (like neighborhoods, clusters)\n",
                "- Reduce noise and uncover hidden structure\n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine a crumpled piece of paper (3D) that you flatten onto a desk (2D).  \n",
                "> If you preserve the **ink patterns** on the page during flattening, you've done manifold learning.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman Explanation |\n",
                "|------|---------------------|\n",
                "| **Manifold** | A smooth, curved surface hidden inside high-dimensional space (e.g., digits on a number line inside pixel space) |\n",
                "| **Nonlinear Reduction** | Finding a curved shortcut instead of a straight-line projection like PCA |\n",
                "| **Local Structure** | Preserving who‚Äôs near whom in the dataset |\n",
                "| **Embedding** | A mapping from high-dimensional to low-dimensional space |\n",
                "| **Geodesic Distance** | Distance *along* the surface of the manifold, not through space directly (like walking on Earth's surface vs tunneling through it)\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- Visualizing word embeddings (e.g., BERT, Word2Vec)  \n",
                "- Clustering gene expression or biology data  \n",
                "- Unsupervised learning on high-dimensional time-series  \n",
                "- Recommender system patterns and latent customer behavior  \n",
                "- Image style and pose variations (e.g., digits, faces)\n",
                "\n",
                "```plaintext\n",
                "     High-Dimensional Embeddings\n",
                "                ‚Üì\n",
                "     Do you want to see or cluster patterns?\n",
                "                ‚Üì\n",
                "         ‚Üí Use Manifold Learning (UMAP, t-SNE)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Core Equation Concept\n",
                "\n",
                "While not model-specific like PCA, manifold learning generally tries to:\n",
                "- Preserve pairwise **local distances** (neighbors stay neighbors)\n",
                "- Find a mapping \\( f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d \\) where \\( d \\ll D \\)\n",
                "\n",
                "#### Example: Distance Preservation Objective (e.g., MDS-inspired)\n",
                "Let \\( d_{ij} \\) = original distance, \\( \\hat{d}_{ij} \\) = projected distance:\n",
                "$$\n",
                "\\min \\sum_{i,j} (d_{ij} - \\hat{d}_{ij})^2\n",
                "$$\n",
                "\n",
                "But unlike PCA:\n",
                "- Distance isn‚Äôt **Euclidean**, but **geodesic**  \n",
                "- The optimization is nonlinear, often **iterative** and **stochastic**\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "If your data lies on a **twisty donut shape** (manifold), PCA will **slice through** it linearly ‚Äî losing the meaning.  \n",
                "Manifold learning algorithms walk *along* the surface to unfold it into 2D, like peeling an orange into a flat peel.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                          | Pitfalls                             |\n",
                "|-------------------------------------|--------------------------------------|\n",
                "| Data lies on a low-D manifold       | Doesn't scale well to millions       |\n",
                "| Local distances ‚âà meaningful        | Global structure may be distorted    |\n",
                "| High-dimensional noise is minimal   | Highly noisy data can distort map    |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Strengths                           | Weaknesses                              |\n",
                "|------------------------------------|------------------------------------------|\n",
                "| Great for nonlinear compression    | Not suitable for supervised tasks        |\n",
                "| Visualizes embeddings beautifully  | Not deterministic (especially t-SNE)     |\n",
                "| Reveals hidden clusters            | Can overfit local structure (spaghetti)  |\n",
                "| Works well on dense feature maps   | UMAP/t-SNE require tuning + trial/error  |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- If used for **human clustering** (e.g., criminal risk, hiring), nonlinear compression may **exaggerate separation** or **hide overlap**\n",
                "- Interpretability is low ‚Üí always pair with explanation tools like SHAP\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **t-SNE + UMAP** used to analyze LLMs and BERT activations  \n",
                "- **TriMap**, **PaCMAP**: newer manifold learners with better structure preservation  \n",
                "- **Neural manifold learners**: combine autoencoders + graph preservation  \n",
                "- **Diffusion Maps** for capturing dynamic/temporal manifolds (used in biology)\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: Why is PCA often a poor choice for visualizing image embeddings?**\n",
                "\n",
                "A. PCA has too many hyperparameters  \n",
                "B. PCA doesn‚Äôt preserve local relationships in curved spaces  \n",
                "C. PCA is too slow  \n",
                "D. PCA requires labels\n",
                "\n",
                "‚úÖ **Correct Answer: B**  \n",
                "**Explanation**: PCA flattens data linearly ‚Äî curved patterns get distorted. Manifold learning methods like UMAP or t-SNE preserve the structure better.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Debug Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: trying to visualize with PCA on curved data\n",
                "from sklearn.decomposition import PCA\n",
                "X_pca = PCA(n_components=2).fit_transform(embeddings)\n",
                "```\n",
                "\n",
                "**Fix with Manifold Learning**:\n",
                "\n",
                "```python\n",
                "from sklearn.manifold import TSNE\n",
                "X_vis = TSNE(n_components=2, perplexity=30).fit_transform(embeddings)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Manifold** | A low-dimensional surface embedded in a higher-dimensional space |\n",
                "| **Geodesic Distance** | Distance along the manifold curve, not through space |\n",
                "| **Nonlinear Reduction** | Using flexible, curved mapping to reduce dimensions |\n",
                "| **Embedding** | A numerical, spatial representation of complex input data |\n",
                "| **Unsupervised Visualization** | Viewing structure without labels or supervision |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Hyperparameters (varies by algorithm)\n",
                "\n",
                "- **t-SNE**: `perplexity`, `learning_rate`, `n_iter`\n",
                "- **UMAP**: `n_neighbors`, `min_dist`\n",
                "\n",
                "```python\n",
                "from umap import UMAP\n",
                "model = UMAP(n_neighbors=15, min_dist=0.1)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- No accuracy ‚Üí Evaluate **cluster tightness**, **separation**, or **silhouette score**\n",
                "- Use **label overlays** to validate visuals\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Use **PCA first** to reduce to ~50D, then apply t-SNE or UMAP  \n",
                "- Don‚Äôt over-interpret the global layout (especially t-SNE)  \n",
                "- Run multiple seeds to verify stable structure\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import load_digits\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Load and preprocess\n",
                "digits = load_digits()\n",
                "X = digits.data\n",
                "y = digits.target\n",
                "X_scaled = StandardScaler().fit_transform(X)\n",
                "\n",
                "# Apply t-SNE\n",
                "X_tsne = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_scaled)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(8, 6))\n",
                "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=20)\n",
                "plt.colorbar(scatter, ticks=range(10))\n",
                "plt.title(\"t-SNE Visualization of Digits Dataset\")\n",
                "plt.xlabel(\"Dimension 1\")\n",
                "plt.ylabel(\"Dimension 2\")\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You‚Äôve just unfolded high-dimensional data into 2D ‚Äî without losing its soul.  \n",
                "Next up: deep dive into **t-SNE** (pairwise similarity + perplexity)?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's dive deep into the first of the two manifold-learning stars:  \n",
                "üéØ **t-SNE** ‚Äî a method that maps your high-dimensional data into a 2D world, where clusters and relationships come alive.\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **t-SNE (t-Distributed Stochastic Neighbor Embedding)**  \n",
                "üß© *Pairwise Similarity + Perplexity + Dimensionality Reduction Explained*  \n",
                "(UTHU-structured summary)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "**t-SNE** is a powerful nonlinear algorithm for **visualizing high-dimensional data in 2D or 3D**.  \n",
                "Unlike PCA, it doesn‚Äôt care about variance ‚Äî it cares about **how close things are to each other**, and tries to **preserve neighborhood structure**.\n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine shrinking a massive party (with thousands of people) into a photo where each person is placed next to their closest friends ‚Äî regardless of where they started in the room.\n",
                "\n",
                "That‚Äôs what t-SNE does: builds a **map of your data based on similarity**.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman-Style Explanation |\n",
                "|------|---------------------------|\n",
                "| **Pairwise Similarity** | How much two points ‚Äúfeel‚Äù like neighbors (based on distance) |\n",
                "| **Perplexity** | A balance between local vs global focus ‚Äî like a zoom level |\n",
                "| **Conditional Probability** | The chance that point A picks B as its neighbor |\n",
                "| **Embedding** | New position for a point in lower-dimensional space |\n",
                "| **Stochastic** | Random element included ‚Äî results vary across runs |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- Visualizing **word embeddings**, **sentence vectors**, or **BERT activations**  \n",
                "- Unsupervised pattern discovery (e.g., customer clusters)  \n",
                "- Deep learning feature inspection  \n",
                "- Outlier detection in complex spaces\n",
                "\n",
                "```plaintext\n",
                "  Have high-D embeddings (BERT, images, etc.)?\n",
                "              ‚Üì\n",
                "     Want to explore clusters visually?\n",
                "              ‚Üì\n",
                "      ‚Üí Use t-SNE (2D or 3D map of similarities)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Core Equations\n",
                "\n",
                "#### Step 1: Convert high-dimensional distances into **pairwise probabilities**\n",
                "- For points \\( i \\) and \\( j \\):\n",
                "$$\n",
                "p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}\n",
                "$$\n",
                "\n",
                "- Symmetrized joint probability:\n",
                "$$\n",
                "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}\n",
                "$$\n",
                "\n",
                "#### Step 2: Compute **low-D similarities** using Student t-distribution:\n",
                "$$\n",
                "q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n",
                "$$\n",
                "\n",
                "#### Step 3: Minimize the KL Divergence:\n",
                "$$\n",
                "\\text{KL}(P || Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
                "$$\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "- In high-d space: create fuzzy friendships (probabilities)  \n",
                "- In low-d space: try to **preserve those friendships**  \n",
                "- Use **t-distribution** (fat tails) to avoid ‚Äúcrowding‚Äù effect\n",
                "\n",
                "> Think of it as reconstructing a **friend circle map** ‚Äî if A and B were close before, they should be close now.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                          | Pitfalls                                 |\n",
                "|-------------------------------------|------------------------------------------|\n",
                "| Distances imply relationships       | Global layout can be distorted           |\n",
                "| Input data is scaled and clean      | High noise = bad neighborhoods           |\n",
                "| Random seeds affect outcome         | Not ideal for deterministic pipelines    |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Strengths                         | Weaknesses                               |\n",
                "|----------------------------------|-------------------------------------------|\n",
                "| Reveals **local** structure      | Global distances may lie                 |\n",
                "| Visual clarity for high-D data   | Can look different each run              |\n",
                "| Good for feature exploration     | Slow on large datasets                   |\n",
                "| Great cluster separation         | Hard to interpret axes                   |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- **Coloring** t-SNE plots by labels can falsely suggest strong separation  \n",
                "- Be careful about **bias interpretation** ‚Äî what looks far apart may not be globally meaningful\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **FIt-SNE** and **openTSNE** improve speed + scalability  \n",
                "- Used in **protein structure analysis**, **COVID variant tracking**, and **LLM embedding analysis**  \n",
                "- Often compared with **UMAP** (which is faster, deterministic, and better at preserving topology)\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: What role does perplexity play in t-SNE?**\n",
                "\n",
                "A. It determines the size of the output  \n",
                "B. It balances how local or global the focus is  \n",
                "C. It controls the number of clusters  \n",
                "D. It reduces memory usage\n",
                "\n",
                "‚úÖ **Correct Answer: B**  \n",
                "**Explanation**: A higher perplexity = more global structure. A lower perplexity = zoomed-in local focus.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Fix Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: t-SNE on unscaled data with default settings\n",
                "from sklearn.manifold import TSNE\n",
                "tsne = TSNE(n_components=2)\n",
                "X_embedded = tsne.fit_transform(X)\n",
                "```\n",
                "\n",
                "**Fix**:\n",
                "\n",
                "```python\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "X_scaled = StandardScaler().fit_transform(X)\n",
                "X_embedded = TSNE(n_components=2, perplexity=30, learning_rate=200).fit_transform(X_scaled)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **t-SNE** | Visual technique for converting high-D similarity into 2D/3D map |\n",
                "| **Perplexity** | Tuning knob: local vs global attention |\n",
                "| **KL Divergence** | Measures mismatch between two probability distributions |\n",
                "| **Embedding** | Low-D mapping of high-D data |\n",
                "| **t-Distribution** | Heavy-tailed distribution used in low-D space to avoid crowding |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Hyperparameters\n",
                "\n",
                "- `perplexity`: Typical range = 5‚Äì50  \n",
                "- `learning_rate`: 10‚Äì1000, often set to 200  \n",
                "- `n_iter`: 1000+ recommended\n",
                "\n",
                "```python\n",
                "TSNE(perplexity=30, learning_rate=200, n_iter=1000)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- No accuracy ‚Üí use **visual tightness**, label overlays  \n",
                "- Optionally use **silhouette score** or **trustworthiness**\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Use **PCA ‚Üí t-SNE** pipeline for better results (e.g., reduce to 50D first)  \n",
                "- Always **scale input**  \n",
                "- Run multiple seeds to validate consistent clusters  \n",
                "- Avoid overinterpreting shape or spacing ‚Äî **focus on groupings, not positions**\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import load_digits\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Load and scale data\n",
                "digits = load_digits()\n",
                "X = digits.data\n",
                "y = digits.target\n",
                "X_scaled = StandardScaler().fit_transform(X)\n",
                "\n",
                "# Reduce with t-SNE\n",
                "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000, random_state=42)\n",
                "X_tsne = tsne.fit_transform(X_scaled)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(8, 6))\n",
                "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=20)\n",
                "plt.colorbar(scatter, ticks=range(10))\n",
                "plt.title(\"t-SNE Visualization of Handwritten Digits\")\n",
                "plt.xlabel(\"Dimension 1\")\n",
                "plt.ylabel(\"Dimension 2\")\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ That‚Äôs t-SNE, deconstructed and deployed.  \n",
                "Ready to explore its powerful, often preferred cousin:  \n",
                "‚ö° **UMAP** ‚Äî with better topology and faster performance?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üß© **UMAP (Uniform Manifold Approximation and Projection)**\n",
                "\n",
                "#### **Understanding UMAP‚Äôs Advantages: Topological Preservation and High Performance**\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "**UMAP** is a **nonlinear dimensionality reduction** technique designed to perform well in preserving both **local and global structure** of data while providing better **performance** than methods like t-SNE. It is often used for **visualizing high-dimensional data**, but with the added bonus of being **scalable** to larger datasets. \n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine you're trying to flatten a wrinkled sheet of paper. **t-SNE** would be very careful to preserve the wrinkles in a small area, while **UMAP** keeps the whole structure of the paper in mind, ensuring that it‚Äôs still close to how the sheet would look if you were to view it in 3D.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman-Style Explanation |\n",
                "|------|---------------------------|\n",
                "| **Topological Structure** | The arrangement of points based on proximity or relationship (similarity) in high-D space. |\n",
                "| **Manifold** | A high-dimensional space that, locally, looks flat (like a curved surface, but locally flat). |\n",
                "| **Fuzzy simplicial set** | A way of representing data that captures its geometric structure using overlaps between simplices (simple shapes like triangles). |\n",
                "| **Embedding** | A mapping of high-D data into a lower-dimensional space that maintains as much of the original structure as possible. |\n",
                "| **Local & Global Structure** | Local refers to small, close relationships (neighborhoods), while global refers to larger, broad relationships (overall shape of the data). |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- **NLP**: Visualizing word or sentence embeddings (e.g., BERT, GPT)  \n",
                "- **Computer Vision**: Reducing high-dimensional image feature vectors into interpretable 2D or 3D visualizations  \n",
                "- **Bioinformatics**: Visualizing gene expression data or protein structures  \n",
                "- **Customer Segmentation**: Unsupervised clustering in large customer datasets\n",
                "\n",
                "```plaintext\n",
                "  Need to preserve both **local** and **global** data relationships?\n",
                "              ‚Üì\n",
                "     Want fast, scalable dimensionality reduction?\n",
                "              ‚Üì\n",
                "      ‚Üí Use **UMAP** over t-SNE for **large datasets**\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Core Equations\n",
                "\n",
                "#### Step 1: Construct a **Fuzzy Simplicial Set**  \n",
                "This is the initial step in UMAP where the **local structure** of the data is encoded by calculating pairwise distances and creating a **graph** of neighbors.\n",
                "\n",
                "#### Step 2: Optimize the **Embedding**  \n",
                "UMAP minimizes the difference between the **high-dimensional fuzzy simplicial set** and its **low-dimensional embedding** via an objective function that captures both **local and global structure**.\n",
                "\n",
                "The loss function used in UMAP is based on cross-entropy:\n",
                "\n",
                "$$\n",
                "\\mathcal{L} = \\sum_{i,j} p_{ij} \\log\\left(\\frac{p_{ij}}{q_{ij}}\\right)\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( p_{ij} \\) represents the high-dimensional similarity (based on distances).\n",
                "- \\( q_{ij} \\) represents the low-dimensional similarity (similar to the concept in t-SNE).\n",
                "\n",
                "#### Step 3: Optimization Process\n",
                "Unlike t-SNE, UMAP uses an **optimization method** based on **stochastic gradient descent (SGD)** to optimize the **embedding** in the low-dimensional space.\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "UMAP can be thought of as a **scaling-friendly version** of t-SNE. Both try to maintain **local neighborhoods**, but UMAP additionally captures **global relationships**. It's like using a **smart mapmaker** who can look at the big picture and zoom in to capture local features without losing sight of the overall geography.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                          | Pitfalls                                 |\n",
                "|-------------------------------------|------------------------------------------|\n",
                "| Data has a **manifold structure**    | Doesn't work well for purely random or unstructured data |\n",
                "| Embeddings are **continuous**       | Works best on data with some inherent structure |\n",
                "| Proximity structure is significant | Sometimes can ‚Äúflatten‚Äù important patterns in sparse data |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Strengths                         | Weaknesses                               |\n",
                "|----------------------------------|-------------------------------------------|\n",
                "| **Topological structure** is better preserved than t-SNE | Can sometimes lead to **clustering issues** with very high-dimensional sparse data |\n",
                "| **Faster** and **scalable** to larger datasets | Can still suffer from **crowding issues** in some cases |\n",
                "| **Deterministic** ‚Äî results are reproducible | **Local structure** might not be as precise as t-SNE |\n",
                "| Works with **more diverse data types** | May be computationally intensive for massive datasets without proper optimization |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- **Misinterpretation**: Just like t-SNE, clusters might be visualized in a way that suggests a stronger relationship than actually exists. Always be cautious about over-interpreting the structure, especially for critical applications like medical or financial data.\n",
                "- **Bias in Embeddings**: Embedding models (e.g., NLP models) can reflect societal biases ‚Äî UMAP might highlight these biases if not properly managed.\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- UMAP continues to be refined for scalability with **larger datasets**, often outperforming t-SNE in speed and memory use.\n",
                "- **UMAP for Graphs**: Innovations in using UMAP to visualize graph data, preserving **topological properties** of networks.\n",
                "- Emerging studies in **biological data analysis** and **image classification** leveraging UMAP‚Äôs **speed** and **accuracy**.\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: What advantage does UMAP have over t-SNE for large datasets?**\n",
                "\n",
                "A. UMAP runs faster and scales better with large data  \n",
                "B. UMAP is more deterministic  \n",
                "C. t-SNE preserves local structure better  \n",
                "D. t-SNE is faster on sparse data\n",
                "\n",
                "‚úÖ **Correct Answer: A**  \n",
                "**Explanation**: UMAP is faster and scales better, especially with large datasets.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Fix Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: UMAP on unscaled data with default settings\n",
                "import umap\n",
                "umap_model = umap.UMAP(n_components=2)\n",
                "X_umap = umap_model.fit_transform(X)\n",
                "```\n",
                "\n",
                "**Fix**:\n",
                "\n",
                "```python\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "X_scaled = StandardScaler().fit_transform(X)\n",
                "umap_model = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, metric='euclidean')\n",
                "X_umap = umap_model.fit_transform(X_scaled)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **UMAP** | A scalable, nonlinear dimensionality reduction technique that preserves both local and global data structure. |\n",
                "| **Simplicial Set** | A mathematical construct used to represent the neighborhood relationships in data. |\n",
                "| **Cross-entropy** | A loss function used to measure the dissimilarity between the high-dimensional and low-dimensional representations. |\n",
                "| **Gradient Descent** | Optimization technique used to minimize the loss function. |\n",
                "| **Manifold** | A mathematical space that locally resembles Euclidean space, which is ideal for data that lies in high-dimensional space. |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Hyperparameters\n",
                "\n",
                "- `n_neighbors`: Number of neighbors to consider for local structure (default = 15).  \n",
                "- `min_dist`: Controls the tightness of the clusters in low-D space (default = 0.1).  \n",
                "- `metric`: Distance metric used (default = Euclidean).\n",
                "\n",
                "```python\n",
                "UMAP(n_neighbors=30, min_dist=0.3)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- Use **visual inspection** for local and global structure (like clusters and spread).\n",
                "- Optionally, **trustworthiness score** or **continuity score** for consistency.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- **Scale** your data first ‚Äî UMAP is sensitive to feature magnitude.\n",
                "- For massive datasets, you may need to use **subsampling** or employ parallelism techniques.\n",
                "- UMAP is more suitable for exploratory tasks, so be cautious about using it for highly sensitive final model decisions.\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import load_digits\n",
                "import umap\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Load and scale data\n",
                "digits = load_digits()\n",
                "X = digits.data\n",
                "y = digits.target\n",
                "X_scaled = StandardScaler().fit_transform(X)\n",
                "\n",
                "# Reduce with UMAP\n",
                "umap_model = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, metric='euclidean')\n",
                "X_umap = umap_model.fit_transform(X_scaled)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(8, 6))\n",
                "scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', s=20)\n",
                "plt.colorbar(scatter, ticks=range(10))\n",
                "plt.title(\"UMAP Visualization of Handwritten Digits\")\n",
                "plt.xlabel(\"Dimension 1\")\n",
                "plt.ylabel(\"Dimension 2\")\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "UMAP has made a mark as an incredibly efficient and topologically aware technique for dimensionality reduction. Now that you've explored it, are you ready to visualize your next high-dimensional dataset?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let‚Äôs wrap up this manifold learning chapter with a **real-world NLP application**:  \n",
                "üìö **Visualizing Complex Embeddings from NLP Models** using **t-SNE** and **UMAP**\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Example ‚Äì Visualizing Embeddings from NLP Models**  \n",
                "üß† *See how words, sentences, or documents organize themselves in meaning-space*  \n",
                "(UTHU-structured summary)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "NLP models like **Word2Vec**, **BERT**, or **GPT** generate **high-dimensional vectors** for words, sentences, and documents.\n",
                "\n",
                "These embeddings **capture meaning** ‚Äî similar words are close, different ones far apart ‚Äî but you can‚Äôt \"see\" this in 768D space.  \n",
                "That‚Äôs where **manifold learning** (t-SNE / UMAP) helps.\n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine your brain has a **mental map of words** ‚Äî where ‚Äúcat‚Äù and ‚Äúdog‚Äù are neighbors, and ‚Äúeconomy‚Äù is in another district.  \n",
                "> UMAP and t-SNE let us draw that map on paper ‚Äî preserving the meaning-driven geography.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Explanation |\n",
                "|------|-------------|\n",
                "| **Word Embedding** | A dense vector representing word meaning in high-D space |\n",
                "| **Contextual Embedding** | Word or sentence vector that changes based on context (from models like BERT) |\n",
                "| **Dimensionality Reduction** | Turning 768D vectors into 2D so we can plot them |\n",
                "| **Semantic Clustering** | Words with similar meaning group together |\n",
                "| **Projection** | Mapping from high-D to 2D using manifold learning techniques |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- **Understanding embeddings** from BERT, GPT, Word2Vec, etc.  \n",
                "- **Debugging NLP models** (e.g., Are similar words near each other?)  \n",
                "- **Visualizing semantic clusters** (e.g., professions, animals, emotions)  \n",
                "- **Explaining model behavior** (Why did GPT pick this word?)\n",
                "\n",
                "```plaintext\n",
                "  Text ‚Üí Tokenize ‚Üí Get Embeddings ‚Üí Reduce Dimensionality ‚Üí Plot Similarities\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "You start with a matrix of embeddings:\n",
                "\n",
                "- \\( X \\in \\mathbb{R}^{n \\times d} \\), e.g., 1000 words √ó 768D from BERT\n",
                "\n",
                "Use **t-SNE** or **UMAP** to create:\n",
                "\n",
                "- \\( Y \\in \\mathbb{R}^{n \\times 2} \\), e.g., 2D for visualization\n",
                "\n",
                "Underlying math (example: UMAP) minimizes:\n",
                "\n",
                "$$\n",
                "\\mathcal{L} = \\sum_{i,j} p_{ij} \\log\\left(\\frac{p_{ij}}{q_{ij}}\\right)\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( p_{ij} \\): High-D similarity\n",
                "- \\( q_{ij} \\): Low-D similarity\n",
                "\n",
                "Goal: preserve neighborhood structure.\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "Words that are close in BERT space (e.g., \"cat\", \"kitten\") should **stay close** in the 2D plot.  \n",
                "Manifold learning makes the **invisible clusters** of meaning **visible**.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                        | Pitfalls                                |\n",
                "|-----------------------------------|------------------------------------------|\n",
                "| Embeddings reflect semantic structure | Poor embeddings ‚Üí messy plots            |\n",
                "| Words have consistent meaning     | Ambiguity (e.g., ‚Äúbank‚Äù) causes clutter |\n",
                "| Context is handled (for BERT)     | Averaging over context may dilute meaning |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Strengths                       | Weaknesses                                |\n",
                "|--------------------------------|--------------------------------------------|\n",
                "| Makes embeddings interpretable | 2D view oversimplifies complex structures  |\n",
                "| Great for demo/debug            | Sensitive to preprocessing, scaling       |\n",
                "| Reveals relationships visually | Difficult to explain exact geometry        |\n",
                "| Helps cluster interpretation    | Results vary with seed/perplexity         |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- Visualizing embeddings may reveal **biases** (e.g., gender clustering)  \n",
                "- Must be careful **not to over-interpret clusters** as ground truth  \n",
                "- Embeddings are only as fair as the data they were trained on\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **UMAP + transformer embeddings** used to cluster legal, medical, and social media documents  \n",
                "- **Embedding bias audits** use 2D maps to spot and fix unfair grouping  \n",
                "- Integrated into tools like **BERTViz**, **Embedding Projector**, and **LLM explainers**\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: What does it mean if two word embeddings are close in a t-SNE plot?**\n",
                "\n",
                "A. They have the same number of syllables  \n",
                "B. They co-occur often in documents  \n",
                "C. Their meanings are semantically similar  \n",
                "D. They were both seen recently in training\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: Embeddings close together represent similar meanings, not syntax or co-occurrence alone.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Fix Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: No scaling before t-SNE\n",
                "X_tsne = TSNE(n_components=2).fit_transform(word_vectors)\n",
                "```\n",
                "\n",
                "**Fix:**\n",
                "\n",
                "```python\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "X_scaled = StandardScaler().fit_transform(word_vectors)\n",
                "X_tsne = TSNE(n_components=2, perplexity=30, learning_rate=200).fit_transform(X_scaled)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Word Embedding** | Vector that represents a word‚Äôs meaning |\n",
                "| **Contextual Embedding** | Embedding that changes based on sentence context |\n",
                "| **Projection** | Mapping from high-dimensional to lower dimensions |\n",
                "| **Semantic Space** | Where embeddings \"live\" based on meaning |\n",
                "| **Cluster** | Group of similar points (e.g., all animal words) in embedding space |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Preprocessing Tips\n",
                "\n",
                "- Use **mean pooling** over tokens for sentence-level embedding  \n",
                "- Normalize embeddings with **StandardScaler**\n",
                "- If from BERT: use final hidden layer (or mean of last 4 layers for stability)\n",
                "\n",
                "### üìè Evaluation\n",
                "\n",
                "- Visual inspection: Do words of same category cluster?  \n",
                "- Optionally use **clustering scores** (silhouette, Davies-Bouldin) on reduced vectors\n",
                "\n",
                "### ‚öôÔ∏è Production Notes\n",
                "\n",
                "- Use UMAP for larger vocab sets  \n",
                "- Use interactive tools like **Plotly** or **TensorBoard projector** for deeper exploration  \n",
                "- Always label points with the actual **word/token** for clarity\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# Get embeddings from a BERT-like model\n",
                "sentences = [\"dog\", \"cat\", \"puppy\", \"car\", \"engine\", \"banana\", \"apple\", \"mango\", \"economy\", \"inflation\"]\n",
                "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "embeddings = model.encode(sentences)\n",
                "\n",
                "# Normalize\n",
                "X_scaled = StandardScaler().fit_transform(embeddings)\n",
                "\n",
                "# Reduce to 2D with t-SNE\n",
                "X_2d = TSNE(n_components=2, perplexity=5, random_state=42).fit_transform(X_scaled)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.scatter(X_2d[:, 0], X_2d[:, 1], c='blue', s=50)\n",
                "\n",
                "for i, label in enumerate(sentences):\n",
                "    plt.text(X_2d[i, 0]+0.1, X_2d[i, 1]+0.1, label, fontsize=12)\n",
                "\n",
                "plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
                "plt.xlabel(\"Dimension 1\")\n",
                "plt.ylabel(\"Dimension 2\")\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You've now visualized **meaning in motion** ‚Äî compressed semantic space into human view.  \n",
                "Want to add a next-level version using **UMAP with sentence clusters** or transition to **Reinforcement Learning** next module?"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
