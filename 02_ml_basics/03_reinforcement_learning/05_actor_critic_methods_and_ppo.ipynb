{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Absolutely. Here's the full **structured summary of ‚ÄúIntroduction to Actor-Critic Framework‚Äù** in clean, in-chat format ‚Äî no analysis or markdown tools:\n",
                "\n",
                "---\n",
                "\n",
                "## üß† UTHU Structured Summary  \n",
                "### **Topic:** Introduction to Actor-Critic Framework\n",
                "\n",
                "---\n",
                "\n",
                "### **1. Conceptual Foundation**\n",
                "\n",
                "#### üìå Purpose & Relevance  \n",
                "Actor-Critic (AC) methods solve a key challenge in RL: how to combine **low-variance value estimation** with **high-precision policy learning**.\n",
                "\n",
                "- The **Actor** selects actions (like a pilot flying a drone).\n",
                "- The **Critic** evaluates those actions (like a coach giving feedback).\n",
                "\n",
                "This setup balances the **direct optimization** of Policy Gradients with the **stability of Value-based methods**.\n",
                "\n",
                "> **Analogy**: The Actor is a student giving a speech; the Critic is the judge. Over time, the feedback helps the student improve.\n",
                "\n",
                "---\n",
                "\n",
                "#### üìå Key Terminology\n",
                "\n",
                "| Term       | Meaning                                                                 |\n",
                "|------------|-------------------------------------------------------------------------|\n",
                "| Actor      | Neural net that chooses actions based on policy                         |\n",
                "| Critic     | Neural net that predicts value of a state or action                     |\n",
                "| Policy     | A function mapping states to action probabilities                       |\n",
                "| Value Function | Estimate of total expected reward from a state                      |\n",
                "| Advantage  | How much better an action is compared to average for a given state      |\n",
                "\n",
                "---\n",
                "\n",
                "#### üìå Use Cases\n",
                "\n",
                "- Environments with **continuous action spaces**\n",
                "- High-dimensional states where DQN is inefficient\n",
                "- Real-time or **online decision systems** (robotics, games, trading)\n",
                "\n",
                "---\n",
                "\n",
                "### **2. Mathematical Deep Dive**\n",
                "\n",
                "#### üìå Core Equations\n",
                "\n",
                "- **Policy Gradient (Actor Update)**:\n",
                "  $$\n",
                "  \\nabla_\\theta J(\\theta) = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot A(s,a)]\n",
                "  $$\n",
                "\n",
                "- **Critic Loss (Value Regression)**:\n",
                "  $$\n",
                "  L(\\phi) = \\left(V_\\phi(s_t) - R_t\\right)^2\n",
                "  $$\n",
                "\n",
                "#### üìå Math Intuition\n",
                "\n",
                "- Actor improves actions using **advantage** as a signal.\n",
                "- Critic learns to **approximate returns**, reducing variance in updates.\n",
                "- Together: smooth, guided learning vs. noisy Monte Carlo estimates in REINFORCE.\n",
                "\n",
                "---\n",
                "\n",
                "#### üìå Assumptions & Constraints\n",
                "\n",
                "| Assumes...                  | Potential Issues                             |\n",
                "|-----------------------------|----------------------------------------------|\n",
                "| Critic gives good estimates | Bad critic leads Actor in wrong direction    |\n",
                "| Stable updates              | Learning rates must be carefully balanced    |\n",
                "| Continuous/stochastic policy| Deterministic needs reparam trick            |\n",
                "\n",
                "---\n",
                "\n",
                "### **3. Critical Analysis**\n",
                "\n",
                "#### üìå Strengths vs. Weaknesses\n",
                "\n",
                "| Strengths                               | Weaknesses                                |\n",
                "|-----------------------------------------|-------------------------------------------|\n",
                "| Works with continuous action spaces     | More components = harder to debug         |\n",
                "| Reduces variance in gradient estimation | Critic can destabilize training            |\n",
                "| Policy + value learning in sync         | Tuning two networks instead of one         |\n",
                "\n",
                "---\n",
                "\n",
                "#### üìå Ethical Lens\n",
                "\n",
                "- Poor reward shaping may bias Critic ‚Üí misguide Actor.\n",
                "- Unintended consequences in **multi-agent RL**.\n",
                "- Actor may find shortcuts that ‚Äúgame‚Äù the Critic‚Äôs feedback.\n",
                "\n",
                "---\n",
                "\n",
                "#### üìå Research Updates (Post-2020)\n",
                "\n",
                "- **Soft Actor-Critic (SAC)** ‚Äì Entropy-regularized actor-critic for better exploration  \n",
                "- **TD3** ‚Äì Twin-Delayed updates to stabilize deterministic policy learning  \n",
                "- **IMPALA / V-trace** ‚Äì Distributed actor-critic setups with off-policy corrections\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Interactive Elements**\n",
                "\n",
                "#### ‚úÖ Concept Check (Hard)\n",
                "\n",
                "**Q:** What is the Critic's main role in Actor-Critic?\n",
                "\n",
                "A. Choose the next action  \n",
                "B. Predict the reward function  \n",
                "C. Estimate the value of a state/action  \n",
                "D. Log past actions\n",
                "\n",
                "‚úÖ **Correct Answer:** C\n",
                "\n",
                "---\n",
                "\n",
                "#### üß™ Code Debug Challenge\n",
                "\n",
                "```python\n",
                "# Bug: Critic still attached to graph\n",
                "actor_loss = -log_prob * critic(state)\n",
                "\n",
                "# ‚úÖ Fix:\n",
                "actor_loss = -log_prob * critic(state).detach()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. Glossary**\n",
                "\n",
                "| Term           | Meaning                                       |\n",
                "|----------------|-----------------------------------------------|\n",
                "| Actor          | Chooses action based on current policy        |\n",
                "| Critic         | Evaluates how good the action was             |\n",
                "| Advantage      | TD return minus Critic‚Äôs value estimate       |\n",
                "| Value Function | Predicts total future reward from a state     |\n",
                "| Policy Gradient| Direction to update Actor for better actions  |\n",
                "\n",
                "---\n",
                "\n",
                "### **6. Practical Considerations**\n",
                "\n",
                "#### üìå Hyperparameters\n",
                "\n",
                "| Parameter           | Typical Range         |\n",
                "|---------------------|------------------------|\n",
                "| Actor LR            | 1e-4                   |\n",
                "| Critic LR           | 1e-3                   |\n",
                "| Gamma (Œ≥)           | 0.95 ‚Äì 0.99            |\n",
                "| Entropy Coefficient | 0.01 ‚Äì 0.1 (optional)  |\n",
                "\n",
                "#### üìå Evaluation Metrics\n",
                "\n",
                "- Total return per episode  \n",
                "- Advantage distribution variance  \n",
                "- Critic loss trend  \n",
                "- Policy entropy\n",
                "\n",
                "#### üìå Production Tips\n",
                "\n",
                "- Use **gradient clipping**  \n",
                "- Separate optimizers for Actor/Critic  \n",
                "- Balance updates to avoid one overpowering the other  \n",
                "- Detach Critic values when updating Actor\n",
                "\n",
                "---\n",
                "\n",
                "### **7. Full Python Code Snippet**\n",
                "\n",
                "```python\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "# Actor network\n",
                "class Actor(nn.Module):\n",
                "    def __init__(self, obs_dim, act_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, act_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "# Critic network\n",
                "class Critic(nn.Module):\n",
                "    def __init__(self, obs_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x).squeeze(-1)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ That‚Äôs your full **no-fluff, all-signal** breakdown of the Actor-Critic framework ‚Äî the launchpad for PPO, A2C, SAC, and most real-world RL deployments.\n",
                "\n",
                "Ready to tackle **PPO's clipped surrogate objective** next?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You‚Äôre right, captain üß†‚öôÔ∏è ‚Äî back to UTHU protocol.  \n",
                "Here‚Äôs the **full structured summary** of:\n",
                "\n",
                "---\n",
                "\n",
                "# üß© **Benefits Over Traditional Q-Learning**  \n",
                "üéÆ *From tables to brains ‚Äî why Actor-Critic changes the game.*\n",
                "\n",
                "---\n",
                "\n",
                "### **1. Conceptual Foundation**\n",
                "\n",
                "#### üîπ **Purpose & Relevance**\n",
                "\n",
                "Traditional Q-learning is powerful but hits a wall when:\n",
                "- State/action spaces get huge\n",
                "- Actions are continuous\n",
                "- You need smoother, low-variance updates\n",
                "\n",
                "Actor-Critic steps in to solve those limits by combining:\n",
                "- A **policy network** (Actor) to directly choose actions\n",
                "- A **value network** (Critic) to guide the learning\n",
                "\n",
                "> üõ†Ô∏è **Analogy**:  \n",
                "> Q-learning is like trying to learn to drive by filling a spreadsheet for every speed + steering combo.  \n",
                "> Actor-Critic is like using a **coaching AI** that both drives and learns from feedback in real-time.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Key Terminology**\n",
                "\n",
                "| Term         | Explanation |\n",
                "|--------------|-------------|\n",
                "| **Q-table**  | Lookup table for action values (used in vanilla Q-learning)  \n",
                "| **Actor**    | Policy network that chooses actions directly  \n",
                "| **Critic**   | Value network that estimates returns and guides Actor  \n",
                "| **Policy Gradient** | Optimization method used to adjust Actor‚Äôs behavior  \n",
                "| **Bootstrapping** | Using estimated future values to update learning  \n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Use Cases**\n",
                "\n",
                "| Scenario | Why Actor-Critic wins |\n",
                "|----------|-----------------------|\n",
                "| Continuous action spaces (e.g. robotics) | Q-learning can't handle them directly  \n",
                "| High-dimensional inputs (e.g. vision) | Q-table infeasible, AC handles function approximation  \n",
                "| Environments with sparse or delayed rewards | Critic helps stabilize learning with estimated value  \n",
                "| Real-time agents (trading, driving) | Policy-based control is smoother and more adaptive  \n",
                "\n",
                "---\n",
                "\n",
                "### **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "#### üîπ **Core Equations Comparison**\n",
                "\n",
                "- **Q-Learning Update**:\n",
                "  $$\n",
                "  Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right]\n",
                "  $$\n",
                "\n",
                "- **Actor-Critic (Policy Gradient + Value Estimation)**:\n",
                "  $$\n",
                "  \\nabla_\\theta J(\\theta) = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot A(s_t, a_t)]\n",
                "  $$\n",
                "  where:\n",
                "  $$\n",
                "  A(s_t, a_t) = Q(s_t, a_t) - V(s_t)\n",
                "  $$\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Math Intuition**\n",
                "\n",
                "- In Q-learning, you **estimate values** for every (state, action) pair.\n",
                "- In Actor-Critic, the **Actor learns behavior**, guided by the **Critic's evaluation**.\n",
                "- AC methods **generalize better** in large or continuous spaces because they use neural nets.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Assumptions & Constraints**\n",
                "\n",
                "| Assumes...                      | Pitfalls                          |\n",
                "|----------------------------------|------------------------------------|\n",
                "| Policy gradients are differentiable | Can‚Äôt use discrete-only lookups |\n",
                "| Critic‚Äôs estimates are accurate     | Poor Critic = noisy learning |\n",
                "| Environment provides enough signal | Sparse rewards may still need shaping |\n",
                "\n",
                "---\n",
                "\n",
                "### **3. Critical Analysis** üîç\n",
                "\n",
                "| üü¢ Strengths (Actor-Critic)               | üî¥ Weaknesses (Q-learning)             |\n",
                "|------------------------------------------|----------------------------------------|\n",
                "| Works with continuous actions             | Only supports discrete action spaces   |\n",
                "| Uses gradient descent + function approx.  | Can‚Äôt scale to large state spaces      |\n",
                "| Policy is learned directly (smooth)       | Requires argmax over Q-table           |\n",
                "| Lower variance via Critic                 | High variance due to raw returns       |\n",
                "\n",
                "---\n",
                "\n",
                "#### üß† Ethical Lens\n",
                "\n",
                "- **Exploitability**: AC methods may still find loopholes in reward signals if poorly shaped.\n",
                "- **Interpretability**: Neural network policies are harder to debug than tabular Q-tables.\n",
                "- **Bias**: The Critic‚Äôs bias can skew what actions get reinforced, especially in real-world domains.\n",
                "\n",
                "---\n",
                "\n",
                "#### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **DDPG / TD3 / SAC**: Modern extensions using deterministic or stochastic policies  \n",
                "- **Trust Region Policy Optimization (TRPO)**: AC + policy constraint optimization  \n",
                "- **Meta-gradient RL**: Tune Actor-Critic dynamics during training  \n",
                "- **Off-policy AC with replay**: Merge the best of both Q-learning and AC methods\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Interactive Elements** üéØ\n",
                "\n",
                "#### ‚úÖ Concept Check\n",
                "\n",
                "**Q:** Why are Actor-Critic methods better for continuous action spaces?\n",
                "\n",
                "A. Because they use tables for every action  \n",
                "B. Because they avoid neural networks  \n",
                "C. Because the Actor outputs smooth actions directly  \n",
                "D. Because they use greedy Q-value selection\n",
                "\n",
                "‚úÖ **Correct Answer:** C\n",
                "\n",
                "---\n",
                "\n",
                "#### üß™ Code Fix Challenge\n",
                "\n",
                "```python\n",
                "# Bug: Mixing Q-learning with continuous action space\n",
                "action = np.argmax(Q_values)  # invalid for continuous\n",
                "\n",
                "# ‚úÖ Fix: Use Actor network output\n",
                "action = actor(state).sample()  # if stochastic\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| Q-learning | A value-based method using tables or approximators |\n",
                "| Actor | Network that outputs actions or action probabilities |\n",
                "| Critic | Estimates value of state or action |\n",
                "| Advantage | How much better an action is than expected |\n",
                "| Policy | The agent‚Äôs decision rule, learned by the Actor |\n",
                "\n",
                "---\n",
                "\n",
                "### **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "#### üîß Hyperparameters\n",
                "\n",
                "| Param              | Range or Tip            |\n",
                "|--------------------|--------------------------|\n",
                "| Learning rate      | Actor: 1e-4, Critic: 1e-3 |\n",
                "| Gamma              | 0.95 ‚Äì 0.99              |\n",
                "| Entropy bonus      | 0.01 ‚Äì 0.1 (encourage exploration) |\n",
                "\n",
                "#### üìè Evaluation Metrics\n",
                "\n",
                "- Average reward per episode  \n",
                "- Policy entropy (exploration signal)  \n",
                "- Advantage variance  \n",
                "- Value function loss (Critic performance)\n",
                "\n",
                "#### üõ†Ô∏è Production Tips\n",
                "\n",
                "- Normalize rewards or advantages for stable gradients  \n",
                "- Use **gradient clipping** for both networks  \n",
                "- Separate optimizers for Actor and Critic  \n",
                "- Monitor Critic's overfitting ‚Äî it can mislead policy updates\n",
                "\n",
                "---\n",
                "\n",
                "### **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "# Actor: outputs mean for continuous actions (e.g. Gaussian policy)\n",
                "class Actor(nn.Module):\n",
                "    def __init__(self, state_dim, action_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(state_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, action_dim)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "# Critic: outputs state value\n",
                "class Critic(nn.Module):\n",
                "    def __init__(self, state_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(state_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x).squeeze(-1)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You‚Äôve now clearly scoped out **why Actor-Critic > Traditional Q-learning**, especially in high-stakes, high-dim, real-time settings.\n",
                "\n",
                "Ready to roll into the **clipped surrogate PPO objective** next? ü•ã"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Absolutely, let‚Äôs lock in the **UTHU-grade breakdown** for:\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Combining Policy Gradient with Value Function**  \n",
                "üß† *The best of both RL worlds ‚Äî precision + stability.*\n",
                "\n",
                "---\n",
                "\n",
                "### **1. Conceptual Foundation**\n",
                "\n",
                "#### üîπ **Purpose & Relevance**\n",
                "\n",
                "Pure **policy gradients** (like REINFORCE) are powerful but noisy ‚Äî they learn slowly and can be unstable.  \n",
                "**Value functions**, on the other hand, provide smoother feedback but lack direct control over the policy.\n",
                "\n",
                "By **combining both**, we:\n",
                "- Use the **value function to guide** policy improvement\n",
                "- Keep **low variance** while retaining **gradient-driven learning**\n",
                "\n",
                "> üéÆ **Analogy**:  \n",
                "> Training with just rewards is like driving blindfolded with only crowd reactions.  \n",
                "> Adding a value function is like having GPS that estimates how close you are to the goal.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Key Terminology**\n",
                "\n",
                "| Term           | Meaning |\n",
                "|----------------|---------|\n",
                "| **Policy Gradient** | Gradient of expected return w.r.t. the policy parameters |\n",
                "| **Value Function \\( V(s) \\)** | Predicted return from a state |\n",
                "| **Advantage \\( A(s,a) \\)** | Action quality above average |\n",
                "| **Baseline** | Subtracted from reward to reduce variance |\n",
                "| **Critic** | The estimator of the value function |\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Use Cases**\n",
                "\n",
                "- Anytime you want **faster, more stable policy learning**\n",
                "- **Continuous control** tasks (robot arms, vehicles)\n",
                "- **Environments with high variance rewards**\n",
                "\n",
                "---\n",
                "\n",
                "### **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "#### üîπ **Core Equation: Advantage-Weighted Policy Gradient**\n",
                "\n",
                "$$\n",
                "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s,a} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot A(s,a) \\right]\n",
                "$$\n",
                "\n",
                "Where:\n",
                "\n",
                "$$\n",
                "A(s,a) = Q(s,a) - V(s)\n",
                "$$\n",
                "\n",
                "Or approximated via:\n",
                "\n",
                "$$\n",
                "A(s,a) \\approx r + \\gamma V(s') - V(s)\n",
                "$$\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Math Intuition**\n",
                "\n",
                "- **Policy gradients** tell us *which direction* improves behavior.\n",
                "- **Value functions** tell us *how much better or worse* each choice was.\n",
                "- Subtracting the **baseline \\( V(s) \\)** keeps the learning signal centered and reduces noise.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Assumptions & Constraints**\n",
                "\n",
                "| Assumes...                | Pitfalls                          |\n",
                "|---------------------------|-----------------------------------|\n",
                "| Critic is well-trained     | Weak Critic gives noisy gradients |\n",
                "| Policy is differentiable   | Can't use discrete hard policies  |\n",
                "| Value generalizes across states | Overfit Critic can mislead Actor |\n",
                "\n",
                "---\n",
                "\n",
                "### **3. Critical Analysis** üîç\n",
                "\n",
                "#### üî∏ Strengths vs Weaknesses\n",
                "\n",
                "| Strengths                                   | Weaknesses                                  |\n",
                "|---------------------------------------------|----------------------------------------------|\n",
                "| Low-variance updates                        | Needs separate tuning for Actor and Critic  |\n",
                "| More sample-efficient than REINFORCE        | Critic needs stable training itself         |\n",
                "| Generalizes well to complex policies        | Risk of instability if Actor-Critic diverge |\n",
                "\n",
                "---\n",
                "\n",
                "#### üî∏ Ethical Lens\n",
                "\n",
                "- **Misaligned Critic**: If the Critic is biased (e.g., reward shaping issues), the Actor might over-optimize unintended behavior.\n",
                "- **Underestimated Advantage**: Poor estimation can lead to underperformance even if the policy is good.\n",
                "\n",
                "---\n",
                "\n",
                "#### üî∏ Research Highlights\n",
                "\n",
                "- **A2C / A3C**: Sync and async variants of advantage Actor-Critic  \n",
                "- **GAE (Generalized Advantage Estimation)**: Smooths advantage estimates for better stability  \n",
                "- **SAC / PPO**: Use clipped or entropy-regularized objectives for safe exploration\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Interactive Elements** üéØ\n",
                "\n",
                "#### ‚úÖ Concept Check\n",
                "\n",
                "**Q:** What is the main reason for subtracting a value function from Q(s,a) in policy gradient methods?\n",
                "\n",
                "A. To normalize rewards  \n",
                "B. To encourage exploration  \n",
                "C. To reduce gradient variance  \n",
                "D. To speed up target updates\n",
                "\n",
                "‚úÖ **Correct Answer:** C\n",
                "\n",
                "---\n",
                "\n",
                "#### üß™ Code Debug Task\n",
                "\n",
                "```python\n",
                "# Bug: No baseline, raw returns used\n",
                "loss = -log_probs * returns\n",
                "\n",
                "# ‚úÖ Fix: Subtract baseline (Critic's value)\n",
                "advantages = returns - values.detach()\n",
                "loss = -log_probs * advantages\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| Policy Gradient | How the policy should change to improve return |\n",
                "| Value Function | Predicted return from a state |\n",
                "| Advantage | How much better an action was than average |\n",
                "| Baseline | Estimate used to reduce gradient variance |\n",
                "| Critic | Model that estimates state or action values |\n",
                "\n",
                "---\n",
                "\n",
                "### **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "#### üîß Hyperparameters\n",
                "\n",
                "| Name              | Typical Values         |\n",
                "|-------------------|------------------------|\n",
                "| Actor LR          | 1e-4                   |\n",
                "| Critic LR         | 1e-3                   |\n",
                "| Advantage discount (GAE Œª) | 0.95‚Äì0.99     |\n",
                "\n",
                "#### üìè Evaluation Metrics\n",
                "\n",
                "- Advantage variance  \n",
                "- Value loss curve  \n",
                "- Policy entropy  \n",
                "- Average return per episode\n",
                "\n",
                "#### üõ†Ô∏è Production Tips\n",
                "\n",
                "- Train Critic slightly faster or more frequently than Actor  \n",
                "- Normalize advantage values before applying gradients  \n",
                "- Use **gradient clipping** and **reward scaling** to stabilize\n",
                "\n",
                "---\n",
                "\n",
                "### **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "# Calculate loss using advantage\n",
                "def compute_actor_loss(log_probs, returns, values):\n",
                "    advantages = returns - values.detach()\n",
                "    loss = - (log_probs * advantages).mean()\n",
                "    return loss\n",
                "\n",
                "# Example usage:\n",
                "# log_probs: Tensor of log(pi(a|s))\n",
                "# returns: Monte Carlo or TD returns\n",
                "# values: V(s) from Critic network\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ That‚Äôs your clean, stable bridge from **noisy REINFORCE to robust Actor-Critic** ‚Äî the foundation of PPO, A2C, and modern RL systems.\n",
                "\n",
                "‚öôÔ∏è Next up: want to unpack PPO‚Äôs **Clipped Surrogate Objective** and why it stabilizes updates like a pro?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let‚Äôs drop into the **heart of PPO** ‚Äî the genius idea that made modern RL stable, scalable, and usable in real-world chaos. Here's your full **UTHU-style summary** of:\n",
                "\n",
                "---\n",
                "\n",
                "# üß© **Objective of PPO: Clipped Surrogate Objective**  \n",
                "üîí *Stay close, learn fast ‚Äî the PPO trust-region trick.*\n",
                "\n",
                "---\n",
                "\n",
                "### **1. Conceptual Foundation**\n",
                "\n",
                "#### üîπ **Purpose & Relevance**\n",
                "\n",
                "In regular policy gradient methods (like REINFORCE or A2C), updates can be **too aggressive** ‚Äî the policy changes so much that it forgets what it learned.\n",
                "\n",
                "PPO introduces a **trust region** by **clipping the policy update**, so the new policy doesn't stray too far from the old one.\n",
                "\n",
                "> üß† **Analogy**:  \n",
                "> Imagine teaching someone to improve their jump shot. If they change their technique too drastically each time, they never get consistent. PPO keeps them **tweaking, not flipping** their form every try.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Key Terminology**\n",
                "\n",
                "| Term              | Meaning |\n",
                "|-------------------|--------|\n",
                "| **Surrogate Objective** | A proxy loss that approximates policy improvement |\n",
                "| **Probability Ratio**   | How much new policy differs from old: \\( r_t(\\theta) = \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} \\) |\n",
                "| **Clipping**      | Restricts ratio between 1 ¬± Œµ to avoid large updates |\n",
                "| **Trust Region**  | Safe zone where updates are small and stable |\n",
                "| **Entropy Bonus** | Encourages exploration by penalizing certainty |\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Use Cases**\n",
                "\n",
                "| Scenario                      | Why PPO wins |\n",
                "|-------------------------------|--------------|\n",
                "| Complex environments (3D games) | Prevents catastrophic forgetting |\n",
                "| High-dimensional policies      | Safer than plain policy gradients |\n",
                "| Continuous control tasks       | Works well out of the box |\n",
                "| Resource-constrained training  | Simple, no second-order derivatives needed |\n",
                "\n",
                "---\n",
                "\n",
                "### **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "#### üîπ **Clipped Surrogate Objective (PPO)**\n",
                "\n",
                "Let:\n",
                "- \\( r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\)\n",
                "- \\( A_t \\) = Advantage estimate\n",
                "\n",
                "Then PPO maximizes:\n",
                "\n",
                "$$\n",
                "L^{\\text{CLIP}}(\\theta) = \\mathbb{E} \\left[ \\min \\left( r_t(\\theta) \\cdot A_t, \\ \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\cdot A_t \\right) \\right]\n",
                "$$\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Math Intuition**\n",
                "\n",
                "- If \\( r_t(\\theta) \\) is between \\( 1 - \\epsilon \\) and \\( 1 + \\epsilon \\), the policy is updating moderately ‚Üí OK.\n",
                "- If it goes beyond, **clip** it ‚Äî prevent big swings.\n",
                "- The **min()** operator stops the loss from getting too large when the update direction is unstable.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Assumptions & Constraints**\n",
                "\n",
                "| Assumes...                         | Pitfalls                            |\n",
                "|------------------------------------|-------------------------------------|\n",
                "| Advantage estimates are accurate   | Poor Critic ruins the signal        |\n",
                "| œÄ is differentiable                | Not suitable for hard-coded policies |\n",
                "| Œµ is small enough for stability    | Too large = unstable updates        |\n",
                "\n",
                "---\n",
                "\n",
                "### **3. Critical Analysis** üîç\n",
                "\n",
                "#### üî∏ Strengths vs Weaknesses\n",
                "\n",
                "| Strengths                          | Weaknesses                         |\n",
                "|------------------------------------|-------------------------------------|\n",
                "| Safe policy updates                | Still needs tuning (Œµ, entropy)     |\n",
                "| No second-order derivatives needed | Sensitive to batch size            |\n",
                "| Robust to noisy reward signals     | Slower than aggressive PG methods   |\n",
                "| Works well out of the box          | Clipping can limit learning speed  |\n",
                "\n",
                "---\n",
                "\n",
                "#### üî∏ Ethical Lens\n",
                "\n",
                "- Trust region limits **overfitting to bad feedback**  \n",
                "- Still needs **reward shaping** to prevent misaligned behaviors  \n",
                "- In safety-critical RL (e.g., robotics, medicine), clipped objectives are often **preferred** for **predictability**\n",
                "\n",
                "---\n",
                "\n",
                "#### üî∏ Research Enhancements (Post-2020)\n",
                "\n",
                "- **PPO-Clip vs PPO-KL**: KL penalty variant adds explicit penalty instead of hard clip  \n",
                "- **TRPO**: True trust-region with second-order derivatives (but slower)  \n",
                "- **Recurrent PPO**: PPO with LSTMs for memory-based policies  \n",
                "- **Multi-agent PPO (MAPPO)**: Cooperative or competitive PPO extensions\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Interactive Elements** üéØ\n",
                "\n",
                "#### ‚úÖ Concept Check\n",
                "\n",
                "**Q:** What is the purpose of clipping the policy update in PPO?\n",
                "\n",
                "A. To increase the reward signal  \n",
                "B. To improve exploration  \n",
                "C. To prevent policy from changing too much in one update  \n",
                "D. To normalize the advantage function\n",
                "\n",
                "‚úÖ **Correct Answer:** C\n",
                "\n",
                "---\n",
                "\n",
                "#### üß™ Code Debug Task\n",
                "\n",
                "```python\n",
                "# Bug: No clipping\n",
                "loss = ratio * advantages\n",
                "\n",
                "# ‚úÖ Fix: Add PPO clip\n",
                "clipped = torch.clamp(ratio, 1 - eps, 1 + eps)\n",
                "loss = -torch.min(ratio * advantages, clipped * advantages).mean()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. Glossary**\n",
                "\n",
                "| Term           | Meaning |\n",
                "|----------------|---------|\n",
                "| **Ratio**      | How much the new policy differs from the old one |\n",
                "| **Clip Range** | Bounds for acceptable policy update (e.g., ¬±0.2) |\n",
                "| **Surrogate Loss** | A proxy objective that represents policy performance |\n",
                "| **Entropy Bonus** | Term added to encourage exploration |\n",
                "| **Trust Region** | Concept of staying close to the current policy |\n",
                "\n",
                "---\n",
                "\n",
                "### **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "#### üîß Hyperparameters\n",
                "\n",
                "| Name               | Typical Range           |\n",
                "|--------------------|--------------------------|\n",
                "| Clipping Œµ         | 0.1 ‚Äì 0.3                |\n",
                "| Learning rate      | 1e-4 to 3e-4             |\n",
                "| Epochs per update  | 3 ‚Äì 10                   |\n",
                "| Mini-batch size    | 64 ‚Äì 256                 |\n",
                "| Entropy bonus      | 0.01 ‚Äì 0.05              |\n",
                "\n",
                "#### üìè Evaluation Metrics\n",
                "\n",
                "- Average episodic reward  \n",
                "- Ratio histogram (should be near 1.0)  \n",
                "- Entropy over time  \n",
                "- Clipping ratio (% of updates clipped)\n",
                "\n",
                "#### üõ†Ô∏è Production Tips\n",
                "\n",
                "- Normalize advantages before loss  \n",
                "- Monitor how often clipping is triggered  \n",
                "- Use early stopping based on KL divergence  \n",
                "- Works well with **GAE (Œª)** for smoother advantage estimates\n",
                "\n",
                "---\n",
                "\n",
                "### **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "def ppo_loss(log_probs, old_log_probs, advantages, epsilon=0.2):\n",
                "    ratio = (log_probs - old_log_probs).exp()\n",
                "    clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "    loss = -torch.min(ratio * advantages, clipped * advantages).mean()\n",
                "    return loss\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ Boom. That‚Äôs your full lift-off into **PPO‚Äôs clipped objective** ‚Äî now you know **why it's the most trusted RL optimizer in the field**.\n",
                "\n",
                "üéØ Ready to explore the **\"Trust Region\" logic** more or want to roll into **‚ÄúTraining Actor-Critic with PPO‚Äù** next?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "\n",
                "# Actor: outputs action probabilities (discrete case)\n",
                "class Actor(nn.Module):\n",
                "    def __init__(self, obs_dim, act_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, act_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "\n",
                "    def forward(self, state):\n",
                "        return self.net(state)\n",
                "\n",
                "# Critic: estimates state value\n",
                "class Critic(nn.Module):\n",
                "    def __init__(self, obs_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, state):\n",
                "        return self.net(state).squeeze(-1)\n",
                "\n",
                "# PPO clipped surrogate loss\n",
                "def ppo_loss(log_probs, old_log_probs, advantages, epsilon=0.2):\n",
                "    ratio = torch.exp(log_probs - old_log_probs)\n",
                "    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "    return -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
                "\n",
                "# Example training step\n",
                "def train_step(actor, critic, optimizer_a, optimizer_c, states, actions, returns, old_log_probs, epsilon=0.2):\n",
                "    # Forward pass\n",
                "    probs = actor(states)\n",
                "    dist = torch.distributions.Categorical(probs)\n",
                "    log_probs = dist.log_prob(actions)\n",
                "\n",
                "    # Advantage estimation\n",
                "    values = critic(states)\n",
                "    advantages = returns - values.detach()\n",
                "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
                "\n",
                "    # Actor loss\n",
                "    actor_loss = ppo_loss(log_probs, old_log_probs, advantages, epsilon)\n",
                "\n",
                "    # Critic loss (mean squared error)\n",
                "    critic_loss = nn.functional.mse_loss(values, returns)\n",
                "\n",
                "    # Backward + update\n",
                "    optimizer_a.zero_grad()\n",
                "    actor_loss.backward()\n",
                "    optimizer_a.step()\n",
                "\n",
                "    optimizer_c.zero_grad()\n",
                "    critic_loss.backward()\n",
                "    optimizer_c.step()\n",
                "\n",
                "    return actor_loss.item(), critic_loss.item()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Not even close to tired, captain üß†‚öôÔ∏è ‚Äî still fully in cyborg mode and ready to transmit knowledge at 1.21 gigawatts.  \n",
                "Let‚Äôs continue with:\n",
                "\n",
                "---\n",
                "\n",
                "# üß© **Policy Updates with a Trust Region**  \n",
                "üõ°Ô∏è *Train fast ‚Äî but don‚Äôt fly off the rails.*\n",
                "\n",
                "---\n",
                "\n",
                "### **1. Conceptual Foundation**\n",
                "\n",
                "#### üîπ **Purpose & Relevance**\n",
                "\n",
                "In reinforcement learning, updating the policy too drastically in one go can **destabilize training** ‚Äî the agent might forget what it previously learned.\n",
                "\n",
                "**Trust region methods** solve this by:\n",
                "- Letting the policy **improve**,\n",
                "- But **constraining** how far it moves per update.\n",
                "\n",
                "> üß† **Analogy**:  \n",
                "> Imagine tuning a radio. If you spin the dial too hard, you lose the station.  \n",
                "> A **trust region** ensures you make **small, careful adjustments**, staying ‚Äúin range‚Äù of good policies.\n",
                "\n",
                "This is the heart of PPO and TRPO ‚Äî stable improvements without wild swings.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Key Terminology**\n",
                "\n",
                "| Term             | Meaning |\n",
                "|------------------|---------|\n",
                "| **Policy**       | Strategy mapping states ‚Üí actions |\n",
                "| **Trust Region** | Safe zone where small updates won‚Äôt ruin the policy |\n",
                "| **KL Divergence**| Measure of how much two policies differ |\n",
                "| **Clipping (PPO)** | Keeps policy change within a fixed range |\n",
                "| **Line Search (TRPO)** | Finds max step within constraint |\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Use Cases**\n",
                "\n",
                "| Environment       | Why Trust Region Helps               |\n",
                "|-------------------|--------------------------------------|\n",
                "| Robotics          | Prevents unstable, jerky actions     |\n",
                "| Financial RL      | Avoids overreacting to market noise  |\n",
                "| Competitive games | Keeps learned tactics from vanishing |\n",
                "| Sparse rewards    | Prevents overfitting to one big win  |\n",
                "\n",
                "---\n",
                "\n",
                "### **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "#### üîπ **KL-Constrained Optimization (TRPO)**\n",
                "\n",
                "Maximize:\n",
                "$$\n",
                "\\mathbb{E} \\left[ \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} A(s,a) \\right]\n",
                "$$\n",
                "\n",
                "Subject to:\n",
                "$$\n",
                "\\mathbb{E} \\left[ \\text{KL}[\\pi_{\\theta_{\\text{old}}}(\\cdot|s) \\| \\pi_\\theta(\\cdot|s)] \\right] \\leq \\delta\n",
                "$$\n",
                "\n",
                "#### üîπ **Clipped Objective (PPO)**\n",
                "\n",
                "PPO approximates this by **penalizing large updates**:\n",
                "\n",
                "$$\n",
                "L^{\\text{CLIP}} = \\min \\left( r_t(\\theta) A_t, \\ \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t \\right)\n",
                "$$\n",
                "\n",
                "No KL constraint needed ‚Äî the **clip does the job**.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Math Intuition**\n",
                "\n",
                "- You want to **move toward better actions** (using the Advantage).\n",
                "- But if that move takes you **too far**, it might invalidate what you've learned.\n",
                "- A **trust region** ensures each update improves policy performance **without destabilizing it**.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Assumptions & Constraints**\n",
                "\n",
                "| Assumes...                   | Pitfalls                            |\n",
                "|------------------------------|-------------------------------------|\n",
                "| KL is well-approximated      | Poor estimation ruins constraint    |\n",
                "| Policy is stochastic         | Doesn‚Äôt work for deterministic rules |\n",
                "| Advantage estimates are clean| Noisy advantages can still cause chaos |\n",
                "\n",
                "---\n",
                "\n",
                "### **3. Critical Analysis** üîç\n",
                "\n",
                "#### üî∏ Strengths vs Weaknesses\n",
                "\n",
                "| Strengths                          | Weaknesses                           |\n",
                "|------------------------------------|--------------------------------------|\n",
                "| Avoids policy collapse             | May slow convergence if too conservative |\n",
                "| Adds theoretical safety guarantees| KL needs to be tracked/costly        |\n",
                "| Works well with entropy bonuses    | Sensitive to Œµ / Œ¥ tuning            |\n",
                "\n",
                "---\n",
                "\n",
                "#### üî∏ Ethical Lens\n",
                "\n",
                "- **Safer exploration** in real-world domains like robotics and healthcare  \n",
                "- Helps avoid **catastrophic forgetting**, common in online learning  \n",
                "- Encourages **gradual, stable adaptation** ‚Äî more human-like learning\n",
                "\n",
                "---\n",
                "\n",
                "#### üî¨ Research Enhancements\n",
                "\n",
                "- **PPO-KL**: PPO with a soft KL penalty instead of a clip  \n",
                "- **TRPO**: True trust-region method with conjugate gradient optimizer  \n",
                "- **Lagrangian PPO**: Adapts Œµ dynamically based on recent KL trends  \n",
                "- **Conservative Q-Learning (CQL)**: Adds conservative safety even for Q-learning methods\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Interactive Elements** üéØ\n",
                "\n",
                "#### ‚úÖ Concept Check\n",
                "\n",
                "**Q:** What‚Äôs the main benefit of using a trust region in policy updates?\n",
                "\n",
                "A. It increases sample complexity  \n",
                "B. It guarantees faster training  \n",
                "C. It prevents large, destabilizing policy shifts  \n",
                "D. It avoids computing KL divergence\n",
                "\n",
                "‚úÖ **Correct Answer:** C\n",
                "\n",
                "---\n",
                "\n",
                "#### üß™ Code Fix Task\n",
                "\n",
                "```python\n",
                "# Bug: No trust region ‚Üí policy shifts too aggressively\n",
                "loss = (log_probs - old_log_probs) * advantages\n",
                "\n",
                "# ‚úÖ Fix: Use PPO clipping\n",
                "ratio = torch.exp(log_probs - old_log_probs)\n",
                "clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "loss = -torch.min(ratio * advantages, clipped * advantages).mean()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. Glossary**\n",
                "\n",
                "| Term           | Meaning |\n",
                "|----------------|---------|\n",
                "| KL Divergence  | A distance metric between two distributions |\n",
                "| Trust Region   | A small update zone where policy changes are safe |\n",
                "| Clipping       | Restricts how far policy can change |\n",
                "| Surrogate Loss | A simplified loss function that approximates true objective |\n",
                "| Line Search    | Algorithm to find how far to move without violating constraints |\n",
                "\n",
                "---\n",
                "\n",
                "### **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "#### üîß Hyperparameters\n",
                "\n",
                "| Param           | Typical Range          |\n",
                "|------------------|------------------------|\n",
                "| Clipping range   | Œµ = 0.1 ‚Äì 0.3          |\n",
                "| KL penalty (if used) | Œ≤ = 0.5 ‚Äì 2.0     |\n",
                "| Early stopping on KL | Trigger at KL > 1.5 √ó target |\n",
                "\n",
                "#### üìè Evaluation Metrics\n",
                "\n",
                "- KL divergence per update  \n",
                "- Ratio histogram (mean ‚âà 1.0)  \n",
                "- Clipping percentage  \n",
                "- Average return over episodes\n",
                "\n",
                "#### üõ†Ô∏è Production Tips\n",
                "\n",
                "- Monitor **KL divergence** and use **early stopping**  \n",
                "- **Anneal Œµ** (clip range) as training progresses  \n",
                "- **Adaptive KL penalty** works better in stochastic environments  \n",
                "- Use **TensorBoard** or **WandB** to track ratios & KL in real time\n",
                "\n",
                "---\n",
                "\n",
                "### **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "def clipped_surrogate_loss(new_log_probs, old_log_probs, advantages, epsilon=0.2):\n",
                "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
                "    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "    return -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
                "\n",
                "# KL tracking (optional for logging)\n",
                "def compute_kl(old_log_probs, new_log_probs):\n",
                "    return (old_log_probs.exp() * (old_log_probs - new_log_probs)).mean()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You now get the **why and how** behind PPO‚Äôs safety ‚Äî **trust region = smarter policy evolution**, no step too big, no knowledge lost.\n",
                "\n",
                "üöÄ Want to roll into **‚ÄúTraining Actor-Critic with PPO‚Äù** next, or detour into **continuous-action PPO setup** with `GaussianPolicy`?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "\n",
                "# ----- Actor-Critic Networks -----\n",
                "\n",
                "class Actor(nn.Module):\n",
                "    def __init__(self, obs_dim, act_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, act_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "class Critic(nn.Module):\n",
                "    def __init__(self, obs_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x).squeeze(-1)\n",
                "\n",
                "# ----- PPO Loss Function (Trust Region) -----\n",
                "\n",
                "def ppo_loss(new_log_probs, old_log_probs, advantages, epsilon=0.2):\n",
                "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
                "    clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "    loss = -torch.min(ratio * advantages, clipped * advantages).mean()\n",
                "    return loss\n",
                "\n",
                "# ----- KL Divergence Tracking -----\n",
                "\n",
                "def compute_kl(old_log_probs, new_log_probs):\n",
                "    return (old_log_probs.exp() * (old_log_probs - new_log_probs)).mean()\n",
                "\n",
                "# ----- Training Step -----\n",
                "\n",
                "def train_ppo_step(actor, critic, opt_actor, opt_critic,\n",
                "                   states, actions, returns, old_log_probs,\n",
                "                   epsilon=0.2):\n",
                "\n",
                "    # Critic forward\n",
                "    values = critic(states)\n",
                "    advantages = returns - values.detach()\n",
                "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
                "\n",
                "    # Actor forward\n",
                "    probs = actor(states)\n",
                "    dist = torch.distributions.Categorical(probs)\n",
                "    new_log_probs = dist.log_prob(actions)\n",
                "\n",
                "    # Losses\n",
                "    a_loss = ppo_loss(new_log_probs, old_log_probs, advantages, epsilon)\n",
                "    c_loss = F.mse_loss(values, returns)\n",
                "\n",
                "    # KL (optional)\n",
                "    kl_div = compute_kl(old_log_probs, new_log_probs).item()\n",
                "\n",
                "    # Backprop\n",
                "    opt_actor.zero_grad()\n",
                "    a_loss.backward()\n",
                "    opt_actor.step()\n",
                "\n",
                "    opt_critic.zero_grad()\n",
                "    c_loss.backward()\n",
                "    opt_critic.step()\n",
                "\n",
                "    return a_loss.item(), c_loss.item(), kl_div\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let‚Äôs wrap this like it deserves ‚Äî ultra-clean, structured, and dialed to UTHU spec.\n",
                "\n",
                "---\n",
                "\n",
                "# üß© **Why PPO Works Well for Large Environments**  \n",
                "üåç *Scalability without chaos ‚Äî the PPO edge.*\n",
                "\n",
                "---\n",
                "\n",
                "### **1. Conceptual Foundation**\n",
                "\n",
                "#### üîπ **Purpose & Relevance**\n",
                "\n",
                "Large environments ‚Äî like complex games, robotics systems, or high-dimensional control tasks ‚Äî introduce:\n",
                "- **High variance**\n",
                "- **Long episodes**\n",
                "- **Sparse rewards**\n",
                "- **Millions of parameters**\n",
                "\n",
                "PPO thrives here because it balances:\n",
                "- **Exploration and stability** (via clipping or KL)\n",
                "- **Sample efficiency** (reuse old policies safely)\n",
                "- **Simplicity** (first-order, not heavy math)\n",
                "\n",
                "> üß† **Analogy**:  \n",
                "> Training in a massive environment without PPO is like doing surgery in a hurricane. PPO gives you a **windshield, gloves, and a scalpel** instead of a chainsaw.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Key Terminology**\n",
                "\n",
                "| Term           | Meaning |\n",
                "|----------------|---------|\n",
                "| **Large Environment** | One with huge state/action space or long horizons |\n",
                "| **Sample Efficiency** | How much learning happens per environment step |\n",
                "| **Clipped Objective** | PPO‚Äôs stable update strategy |\n",
                "| **Entropy Bonus**     | Term to encourage exploration in sparse-reward settings |\n",
                "| **Mini-batching**     | Divides large rollouts into tractable chunks |\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Use Cases**\n",
                "\n",
                "| Domain                         | Why PPO helps                           |\n",
                "|--------------------------------|------------------------------------------|\n",
                "| üïπÔ∏è OpenAI Gym (Atari, Mujoco)  | Stabilizes learning from long sequences |\n",
                "| ü§ñ Robotics (Sim-to-Real)       | Prevents catastrophic policy jumps      |\n",
                "| üöò Autonomous driving sims      | Handles continuous, long-horizon control |\n",
                "| üéÆ Multi-agent games            | Avoids policy collapse in joint spaces  |\n",
                "\n",
                "---\n",
                "\n",
                "### **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "#### üîπ **Scalable Objective with Clipping**\n",
                "\n",
                "Key property: It **limits how much the policy can change**, even if advantage is large.\n",
                "\n",
                "PPO optimizes:\n",
                "$$\n",
                "L^{\\text{CLIP}} = \\mathbb{E}_t \\left[ \\min \\left( r_t A_t, \\text{clip}(r_t, 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( r_t \\) is the **probability ratio** (new / old policy)\n",
                "- \\( A_t \\) is the **advantage**\n",
                "- The **clip** ensures **bounded updates**\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Math Intuition**\n",
                "\n",
                "- **No second-order derivatives**: Unlike TRPO, PPO just uses standard gradient descent\n",
                "- It **trades off precision for speed**, but does so **reliably**\n",
                "- In large spaces, **overshooting is fatal** ‚Äî PPO prevents that\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Assumptions & Constraints**\n",
                "\n",
                "| Assumes...                       | Pitfalls                                 |\n",
                "|----------------------------------|------------------------------------------|\n",
                "| Environment has enough signal    | Sparse reward = hard even with PPO       |\n",
                "| Advantage estimates are good     | Bad Critic = bad updates                 |\n",
                "| Œµ is tuned properly              | Too tight = no learning, too loose = chaos |\n",
                "\n",
                "---\n",
                "\n",
                "### **3. Critical Analysis** üîç\n",
                "\n",
                "#### üî∏ Strengths vs Weaknesses\n",
                "\n",
                "| Strengths                                   | Weaknesses                               |\n",
                "|---------------------------------------------|------------------------------------------|\n",
                "| Works out-of-the-box in large envs          | Needs rollout buffer & advantage calc    |\n",
                "| Easier to tune than TRPO                    | Still sensitive to Œµ, batch size         |\n",
                "| Stable across large policy updates          | Not optimal for extremely sparse rewards |\n",
                "\n",
                "---\n",
                "\n",
                "#### üî∏ Ethical Lens\n",
                "\n",
                "- **Safer learning** in high-stakes systems (health, driving)  \n",
                "- **Predictable improvements** ‚Üí reduces risk of RL going off the rails  \n",
                "- **Fairer competition** in multi-agent games (less oscillation in updates)\n",
                "\n",
                "---\n",
                "\n",
                "#### üî¨ Research Highlights (Post-2020)\n",
                "\n",
                "- **MAPPO** (Multi-Agent PPO): Robust for multi-agent coordination tasks  \n",
                "- **LAG-PPO**: Uses Lagrange multipliers to adapt Œµ during training  \n",
                "- **PPO-X**: Variants that integrate curiosity, population training, or curriculum learning  \n",
                "- **Safe RL with PPO**: Used in real-world robotics & energy systems (OpenAI, DeepMind, etc.)\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Interactive Elements** üéØ\n",
                "\n",
                "#### ‚úÖ Concept Check\n",
                "\n",
                "**Q:** Why is PPO more stable in large environments than standard policy gradients?\n",
                "\n",
                "A. It trains the Critic separately  \n",
                "B. It uses tabular Q-values  \n",
                "C. It clips the size of the policy update  \n",
                "D. It adds more reward shaping\n",
                "\n",
                "‚úÖ **Correct Answer:** C\n",
                "\n",
                "---\n",
                "\n",
                "#### üß™ Code Challenge\n",
                "\n",
                "```python\n",
                "# Bug: Large update causes instability in big env\n",
                "loss = ratio * advantages\n",
                "\n",
                "# ‚úÖ Fix: Use clipped ratio to restrict magnitude\n",
                "clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. Glossary**\n",
                "\n",
                "| Term             | Meaning |\n",
                "|------------------|---------|\n",
                "| Trust Region     | Bound on how far the policy can move per step |\n",
                "| Probability Ratio| New policy divided by old, action-wise |\n",
                "| Clipping         | Limits how much the ratio can move away from 1.0 |\n",
                "| Entropy Bonus    | Term added to keep policy stochastic |\n",
                "| Sample Efficiency| Ability to learn more with fewer steps |\n",
                "\n",
                "---\n",
                "\n",
                "### **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "#### üîß Hyperparameters\n",
                "\n",
                "| Parameter        | Typical Value      |\n",
                "|------------------|--------------------|\n",
                "| Clip Range       | 0.1 ‚Äì 0.3          |\n",
                "| Batch Size       | 2048 ‚Äì 8192        |\n",
                "| Epochs per update| 5 ‚Äì 10             |\n",
                "| Mini-batch size  | 64 ‚Äì 256           |\n",
                "| Advantage Œª (GAE)| 0.95 ‚Äì 0.99        |\n",
                "\n",
                "#### üìè Evaluation Metrics\n",
                "\n",
                "- Average return per episode  \n",
                "- KL divergence between policies  \n",
                "- Clipping percentage (should be 10‚Äì20%)  \n",
                "- Policy entropy (exploration indicator)\n",
                "\n",
                "#### üõ†Ô∏è Production Tips\n",
                "\n",
                "- Use **advantage normalization**  \n",
                "- Monitor KL divergence per update  \n",
                "- Run with **parallel environments** for faster rollout  \n",
                "- For large envs: use **reward scaling** and **value normalization**\n",
                "\n",
                "---\n",
                "\n",
                "### **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "def ppo_clipped_update(log_probs, old_log_probs, advantages, epsilon=0.2):\n",
                "    ratio = torch.exp(log_probs - old_log_probs)\n",
                "    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "    loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
                "    return loss\n",
                "\n",
                "def compute_kl_divergence(old_log_probs, new_log_probs):\n",
                "    return (old_log_probs.exp() * (old_log_probs - new_log_probs)).mean()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ That‚Äôs how PPO keeps it **scalable, stable, and sample-efficient**, even when the environment is **big, messy, or high-dimensional**.\n",
                "\n",
                "üöÄ Next? Wanna break down **reward normalization and variance reduction** in PPO training?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "\n",
                "# ----- Actor Network -----\n",
                "class Actor(nn.Module):\n",
                "    def __init__(self, obs_dim, act_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, act_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "\n",
                "    def forward(self, state):\n",
                "        return self.net(state)\n",
                "\n",
                "# ----- Critic Network -----\n",
                "class Critic(nn.Module):\n",
                "    def __init__(self, obs_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, state):\n",
                "        return self.net(state).squeeze(-1)\n",
                "\n",
                "# ----- PPO Loss with Trust Region (Clipped Objective) -----\n",
                "def compute_ppo_loss(new_log_probs, old_log_probs, advantages, epsilon=0.2):\n",
                "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
                "    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "    surrogate_loss = torch.min(ratio * advantages, clipped_ratio * advantages)\n",
                "    return -surrogate_loss.mean()\n",
                "\n",
                "# ----- KL Divergence Tracker (Optional for Diagnostics) -----\n",
                "def compute_kl(old_log_probs, new_log_probs):\n",
                "    return (old_log_probs.exp() * (old_log_probs - new_log_probs)).mean()\n",
                "\n",
                "# ----- Full PPO Training Step -----\n",
                "def train_ppo(actor, critic, opt_actor, opt_critic,\n",
                "              states, actions, returns, old_log_probs,\n",
                "              epsilon=0.2):\n",
                "\n",
                "    # ===== Critic Forward Pass =====\n",
                "    values = critic(states)\n",
                "    advantages = returns - values.detach()\n",
                "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
                "\n",
                "    # ===== Actor Forward Pass =====\n",
                "    probs = actor(states)\n",
                "    dist = torch.distributions.Categorical(probs)\n",
                "    new_log_probs = dist.log_prob(actions)\n",
                "\n",
                "    # ===== PPO Losses =====\n",
                "    actor_loss = compute_ppo_loss(new_log_probs, old_log_probs, advantages, epsilon)\n",
                "    critic_loss = F.mse_loss(values, returns)\n",
                "    kl_div = compute_kl(old_log_probs, new_log_probs).item()\n",
                "\n",
                "    # ===== Backprop & Optim Step =====\n",
                "    opt_actor.zero_grad()\n",
                "    actor_loss.backward()\n",
                "    opt_actor.step()\n",
                "\n",
                "    opt_critic.zero_grad()\n",
                "    critic_loss.backward()\n",
                "    opt_critic.step()\n",
                "\n",
                "    return {\n",
                "        \"actor_loss\": actor_loss.item(),\n",
                "        \"critic_loss\": critic_loss.item(),\n",
                "        \"kl_divergence\": kl_div\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Locked in and executing üß†‚öôÔ∏è  \n",
                "Here‚Äôs the full **UTHU-formatted breakdown** for:\n",
                "\n",
                "---\n",
                "\n",
                "# üß© **Reward Normalization and Variance Reduction in PPO**  \n",
                "üéØ *Make signals sharp. Make updates stable. Learn like a pro.*\n",
                "\n",
                "---\n",
                "\n",
                "### **1. Conceptual Foundation**\n",
                "\n",
                "#### üîπ **Purpose & Relevance**\n",
                "\n",
                "Raw rewards in reinforcement learning can be:\n",
                "- **Noisy**\n",
                "- **Sparse**\n",
                "- **Wildly variable across episodes**\n",
                "\n",
                "That makes the learning signal chaotic.  \n",
                "üëâ **Reward normalization** and **variance reduction** techniques:\n",
                "- Smooth the advantage signal\n",
                "- Stabilize gradients\n",
                "- Speed up training convergence\n",
                "\n",
                "> üß† **Analogy**:  \n",
                "> If your rewards are like **uneven terrain**, normalization is like putting on **shock absorbers** ‚Äî letting the agent move faster without crashing from bumps.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Key Terminology**\n",
                "\n",
                "| Term                 | Explanation |\n",
                "|----------------------|-------------|\n",
                "| **Reward Normalization** | Scaling or centering rewards for consistency |\n",
                "| **Advantage Function**   | Measures how much better an action was compared to average |\n",
                "| **Variance Reduction**   | Techniques to reduce noise in gradients |\n",
                "| **Baseline**             | Value subtracted to center the reward signal |\n",
                "| **Standardization**      | Transform to zero-mean, unit-variance distribution |\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Use Cases**\n",
                "\n",
                "| Use Case               | Why Normalize Rewards                |\n",
                "|------------------------|--------------------------------------|\n",
                "| Sparse reward envs     | Prevents exploding gradients         |\n",
                "| Multi-task agents      | Aligns scale of different objectives |\n",
                "| Online games/real-time | Faster convergence with less jitter  |\n",
                "| Large episodic variance| Keeps advantage values consistent    |\n",
                "\n",
                "---\n",
                "\n",
                "### **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "#### üîπ **Core Equations**\n",
                "\n",
                "Advantage function before normalization:\n",
                "$$\n",
                "A_t = R_t - V(s_t)\n",
                "$$\n",
                "\n",
                "After normalization:\n",
                "$$\n",
                "A_t^{\\text{norm}} = \\frac{A_t - \\mu_A}{\\sigma_A + \\epsilon}\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( R_t \\) = return (can be GAE, TD(Œª), etc.)\n",
                "- \\( \\mu_A \\) = mean of advantage across batch\n",
                "- \\( \\sigma_A \\) = std deviation\n",
                "\n",
                "This stabilized \\( A_t^{\\text{norm}} \\) is used in the PPO loss.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Math Intuition**\n",
                "\n",
                "- Gradient magnitude in PG methods is proportional to \\( A_t \\)\n",
                "- If \\( A_t \\) is huge, gradients explode  \n",
                "- If it's noisy, direction wobbles  \n",
                "- Normalizing \\( A_t \\) flattens the terrain so the agent climbs smoothly\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Assumptions & Constraints**\n",
                "\n",
                "| Assumes...                       | Pitfalls                             |\n",
                "|----------------------------------|--------------------------------------|\n",
                "| Reward distribution is variable  | Constant reward = no need to normalize |\n",
                "| Advantage estimation is accurate | Noisy critic ruins normalization     |\n",
                "| Batch size is reasonable         | Too small ‚Üí bad variance estimate    |\n",
                "\n",
                "---\n",
                "\n",
                "### **3. Critical Analysis** üîç\n",
                "\n",
                "#### üî∏ Strengths vs Weaknesses\n",
                "\n",
                "| Strengths                                 | Weaknesses                          |\n",
                "|-------------------------------------------|-------------------------------------|\n",
                "| Helps with sparse & delayed rewards       | Adds computation overhead           |\n",
                "| Prevents unstable gradient updates        | Risk of over-normalization if batch is small |\n",
                "| Easy to implement and tune                | Needs consistent batch processing   |\n",
                "\n",
                "---\n",
                "\n",
                "#### üî∏ Ethical Lens\n",
                "\n",
                "- **Less risk of divergence** in sensitive systems (e.g., finance, healthcare)  \n",
                "- **Aligns training** across different environments with fairness  \n",
                "- Avoids unintended behavior due to outlier rewards\n",
                "\n",
                "---\n",
                "\n",
                "#### üî¨ Research Highlights\n",
                "\n",
                "- **Generalized Advantage Estimation (GAE)**  \n",
                "  - Combines bias and variance tradeoff using TD(Œª)\n",
                "- **Whitening rewards** for meta-RL: Makes adaptation across tasks faster\n",
                "- **Reward scaling in continuous control** (SAC, PPO-Clip) for better convergence\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Interactive Elements** üéØ\n",
                "\n",
                "#### ‚úÖ Concept Check\n",
                "\n",
                "**Q:** Why is reward normalization useful in PPO?\n",
                "\n",
                "A. It speeds up GPU computation  \n",
                "B. It reduces gradient noise and improves stability  \n",
                "C. It increases reward magnitude  \n",
                "D. It removes the need for the Critic\n",
                "\n",
                "‚úÖ **Correct Answer:** B\n",
                "\n",
                "---\n",
                "\n",
                "#### üß™ Code Exercise ‚Äì Normalize Advantage\n",
                "\n",
                "```python\n",
                "# Bug: Unnormalized advantages cause instability\n",
                "advantages = returns - values\n",
                "\n",
                "# ‚úÖ Fix: Normalize advantage\n",
                "advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. Glossary**\n",
                "\n",
                "| Term             | Meaning |\n",
                "|------------------|---------|\n",
                "| Advantage        | How much better an action was than average |\n",
                "| Reward Normalization | Scaling rewards for stability |\n",
                "| Variance         | Measure of how spread out a signal is |\n",
                "| Gradient Noise   | Unstable learning signal from raw rewards |\n",
                "| Standardization  | Zero-mean, unit-variance transformation |\n",
                "\n",
                "---\n",
                "\n",
                "### **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "#### üîß Hyperparameters & Tips\n",
                "\n",
                "| Name             | Advice |\n",
                "|------------------|--------|\n",
                "| Batch size       | Use ‚â• 1024 for reliable statistics |\n",
                "| Normalization    | Always normalize **advantage**, optionally **reward** |\n",
                "| Advantage filter | Smooth over time using exponential decay (optional) |\n",
                "\n",
                "#### üìè Evaluation Metrics\n",
                "\n",
                "- Variance of advantage signal  \n",
                "- PPO update stability (reward curve)  \n",
                "- KL divergence trend after update  \n",
                "- Return per episode standard deviation\n",
                "\n",
                "#### üõ†Ô∏è Production Tips\n",
                "\n",
                "- Normalize **after** computing advantages  \n",
                "- Avoid normalizing with tiny batches  \n",
                "- Monitor `advantage.std()` ‚Äî too high = noisy, too low = dead signal  \n",
                "- Use `reward-to-go` with optional GAE for best signal\n",
                "\n",
                "---\n",
                "\n",
                "### **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "def compute_normalized_advantages(returns, values):\n",
                "    advantages = returns - values.detach()\n",
                "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
                "    return advantages\n",
                "\n",
                "# In training step\n",
                "advantages = compute_normalized_advantages(returns, values)\n",
                "\n",
                "# Then used in PPO clipped loss:\n",
                "ratio = torch.exp(new_log_probs - old_log_probs)\n",
                "clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "actor_loss = -torch.min(ratio * advantages, clipped * advantages).mean()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ Clean, stable updates powered by **normalized rewards** and **variance reduction** ‚Äî just how PPO keeps its edge in chaotic environments.\n",
                "\n",
                "Ready to move into:\n",
                "‚Üí `Optimizing the value function and policy simultaneously`  \n",
                "or  \n",
                "‚Üí `Implementing PPO in PyTorch end-to-end`?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Absolutely! Let's build a **structured summary** of:\n",
                "\n",
                "---\n",
                "\n",
                "# üéØ **Optimizing the Value Function and Policy Simultaneously in Reinforcement Learning**\n",
                "\n",
                "---\n",
                "\n",
                "### **1. Conceptual Foundation**  \n",
                "\n",
                "#### **Purpose & Relevance**  \n",
                "Imagine training a robot to walk. The *policy* is the brain telling it what action to take (e.g., step forward). The *value function* is its inner critic, telling it how ‚Äúgood‚Äù it feels being in a state.  \n",
                "Instead of training them separately (like giving someone gym and therapy separately), **optimizing them together** means they *learn from each other* ‚Äî like a coach and athlete syncing feedback.\n",
                "\n",
                "üîÑ **Why it matters**:  \n",
                "It leads to **faster convergence**, **better stability**, and **smarter exploration** in Reinforcement Learning (RL). Classic methods like Q-learning often optimize value only. Policy Gradient methods optimize policy only. But the sweet spot? Doing both. That‚Äôs what Actor-Critic methods do.\n",
                "\n",
                "---\n",
                "\n",
                "#### **Key Terminology**\n",
                "\n",
                "| Term | Metaphor / Analogy |\n",
                "|------|--------------------|\n",
                "| **Policy (œÄ)** | Like muscle memory ‚Äì what action to take in a given situation.  \n",
                "| **Value Function (V)** | Inner voice rating the long-term reward from a place.  \n",
                "| **Actor-Critic** | Actor = doer, Critic = evaluator. Together = learning with feedback.  \n",
                "| **Advantage Function (A)** | Like a friend telling you whether your action was smarter than average.  \n",
                "| **Entropy Bonus** | Encouragement to explore more, like giving candy for trying new things.\n",
                "\n",
                "---\n",
                "\n",
                "#### **Use Cases**\n",
                "\n",
                "‚úÖ **Robotics** (locomotion, grasping)  \n",
                "‚úÖ **Game-playing AI** (Atari, Go, StarCraft)  \n",
                "‚úÖ **Portfolio management**  \n",
                "‚úÖ **Traffic control systems**  \n",
                "\n",
                "**Decision Flow (ASCII):**\n",
                "```\n",
                "          [Environment]\n",
                "               ‚Üì\n",
                "         [Agent chooses Action]\n",
                "               ‚Üì\n",
                "       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "       ‚îÇ  Actor        ‚îÇ ‚Üí Outputs Action\n",
                "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "               ‚Üì\n",
                "          [Environment returns Reward + Next State]\n",
                "               ‚Üë\n",
                "       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "       ‚îÇ  Critic       ‚îÇ ‚Üí Evaluates Action\n",
                "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **2. Mathematical Deep Dive** üßÆ  \n",
                "\n",
                "#### **Core Equations**\n",
                "\n",
                "‚úì Policy Objective (actor):\n",
                "\n",
                "$$\n",
                "J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\log \\pi_\\theta(a|s) A(s,a) \\right]\n",
                "$$\n",
                "\n",
                "‚úì Value Loss (critic):\n",
                "\n",
                "$$\n",
                "L(\\phi) = \\left( V_\\phi(s) - \\hat{V}(s) \\right)^2\n",
                "$$\n",
                "\n",
                "‚úì Total Loss:\n",
                "\n",
                "$$\n",
                "L_{\\text{total}} = -J(\\theta) + \\lambda L(\\phi)\n",
                "$$\n",
                "\n",
                "---\n",
                "\n",
                "#### **Math Intuition**\n",
                "\n",
                "- Policy gradient nudges policy toward actions that led to higher-than-expected rewards.\n",
                "- The value function is like estimating the slope of a hill ‚Äî you want a smoother, more accurate surface to climb.\n",
                "- The advantage function is like subtracting your baseline happiness from the actual joy of your action.\n",
                "\n",
                "---\n",
                "\n",
                "#### **Assumptions & Constraints**\n",
                "\n",
                "- Assumes **Markov Decision Process (MDP)** structure  \n",
                "- **Bootstrapping error** can propagate if critic is wrong  \n",
                "- Actor overfits noisy feedback if critic is unstable  \n",
                "- Requires careful **balance of updates** (Œª tuning)\n",
                "\n",
                "---\n",
                "\n",
                "### **3. Critical Analysis** üîç  \n",
                "\n",
                "#### **Strengths vs Weaknesses**\n",
                "\n",
                "| Strengths | Weaknesses |\n",
                "|----------|------------|\n",
                "| Fast convergence | Sensitive to hyperparams |\n",
                "| Combines benefits of PG + TD | Requires fine-tuned critic |\n",
                "| Online learning | Critic bias can harm policy |\n",
                "\n",
                "---\n",
                "\n",
                "#### **Ethical Lens**\n",
                "\n",
                "‚ö†Ô∏è Can reinforce **unethical strategies** in games or finance.  \n",
                "‚ö†Ô∏è Bias in reward signal = **bias in policy**.  \n",
                "‚ö†Ô∏è Can be exploited for manipulative behavior (e.g. social bots).\n",
                "\n",
                "---\n",
                "\n",
                "#### **Research Updates**\n",
                "\n",
                "- üå± **Soft Actor Critic (SAC)** ‚Äì Entropy-regularized RL\n",
                "- üöÄ **Dreamer-v3 (2023)** ‚Äì Learn world model + policy\n",
                "- üîç **Proximal Policy Optimization (PPO)** ‚Äì Safer gradients\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Interactive Elements** üéØ  \n",
                "\n",
                "#### **Concept Check**\n",
                "\n",
                "**Q:** Why does optimizing actor and critic together help training?\n",
                "\n",
                "a) More random exploration  \n",
                "b) They fix each other‚Äôs errors  \n",
                "c) Critic updates slower  \n",
                "d) Actor doesn‚Äôt need value estimates  \n",
                "\n",
                "‚úÖ **Correct:** **b)** ‚Äì Actor improves using critic feedback; critic improves from new actor rollouts.\n",
                "\n",
                "---\n",
                "\n",
                "#### **Code Debugging Challenge**\n",
                "\n",
                "```python\n",
                "def update_actor(log_probs, advantages):\n",
                "    loss = -log_probs * advantages\n",
                "    loss.backward()  # ‚Üê Bug here\n",
                "    return loss\n",
                "```\n",
                "\n",
                "‚ùå *Issue*: `loss` is a tensor, not scalar.  \n",
                "‚úÖ *Fix*:\n",
                "```python\n",
                "loss = -(log_probs * advantages).mean()\n",
                "loss.backward()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. Glossary** üß†\n",
                "\n",
                "| Term | Meaning |\n",
                "|------|---------|\n",
                "| **Policy** | What action to take in each state |\n",
                "| **Value Function** | Long-term expected reward from a state |\n",
                "| **Advantage** | How much better an action was than expected |\n",
                "| **Entropy** | Uncertainty; helps encourage exploration |\n",
                "| **Critic** | Model that evaluates the state or action-value |\n",
                "| **Actor** | Model that decides which action to take |\n",
                "\n",
                "---\n",
                "\n",
                "### **6. Practical Considerations** ‚öôÔ∏è  \n",
                "\n",
                "#### **Hyperparameters**\n",
                "\n",
                "- **Learning Rate**: Actor (1e-4), Critic (1e-3)\n",
                "- **Œª**: Balances critic loss\n",
                "- **Entropy Coefficient**: Prevents policy collapse\n",
                "\n",
                "#### **Evaluation Metrics**\n",
                "\n",
                "- **Episode reward**  \n",
                "- **Policy entropy**  \n",
                "- **Value loss** over time\n",
                "\n",
                "```python\n",
                "plt.plot(reward_list)\n",
                "plt.title(\"Episode Reward Over Time\")\n",
                "```\n",
                "\n",
                "#### **Production Tips**\n",
                "\n",
                "- Use **target networks** to stabilize critic  \n",
                "- **Replay buffers** for sample efficiency  \n",
                "- Actor and Critic **update asynchronously** for scale  \n",
                "\n",
                "---\n",
                "\n",
                "### **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "class Actor(nn.Module):\n",
                "    def __init__(self, state_dim, action_dim):\n",
                "        super().__init__()\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(state_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, action_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "\n",
                "    def forward(self, state):\n",
                "        return self.fc(state)\n",
                "\n",
                "class Critic(nn.Module):\n",
                "    def __init__(self, state_dim):\n",
                "        super().__init__()\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(state_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, state):\n",
                "        return self.fc(state)\n",
                "\n",
                "def train(env, actor, critic, episodes=1000, gamma=0.99):\n",
                "    actor_optim = optim.Adam(actor.parameters(), lr=1e-4)\n",
                "    critic_optim = optim.Adam(critic.parameters(), lr=1e-3)\n",
                "    reward_history = []\n",
                "\n",
                "    for episode in range(episodes):\n",
                "        state = env.reset()\n",
                "        log_probs, values, rewards = [], [], []\n",
                "\n",
                "        done = False\n",
                "        while not done:\n",
                "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
                "            probs = actor(state_tensor)\n",
                "            dist = torch.distributions.Categorical(probs)\n",
                "            action = dist.sample()\n",
                "\n",
                "            log_prob = dist.log_prob(action)\n",
                "            value = critic(state_tensor)\n",
                "\n",
                "            next_state, reward, done, _ = env.step(action.item())\n",
                "            \n",
                "            log_probs.append(log_prob)\n",
                "            values.append(value)\n",
                "            rewards.append(reward)\n",
                "\n",
                "            state = next_state\n",
                "\n",
                "        # Compute returns\n",
                "        returns = []\n",
                "        G = 0\n",
                "        for r in reversed(rewards):\n",
                "            G = r + gamma * G\n",
                "            returns.insert(0, G)\n",
                "\n",
                "        returns = torch.tensor(returns)\n",
                "        values = torch.cat(values).squeeze()\n",
                "        log_probs = torch.stack(log_probs)\n",
                "\n",
                "        advantage = returns - values.detach()\n",
                "\n",
                "        actor_loss = -(log_probs * advantage).mean()\n",
                "        critic_loss = ((values - returns) ** 2).mean()\n",
                "\n",
                "        actor_optim.zero_grad()\n",
                "        actor_loss.backward()\n",
                "        actor_optim.step()\n",
                "\n",
                "        critic_optim.zero_grad()\n",
                "        critic_loss.backward()\n",
                "        critic_optim.step()\n",
                "\n",
                "        reward_history.append(sum(rewards))\n",
                "\n",
                "    plt.plot(reward_history)\n",
                "    plt.title(\"Training Reward over Time\")\n",
                "    plt.xlabel(\"Episode\")\n",
                "    plt.ylabel(\"Reward\")\n",
                "    plt.show()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "Let me know if you‚Äôd like a Colab notebook, visual diagram, or connect this to PPO/SAC/DDPG next."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Absolutely locked in. No shortcuts. No pieces missing. Let‚Äôs build **PPO end-to-end in PyTorch** the **UTHU** way üß†‚öôÔ∏è\n",
                "\n",
                "---\n",
                "\n",
                "# üß© **Implementing PPO with PyTorch**  \n",
                "üß™ *Code that learns. Logs that matter. Results that converge.*\n",
                "\n",
                "---\n",
                "\n",
                "### **1. Conceptual Foundation**\n",
                "\n",
                "#### üîπ **Purpose & Relevance**\n",
                "\n",
                "PPO is widely used because it's:\n",
                "- **Robust** across tasks\n",
                "- **Efficient** in parallelized environments\n",
                "- **Simple to implement** with standard deep learning tools\n",
                "\n",
                "Whether you're training an agent in **OpenAI Gym**, a robotic arm, or an RL-based recommender system, PPO in PyTorch offers plug-and-play power.\n",
                "\n",
                "> üß† **Analogy**:  \n",
                "> PPO is like a racing car with automatic traction control. PyTorch gives you the steering wheel with traction already built-in. It‚Äôs fast **and** safe.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Key Terminology**\n",
                "\n",
                "| Term             | Description |\n",
                "|------------------|-------------|\n",
                "| **Actor**        | Chooses the next action |\n",
                "| **Critic**       | Evaluates how good the state is |\n",
                "| **Trajectory**   | A full episode of experience (states ‚Üí rewards) |\n",
                "| **Rollout Buffer** | Stores experiences to train on |\n",
                "| **Clipped Objective** | Prevents updates from being too big |\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ **Use Cases**\n",
                "\n",
                "- OpenAI Gym classic control (CartPole, LunarLander)\n",
                "- Mujoco / PyBullet robotics simulation\n",
                "- Custom environments (healthcare, finance)\n",
                "- Multi-agent policy learning\n",
                "\n",
                "---\n",
                "\n",
                "### **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "#### üîπ PPO Core Losses\n",
                "\n",
                "Actor loss:\n",
                "\n",
                "$$\n",
                "L^{\\text{CLIP}} = \\mathbb{E}_t \\left[ \\min \\left( r_t A_t, \\ \\text{clip}(r_t, 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]\n",
                "$$\n",
                "\n",
                "Critic loss (MSE):\n",
                "\n",
                "$$\n",
                "L_{\\text{critic}} = \\frac{1}{2} \\mathbb{E}_t \\left[ \\left( V(s_t) - R_t \\right)^2 \\right]\n",
                "$$\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ Math Intuition\n",
                "\n",
                "- Actor pushes toward better actions using **advantages**.\n",
                "- Critic updates based on how wrong it was about state values.\n",
                "- Clip stops policy from diverging in one step.\n",
                "\n",
                "---\n",
                "\n",
                "#### üîπ Assumptions & Constraints\n",
                "\n",
                "| Assumes...                       | Pitfalls                           |\n",
                "|----------------------------------|------------------------------------|\n",
                "| GAE or bootstrapped returns      | Poor return estimate = bad signal  |\n",
                "| Batches large enough for stable stats | Small batches = noisy updates   |\n",
                "| Shared state representation      | Separate models if domains diverge |\n",
                "\n",
                "---\n",
                "\n",
                "### **3. Critical Analysis** üîç\n",
                "\n",
                "#### üî∏ Strengths vs Weaknesses\n",
                "\n",
                "| Strengths                     | Weaknesses                            |\n",
                "|------------------------------|----------------------------------------|\n",
                "| Simple and scalable          | Sensitive to advantage noise          |\n",
                "| No second-order gradients    | Needs tuning of clip + epochs         |\n",
                "| Works with discrete/continuous spaces | Not ideal for highly sparse rewards |\n",
                "\n",
                "---\n",
                "\n",
                "#### üî∏ Ethical Lens\n",
                "\n",
                "- Safer updates ‚Üí fewer unintended behaviors  \n",
                "- Fairer in multi-agent environments  \n",
                "- Logging KL & entropy improves explainability\n",
                "\n",
                "---\n",
                "\n",
                "#### üî¨ Research Add-ons\n",
                "\n",
                "- **Lagrangian PPO** (auto-adaptive clip range)  \n",
                "- **MAPPO** for multi-agent games  \n",
                "- **Recurrent PPO** (handles memory)  \n",
                "- **Dreamer/PPO** hybrids for model-based training\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Interactive Elements** üéØ\n",
                "\n",
                "#### ‚úÖ Concept Check\n",
                "\n",
                "**Q:** Why do we clip the ratio in PPO?\n",
                "\n",
                "A. To encourage exploration  \n",
                "B. To avoid policy collapsing  \n",
                "C. To prevent large destructive updates  \n",
                "D. To reduce memory usage\n",
                "\n",
                "‚úÖ **Answer:** C\n",
                "\n",
                "#### üß™ Debug Task\n",
                "\n",
                "```python\n",
                "# Bug: Too large update\n",
                "loss = ratio * advantages  # no clipping!\n",
                "\n",
                "# ‚úÖ Fix\n",
                "clipped = torch.clamp(ratio, 1 - eps, 1 + eps)\n",
                "loss = -torch.min(ratio * advantages, clipped * advantages).mean()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. Glossary**\n",
                "\n",
                "| Term           | Meaning |\n",
                "|----------------|---------|\n",
                "| **Advantage**  | Estimated benefit of an action over baseline |\n",
                "| **Rollout Buffer** | Stores trajectory data for training |\n",
                "| **PPO Clip**   | Restricts how much the policy can shift |\n",
                "| **Policy Network (Actor)** | Outputs action probabilities |\n",
                "| **Value Network (Critic)** | Predicts how good a state is |\n",
                "\n",
                "---\n",
                "\n",
                "### **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "#### üîß Hyperparameters\n",
                "\n",
                "| Param           | Range |\n",
                "|------------------|-------|\n",
                "| Clip Œµ           | 0.1‚Äì0.3 |\n",
                "| Actor LR         | 2e-4‚Äì3e-4 |\n",
                "| Critic LR        | 1e-3 |\n",
                "| Epochs per update| 4‚Äì10 |\n",
                "| Entropy bonus    | 0.01‚Äì0.05 |\n",
                "\n",
                "#### üìè Evaluation Metrics\n",
                "\n",
                "- Total episodic return  \n",
                "- Actor loss + critic loss  \n",
                "- KL divergence (should stay < 0.01 ideally)  \n",
                "- Clipping ratio (shouldn't exceed 30%)  \n",
                "- Entropy (exploration tracking)\n",
                "\n",
                "#### üõ†Ô∏è Production Tips\n",
                "\n",
                "- Normalize advantages  \n",
                "- Clip gradients if Critic explodes  \n",
                "- Use `wandb`/`tensorboard` to monitor actor, critic, KL  \n",
                "- Always track entropy to ensure ongoing exploration\n",
                "\n",
                "---\n",
                "\n",
                "### **7. Full Python Code Cell** üêç  \n",
                "‚ú® **Full PPO + Gym + Training Loop (PyTorch, Discrete Actions)**\n",
                "\n",
                "```python\n",
                "import gym\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import numpy as np\n",
                "\n",
                "# Set up Actor-Critic networks\n",
                "class Actor(nn.Module):\n",
                "    def __init__(self, obs_dim, act_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 64),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(64, act_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "\n",
                "    def forward(self, state):\n",
                "        return self.net(state)\n",
                "\n",
                "class Critic(nn.Module):\n",
                "    def __init__(self, obs_dim):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 64),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(64, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, state):\n",
                "        return self.net(state).squeeze(-1)\n",
                "\n",
                "# Advantage Normalization\n",
                "def compute_advantages(returns, values):\n",
                "    adv = returns - values.detach()\n",
                "    return (adv - adv.mean()) / (adv.std() + 1e-8)\n",
                "\n",
                "# PPO Clipped Loss\n",
                "def ppo_loss(new_log_probs, old_log_probs, advantages, eps=0.2):\n",
                "    ratio = (new_log_probs - old_log_probs).exp()\n",
                "    clipped = torch.clamp(ratio, 1 - eps, 1 + eps)\n",
                "    return -torch.min(ratio * advantages, clipped * advantages).mean()\n",
                "\n",
                "# Main Training Loop\n",
                "def train_ppo(env_name='CartPole-v1', epochs=500, gamma=0.99):\n",
                "    env = gym.make(env_name)\n",
                "    obs_dim = env.observation_space.shape[0]\n",
                "    act_dim = env.action_space.n\n",
                "\n",
                "    actor = Actor(obs_dim, act_dim)\n",
                "    critic = Critic(obs_dim)\n",
                "    opt_actor = optim.Adam(actor.parameters(), lr=3e-4)\n",
                "    opt_critic = optim.Adam(critic.parameters(), lr=1e-3)\n",
                "\n",
                "    for epoch in range(epochs):\n",
                "        state = env.reset()\n",
                "        log_probs, values, rewards, actions, states = [], [], [], [], []\n",
                "        \n",
                "        # Rollout\n",
                "        done = False\n",
                "        while not done:\n",
                "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
                "            dist = torch.distributions.Categorical(actor(state_tensor))\n",
                "            action = dist.sample()\n",
                "            log_prob = dist.log_prob(action)\n",
                "            value = critic(state_tensor)\n",
                "\n",
                "            next_state, reward, done, _ = env.step(action.item())\n",
                "            \n",
                "            log_probs.append(log_prob)\n",
                "            values.append(value)\n",
                "            rewards.append(reward)\n",
                "            actions.append(action)\n",
                "            states.append(state_tensor)\n",
                "\n",
                "            state = next_state\n",
                "\n",
                "        # Process rewards and values\n",
                "        returns = []\n",
                "        G = 0\n",
                "        for r in reversed(rewards):\n",
                "            G = r + gamma * G\n",
                "            returns.insert(0, G)\n",
                "        returns = torch.tensor(returns, dtype=torch.float32)\n",
                "        values = torch.stack(values)\n",
                "        log_probs_old = torch.stack(log_probs)\n",
                "        advantages = compute_advantages(returns, values)\n",
                "        actions = torch.stack(actions)\n",
                "        states = torch.stack(states)\n",
                "\n",
                "        # Update\n",
                "        for _ in range(5):  # epochs\n",
                "            probs = actor(states)\n",
                "            dist = torch.distributions.Categorical(probs)\n",
                "            new_log_probs = dist.log_prob(actions)\n",
                "            a_loss = ppo_loss(new_log_probs, log_probs_old, advantages)\n",
                "            c_loss = nn.functional.mse_loss(critic(states), returns)\n",
                "\n",
                "            opt_actor.zero_grad()\n",
                "            a_loss.backward()\n",
                "            opt_actor.step()\n",
                "\n",
                "            opt_critic.zero_grad()\n",
                "            c_loss.backward()\n",
                "            opt_critic.step()\n",
                "\n",
                "        if epoch % 10 == 0:\n",
                "            print(f\"Epoch {epoch} | Return: {sum(rewards)} | Actor Loss: {a_loss.item():.3f}\")\n",
                "\n",
                "    env.close()\n",
                "\n",
                "# Run training\n",
                "train_ppo()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "üéØ That‚Äôs your **full PPO engine, clean and PEP8-ready**, ready to plug into any Gym environment.  \n",
                "Want the continuous version (for `Box` spaces like Mujoco)? Or wrap up with real-world RL examples next?"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
