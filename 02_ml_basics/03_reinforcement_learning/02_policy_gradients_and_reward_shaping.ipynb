{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here's your full **UTHU-structured summary** of **‚ÄúIntroduction to Policy Gradients‚Äù**, engineered for your cyborg-savant academy of the future:\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Introduction to Policy Gradients**  \n",
                "üéØ *Learning to act by directly optimizing behavior*  \n",
                "(UTHU format, Policy Gradient Core #1)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "Traditional value-based methods like Q-learning learn **what to do** by **estimating values** and acting greedily.  \n",
                "But what if we could **skip the value estimation** and directly learn the **best behavior**?\n",
                "\n",
                "That‚Äôs what **Policy Gradient (PG)** methods do:  \n",
                "They **parametrize the policy itself** (e.g., a neural network), and then **optimize it directly** using gradient ascent.\n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine teaching a dog tricks.  \n",
                "> Value-based RL: track how good each trick is (value learning).  \n",
                "> Policy Gradients: directly tweak the dog‚Äôs instincts to increase reward over time.\n",
                "\n",
                "‚úÖ Especially useful when:\n",
                "- The **action space is continuous**\n",
                "- We want **stochastic policies**\n",
                "- We need **flexible, neural policy approximators**\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman Explanation |\n",
                "|------|---------------------|\n",
                "| **Policy \\( \\pi_\\theta(a | s) \\)** | A function that tells the agent what to do in each state, defined by parameters \\( \\theta \\) |\n",
                "| **Stochastic Policy** | Doesn‚Äôt always choose the same action ‚Äî adds randomness (important for exploration) |\n",
                "| **Policy Gradient** | The direction in which to change \\( \\theta \\) to improve performance |\n",
                "| **REINFORCE Algorithm** | A basic policy gradient method using the log-likelihood trick |\n",
                "| **Trajectory** | A full episode: a sequence of states, actions, and rewards |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- **Robotics**: Smooth, continuous control (e.g., joint angles)  \n",
                "- **Finance**: Direct action sampling in stochastic environments  \n",
                "- **NLP**: RL-based training for text generation (e.g., summarization rewards)  \n",
                "- **Games**: When value-based methods are unstable due to partial observability\n",
                "\n",
                "```plaintext\n",
                "       +---------+        +-----------------+\n",
                "State ‚Üí| Policy  |‚Üí Action| Environment     |‚Üí Reward, Next State\n",
                "       +---------+        +-----------------+\n",
                "              ‚Üë\n",
                "       Update parameters Œ∏ via gradient ascent\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Core Policy Gradient Equation\n",
                "\n",
                "We aim to **maximize the expected return**:\n",
                "$$\n",
                "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} [R(\\tau)]\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( \\theta \\): policy parameters\n",
                "- \\( \\tau \\): trajectory\n",
                "- \\( R(\\tau) \\): total reward from trajectory\n",
                "\n",
                "Using the **log-likelihood trick**:\n",
                "$$\n",
                "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot R_t \\right]\n",
                "$$\n",
                "\n",
                "This is what REINFORCE uses.\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "- Each trajectory gives us feedback on how good our policy is  \n",
                "- If an action led to **high reward**, increase the probability of that action  \n",
                "- This is like **hill-climbing** in policy space using sampled data\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                | Pitfalls                              |\n",
                "|---------------------------|----------------------------------------|\n",
                "| Smooth reward functions   | High variance makes learning unstable |\n",
                "| Enough exploration        | Poor exploration = bad gradients      |\n",
                "| Differentiable policy     | Can‚Äôt be used with hard-coded rules   |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Strengths                              | Weaknesses                                |\n",
                "|----------------------------------------|--------------------------------------------|\n",
                "| Works in continuous action spaces      | High variance in gradient estimates        |\n",
                "| No need to learn value functions       | Slower convergence than Q-learning         |\n",
                "| Naturally handles stochastic policies  | Requires many samples (sample inefficient) |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- In real-world applications, **direct policy learning can overfit to short-term rewards**  \n",
                "- If reward shaping is poor, PG can **optimize undesired behaviors**  \n",
                "- Always align reward design with long-term safety and goals\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **Trust Region PG / PPO**: Solves variance and stability issues  \n",
                "- **Baseline techniques**: Subtracting a baseline (like a value function) to reduce variance  \n",
                "- **Entropy regularization**: Encourages exploration in PG methods  \n",
                "- **Meta-gradients**: Learn how to optimize policies better over time\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: Why does the policy gradient method work even if we don‚Äôt know the dynamics of the environment?**\n",
                "\n",
                "A. It assumes perfect knowledge of the environment  \n",
                "B. It uses an oracle to get the next state  \n",
                "C. It uses sampled trajectories to estimate the gradient  \n",
                "D. It only works in supervised settings\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: PG methods estimate gradients **from data**, not from environment models.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Debug Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: Missing log-probability in policy gradient\n",
                "loss = -reward * action\n",
                "```\n",
                "\n",
                "**Fix:**\n",
                "\n",
                "```python\n",
                "loss = -log_prob * reward  # Use log-likelihood trick\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Policy** | A function that chooses actions based on state |\n",
                "| **Trajectory** | A sequence of (state, action, reward) tuples |\n",
                "| **REINFORCE** | Monte Carlo policy gradient algorithm |\n",
                "| **Gradient Ascent** | Move parameters to increase expected reward |\n",
                "| **Stochastic Policy** | Policy that samples actions probabilistically |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Hyperparameters\n",
                "\n",
                "| Param      | Purpose                     | Typical Values     |\n",
                "|------------|-----------------------------|--------------------|\n",
                "| Learning rate | How fast to update policy | 1e-3 to 1e-4        |\n",
                "| Discount \\( \\gamma \\) | Weight for future rewards    | 0.95 ‚Äì 0.99         |\n",
                "| Episode length | How long to simulate each trajectory | 100 ‚Äì 1000 steps     |\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Average reward per episode**  \n",
                "- **Policy entropy** (diversity of actions)  \n",
                "- **Gradient magnitude** (should decrease over time)\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Normalize rewards to reduce variance  \n",
                "- Use baselines (e.g., critic or moving average) to stabilize training  \n",
                "- Combine with **Advantage Estimation** or PPO for improved results\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import gym\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "\n",
                "env = gym.make(\"CartPole-v1\")\n",
                "obs_dim = env.observation_space.shape[0]\n",
                "act_dim = env.action_space.n\n",
                "\n",
                "class PolicyNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, act_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "policy = PolicyNet()\n",
                "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
                "\n",
                "for episode in range(1000):\n",
                "    obs = env.reset()\n",
                "    log_probs = []\n",
                "    rewards = []\n",
                "    done = False\n",
                "\n",
                "    while not done:\n",
                "        obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
                "        probs = policy(obs_tensor)\n",
                "        dist = torch.distributions.Categorical(probs)\n",
                "        action = dist.sample()\n",
                "        log_probs.append(dist.log_prob(action))\n",
                "\n",
                "        obs, reward, done, _, _ = env.step(action.item())\n",
                "        rewards.append(reward)\n",
                "\n",
                "    total_reward = sum(rewards)\n",
                "    returns = torch.tensor([total_reward] * len(log_probs), dtype=torch.float32)\n",
                "    loss = -torch.stack(log_probs).sum() * total_reward\n",
                "\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "    if episode % 100 == 0:\n",
                "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ That‚Äôs your first step into the world of **Policy Optimization** ‚Äî next up:  \n",
                "üîÅ Want to go deeper into the **REINFORCE Algorithm** next? Or explore **reward shaping** techniques to boost learning?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let‚Äôs now deep-dive into the **REINFORCE algorithm** ‚Äî the OG of policy gradients.  \n",
                "It‚Äôs simple, elegant, and the gateway to advanced policy optimization.\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **REINFORCE Algorithm**  \n",
                "üîÅ *Learning from full episodes with the log-likelihood trick*  \n",
                "(UTHU-style structured breakdown ‚Äî Policy Gradient #2)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "REINFORCE is the **first and most basic policy gradient algorithm**.\n",
                "\n",
                "It works by:\n",
                "1. **Running full episodes**\n",
                "2. **Recording rewards and actions**\n",
                "3. **Updating the policy** to make rewarding actions **more likely**\n",
                "\n",
                "Unlike Q-learning (which uses value estimates), REINFORCE **directly nudges the policy** in directions that led to success.\n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine you‚Äôre coaching a kid to play basketball. You don‚Äôt teach exact values of every move ‚Äî  \n",
                "> you just say: ‚ÄúWhatever you did that time, do more of *that*!‚Äù  \n",
                "> That‚Äôs REINFORCE ‚Äî rewarding actions by increasing their probability.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman-style Explanation |\n",
                "|------|---------------------------|\n",
                "| **Monte Carlo Estimate** | Uses the total return from an episode ‚Äî no bootstrapping |\n",
                "| **Log-Likelihood Trick** | Turn probability updates into differentiable math |\n",
                "| **Stochastic Gradient Ascent** | Move the policy parameters to increase expected return |\n",
                "| **Return \\( G_t \\)** | Total discounted reward from time \\( t \\) onward |\n",
                "| **Trajectory** | A full sequence of (state, action, reward) from start to end |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- **Text generation with reward** (e.g., BLEU score, sentiment)  \n",
                "- **Environments with sparse but delayed rewards**  \n",
                "- **Problems where value estimation is hard or unstable**\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê The REINFORCE Update Rule\n",
                "\n",
                "We want to **maximize expected return** \\( J(\\theta) \\):\n",
                "$$\n",
                "J(\\theta) = \\mathbb{E}_\\pi [R(\\tau)]\n",
                "$$\n",
                "\n",
                "Use the **log-likelihood trick**:\n",
                "$$\n",
                "\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot G_t \\right]\n",
                "$$\n",
                "\n",
                "In practice:\n",
                "- Sample trajectories using \\( \\pi_\\theta \\)\n",
                "- For each timestep:\n",
                "  $$\n",
                "  \\theta \\leftarrow \\theta + \\alpha \\cdot \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot G_t\n",
                "  $$\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "- **Good reward?** Increase the chance of doing that again  \n",
                "- **Bad reward?** Decrease the probability  \n",
                "- Multiply reward by how ‚Äúsurprised‚Äù the model was (log prob) to find direction to update\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                   | Pitfalls                                 |\n",
                "|------------------------------|------------------------------------------|\n",
                "| Full episode completion      | Can't update until end of episode        |\n",
                "| High reward signals available| Sparse rewards ‚Üí noisy gradients         |\n",
                "| Differentiable policies      | Can‚Äôt use hard-coded actions             |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Strengths                             | Weaknesses                               |\n",
                "|--------------------------------------|------------------------------------------|\n",
                "| Very simple to implement             | High variance gradients                  |\n",
                "| No need for a value function         | Slow convergence, sample inefficient     |\n",
                "| Natural fit for stochastic policies  | Can‚Äôt use partial feedback (only full episode) |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- Be cautious in real-time systems ‚Äî REINFORCE **doesn't learn mid-episode**, which could delay correction of dangerous policies  \n",
                "- Always monitor reward definitions ‚Äî wrong incentives = wrong behavior\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **Baseline Subtraction**: Add a value baseline to reduce variance  \n",
                "- **Actor-Critic**: Combine REINFORCE with value function learning  \n",
                "- **Advantage Actor-Critic (A2C)**: Estimate how much better action is than average  \n",
                "- **Generalized Advantage Estimation (GAE)**: Improves REINFORCE‚Äôs stability\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: Why does REINFORCE use the full return \\( G_t \\) instead of a single reward?**\n",
                "\n",
                "A. Because RL rewards are sparse  \n",
                "B. Because Monte Carlo estimates are unbiased  \n",
                "C. Because bootstrapping isn‚Äôt allowed  \n",
                "D. Because Q-learning already uses value updates\n",
                "\n",
                "‚úÖ **Correct Answer: B**  \n",
                "**Explanation**: The full return provides an **unbiased estimate** of the expected return ‚Äî even though it may be high variance.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Fix Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: Using immediate reward instead of return\n",
                "loss = -log_prob * reward\n",
                "```\n",
                "\n",
                "**Fix:**\n",
                "\n",
                "```python\n",
                "loss = -log_prob * Gt  # Gt = sum of discounted future rewards\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **REINFORCE** | A Monte Carlo policy gradient algorithm |\n",
                "| **Return \\( G_t \\)** | Total discounted future reward |\n",
                "| **Log-likelihood trick** | A way to compute gradients for probabilities |\n",
                "| **Trajectory** | A full episode: sequence of (s, a, r) |\n",
                "| **Stochastic Gradient Ascent** | Learning by increasing expected reward |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Hyperparameters\n",
                "\n",
                "| Param | Description | Typical Values |\n",
                "|-------|-------------|----------------|\n",
                "| \\( \\alpha \\) | Learning rate | 1e-2 to 1e-4 |\n",
                "| \\( \\gamma \\) | Discount factor | 0.95 ‚Äì 0.99 |\n",
                "| Episode length | How many steps per episode | 100‚Äì1000 |\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Cumulative reward** per episode  \n",
                "- **Gradient variance**  \n",
                "- **Entropy of policy** (exploration tracking)\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Normalize returns to improve learning speed  \n",
                "- Use **reward baseline** to reduce variance  \n",
                "- Don‚Äôt use REINFORCE in very long episodes ‚Äî high variance\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import gym\n",
                "\n",
                "env = gym.make(\"CartPole-v1\")\n",
                "obs_dim = env.observation_space.shape[0]\n",
                "act_dim = env.action_space.n\n",
                "\n",
                "class PolicyNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, act_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "policy = PolicyNet()\n",
                "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
                "\n",
                "for episode in range(1000):\n",
                "    state = env.reset()\n",
                "    log_probs = []\n",
                "    rewards = []\n",
                "    done = False\n",
                "\n",
                "    while not done:\n",
                "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
                "        probs = policy(state_tensor)\n",
                "        dist = torch.distributions.Categorical(probs)\n",
                "        action = dist.sample()\n",
                "\n",
                "        log_probs.append(dist.log_prob(action))\n",
                "\n",
                "        state, reward, done, _, _ = env.step(action.item())\n",
                "        rewards.append(reward)\n",
                "\n",
                "    # Calculate return\n",
                "    G = sum(rewards)\n",
                "    loss = -torch.stack(log_probs).sum() * G\n",
                "\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "    if episode % 100 == 0:\n",
                "        print(f\"Episode {episode}: Total reward = {G}\")\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You've now fully unlocked **REINFORCE** ‚Äî the foundation of policy gradient methods.  \n",
                "\n",
                "Next up: Want to move into üß™ **Reward Shaping** or continue with üß† **Actor vs Critic** comparisons?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's close out this foundational block of policy gradients with one of its most **defining features**:  \n",
                "üé≤ **Stochastic Policies and Gradients** ‚Äî the reason policy gradients are so **flexible**, **powerful**, and **exploratory**.\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Stochastic Policies and Gradients**  \n",
                "üß¨ *Learning to behave probabilistically ‚Äî not deterministically*  \n",
                "(UTHU-structured summary ‚Äî Policy Gradients #3)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "In value-based methods (like Q-learning), the agent typically learns a **deterministic policy**:  \n",
                "‚ÄúIf I‚Äôm in state S, always take action A.‚Äù\n",
                "\n",
                "But in many real-world settings:\n",
                "- There's **uncertainty**\n",
                "- We need **diversity**\n",
                "- And sometimes, being **unpredictable is optimal**\n",
                "\n",
                "**Stochastic policies** solve this by assigning **probabilities to actions**.  \n",
                "Instead of picking a single best move, the policy **samples from a distribution**.\n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine teaching a poker-playing agent. If it always bets the same way in the same scenario, it becomes predictable ‚Äî and loses.  \n",
                "> Instead, it should play with **controlled randomness** ‚Äî that‚Äôs what a **stochastic policy** enables.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman-style Explanation |\n",
                "|------|---------------------------|\n",
                "| **Stochastic Policy** | A function that returns a **probability distribution** over actions, not just one |\n",
                "| **Sampling** | Choosing actions based on probabilities, not max values |\n",
                "| **Log-Likelihood Gradient** | Computes the gradient of action probabilities with respect to model parameters |\n",
                "| **Entropy** | A measure of randomness in the policy ‚Äî high entropy = more exploration |\n",
                "| **Exploration** | Trying less certain actions to learn their value |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- **Robotics**: Adding noise to avoid local minima  \n",
                "- **Games like Poker**: Where unpredictability is strategic  \n",
                "- **NLP & Text Gen**: To avoid repeating the same word/sentence  \n",
                "- **Multi-Agent RL**: Avoiding being exploited by other agents\n",
                "\n",
                "```plaintext\n",
                "State s ‚Üí Policy œÄŒ∏ ‚Üí [Action1: 0.1, Action2: 0.6, Action3: 0.3]\n",
                "Sample from this distribution ‚Üí Take Action2\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Policy as a Distribution\n",
                "\n",
                "Let \\( \\pi_\\theta(a | s) \\) represent a **probability** of taking action \\( a \\) in state \\( s \\), parameterized by \\( \\theta \\).\n",
                "\n",
                "We want to **maximize the expected return**:\n",
                "$$\n",
                "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} [R(\\tau)]\n",
                "$$\n",
                "\n",
                "Using **log-likelihood trick**:\n",
                "$$\n",
                "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a | s) \\cdot R \\right]\n",
                "$$\n",
                "\n",
                "Because we **sample** actions, we can‚Äôt differentiate through sampling directly.  \n",
                "Instead, we differentiate **through the probability of the action** ‚Äî this is the core of PG methods.\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "- The more **reward** an action gets, the more we **increase its log-probability**  \n",
                "- Since we're sampling, we **don‚Äôt need a model** of the environment  \n",
                "- Gradients tell us **how to shift the policy‚Äôs shape** to increase good action probabilities\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                        | Pitfalls                             |\n",
                "|-----------------------------------|--------------------------------------|\n",
                "| Sampling is cheap and fast        | Can be slow or unstable with large action spaces |\n",
                "| Reward signal is available        | Sparse reward ‚Üí poor gradient estimates |\n",
                "| Enough trajectories are collected | Low sample count ‚Üí noisy gradients   |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Pros                                        | Cons                                      |\n",
                "|--------------------------------------------|-------------------------------------------|\n",
                "| Handles continuous and complex actions      | Gradient estimates can have **high variance** |\n",
                "| Enables exploration without extra strategies| Slower convergence than deterministic policies |\n",
                "| Works well with function approximation      | Hard to debug probabilistic behaviors     |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- Stochasticity in policy behavior is powerful ‚Äî but dangerous if **left unbounded** in real-world environments (robotics, finance, healthcare)  \n",
                "- Add **entropy regularization** to control randomness  \n",
                "- Always test stochastic behaviors in **safe sandboxes first**\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **Entropy bonus**: Encourages exploration by penalizing low-entropy (deterministic) policies  \n",
                "- **Gaussian Policies**: For continuous actions, use normal distributions for sampling  \n",
                "- **Reparameterization trick**: Used in advanced models to sample actions in a differentiable way (e.g., SAC)\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: What is a key reason to use stochastic policies in RL?**\n",
                "\n",
                "A. They're easier to debug  \n",
                "B. They avoid the need for a neural network  \n",
                "C. They allow the agent to explore and avoid predictability  \n",
                "D. They require fewer episodes to converge\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: Stochastic policies add randomness, helping agents explore and avoid exploitation in dynamic environments.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Fix Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: Always takes argmax (deterministic)\n",
                "action = torch.argmax(policy(obs))\n",
                "```\n",
                "\n",
                "**Fix (stochastic sampling):**\n",
                "\n",
                "```python\n",
                "probs = policy(obs)\n",
                "dist = torch.distributions.Categorical(probs)\n",
                "action = dist.sample()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Stochastic Policy** | A policy that outputs action probabilities instead of fixed actions |\n",
                "| **Sampling** | Drawing an action from a probability distribution |\n",
                "| **Log-likelihood gradient** | Gradient used in REINFORCE to optimize stochastic policies |\n",
                "| **Entropy** | A measure of uncertainty in the action distribution |\n",
                "| **Policy Gradient** | Gradient that updates the policy to increase reward |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Hyperparameters\n",
                "\n",
                "| Param          | Use Case                     | Notes                        |\n",
                "|----------------|------------------------------|------------------------------|\n",
                "| Entropy bonus  | Encourages diverse actions   | Useful in PPO, A2C           |\n",
                "| Action std     | In Gaussian policies         | Controls randomness in continuous space |\n",
                "| Batch size     | Affects gradient stability   | Larger = smoother updates    |\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Policy entropy** over time (should drop slowly)  \n",
                "- **Action distribution heatmaps**  \n",
                "- **Exploration coverage** ‚Äî percent of actions sampled\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Use **entropy regularization** to balance exploration  \n",
                "- Track **policy variance** (should reduce as agent learns)  \n",
                "- For continuous actions, **clip standard deviation** of Gaussian outputs to prevent instability\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import gym\n",
                "\n",
                "env = gym.make(\"CartPole-v1\")\n",
                "obs_dim = env.observation_space.shape[0]\n",
                "act_dim = env.action_space.n\n",
                "\n",
                "class StochasticPolicy(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, act_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "policy = StochasticPolicy()\n",
                "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
                "\n",
                "for episode in range(1000):\n",
                "    obs = env.reset()\n",
                "    done = False\n",
                "    log_probs = []\n",
                "    rewards = []\n",
                "\n",
                "    while not done:\n",
                "        obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
                "        probs = policy(obs_tensor)\n",
                "        dist = torch.distributions.Categorical(probs)\n",
                "        action = dist.sample()\n",
                "\n",
                "        log_probs.append(dist.log_prob(action))\n",
                "\n",
                "        obs, reward, done, _, _ = env.step(action.item())\n",
                "        rewards.append(reward)\n",
                "\n",
                "    G = sum(rewards)\n",
                "    loss = -torch.stack(log_probs).sum() * G\n",
                "\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "    if episode % 100 == 0:\n",
                "        print(f\"Episode {episode}: Reward = {G}\")\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You‚Äôve now unlocked the **core advantage of policy gradients** ‚Äî the ability to **act with controlled randomness**.\n",
                "\n",
                "üöÄ Next up: Want to roll into **Reward Shaping** or dive into **Actor vs Critic** to see how policy and value-based learning combine?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's dig into one of the most **creative yet dangerous powers** in RL:  \n",
                "üéÅ **Reward Shaping** ‚Äî where you define what it *means* to be ‚Äúgood.‚Äù  \n",
                "This is **designing the compass** for your RL agent.\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Reward Shaping in RL ‚Äì Designing Reward Functions**  \n",
                "üß† *What you reward is what you get*  \n",
                "(UTHU-structured summary ‚Äî Policy Gradients #4)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "In Reinforcement Learning, the reward function is the **only signal the agent gets** about what it's supposed to do.  \n",
                "It **replaces labels** from supervised learning. If you reward something ‚Äî the agent **will do more of it**.\n",
                "\n",
                "**Reward shaping** is the process of **modifying or designing** the reward function to:\n",
                "- Guide learning more efficiently\n",
                "- Encourage desired behaviors\n",
                "- Avoid unsafe or undesirable outcomes\n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine teaching a dog tricks using treats.  \n",
                "> What you treat, it repeats. But if you give the treat at the wrong moment, you might accidentally reward *jumping on guests instead of sitting nicely*.\n",
                "\n",
                "> Same for agents: **if you shape rewards poorly, they'll game the system**.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman-Style Explanation |\n",
                "|------|---------------------------|\n",
                "| **Reward Function** | A rule that assigns a number (reward) to agent actions/states |\n",
                "| **Sparse Reward** | Rewards happen rarely (e.g., only at goal) |\n",
                "| **Dense Reward** | Rewards are given frequently (e.g., for every step) |\n",
                "| **Shaping** | Adding extra rewards to guide learning |\n",
                "| **Reward Hacking** | Agent exploits loopholes in your reward design |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- **Robotics**: Add rewards for approaching target, not just reaching it  \n",
                "- **Games**: Reward not just wins, but intermediate progress  \n",
                "- **Autonomous driving**: Penalize collisions *and* reward lane-following  \n",
                "- **Healthcare**: Reward safe treatments, not just fast outcomes\n",
                "\n",
                "```plaintext\n",
                "Goal-based reward: +1 if success, 0 otherwise ‚Üí SLOW learning\n",
                "\n",
                "Shaped reward: +0.1 for moving closer to goal ‚Üí FASTER learning\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Standard Return Definition\n",
                "\n",
                "Total return:\n",
                "$$\n",
                "G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}\n",
                "$$\n",
                "\n",
                "In reward shaping, we define a new reward:\n",
                "$$\n",
                "r'(s, a, s') = r(s, a, s') + F(s, s')\n",
                "$$\n",
                "\n",
                "Where \\( F \\) is a **potential-based shaping function**, often:\n",
                "$$\n",
                "F(s, s') = \\gamma \\Phi(s') - \\Phi(s)\n",
                "$$\n",
                "\n",
                "This preserves **optimal policies** (Ng et al., 1999), while accelerating learning.\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "- Think of \\( \\Phi(s) \\) as a **heuristic \"potential energy\"**  \n",
                "- We add artificial rewards that **don‚Äôt change the final destination**, just the path taken  \n",
                "- It‚Äôs like adding slopes on a hill to help the agent ‚Äúslide‚Äù toward the goal faster\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                          | Pitfalls                                   |\n",
                "|-------------------------------------|--------------------------------------------|\n",
                "| Reward signals reflect true goals   | Poor design = reward hacking               |\n",
                "| Shaped rewards still lead to same policy | Bad shaping = diverging from optimal policy |\n",
                "| Agent can perceive shaping features | Requires state features to compute \\( \\Phi \\) |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Pros                                | Cons                                         |\n",
                "|-------------------------------------|----------------------------------------------|\n",
                "| Speeds up learning significantly    | Risk of **reward hacking** (gaming the system) |\n",
                "| Helps in sparse-reward environments| May bias policy away from true goal          |\n",
                "| Easier debugging and progress tracking | Can make reward too complex to tune         |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- **Shaping = subtle control** ‚Äî can **embed bias** or **unintended incentives**  \n",
                "- Over-rewarding speed might cause unsafe driving in self-driving agents  \n",
                "- Poor shaping has led to AI agents *standing still* or *crashing intentionally* for points\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **Curiosity-driven reward shaping**: Internal motivation to explore unknown states  \n",
                "- **Learned reward models**: Use neural networks to infer rewards from demonstrations  \n",
                "- **Human-in-the-loop shaping**: Reward designed by real-time human feedback  \n",
                "- **Inverse RL**: Recover reward function from expert behavior\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: What‚Äôs a major risk of poorly designed shaped rewards?**\n",
                "\n",
                "A. The agent will stop exploring  \n",
                "B. The agent will become deterministic  \n",
                "C. The agent may exploit the reward function in unintended ways  \n",
                "D. The policy will become non-differentiable\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: This is called **reward hacking** ‚Äî agents often find loopholes that humans didn‚Äôt intend.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Debug Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: Only gives reward at goal\n",
                "reward = 1 if done else 0\n",
                "```\n",
                "\n",
                "**Fix (Shaped Reward):**\n",
                "\n",
                "```python\n",
                "distance_to_goal = np.linalg.norm(state - goal)\n",
                "reward = 1 if done else -0.01 * distance_to_goal\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Reward Function** | A mapping from environment states/actions to a scalar reward |\n",
                "| **Reward Shaping** | Adding extra rewards to guide the learning path |\n",
                "| **Potential-Based Shaping** | A proven safe method of shaping without altering the optimal policy |\n",
                "| **Reward Hacking** | Agent finds loopholes to earn high reward in wrong ways |\n",
                "| **Sparse Reward** | Agent only receives reward at rare events (e.g., goal) |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Heuristics for Better Shaping\n",
                "\n",
                "| Tip                           | Why it helps                          |\n",
                "|-------------------------------|---------------------------------------|\n",
                "| Use potential-based shaping   | Safe: doesn‚Äôt change optimal policy   |\n",
                "| Reward intermediate progress  | Makes credit assignment easier        |\n",
                "| Penalize unsafe actions       | Avoids dangerous behavior             |\n",
                "| Normalize reward scales       | Keeps gradient updates stable         |\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Time to reach goal**  \n",
                "- **Average reward per step**  \n",
                "- **Policy stability**  \n",
                "- **Exploration depth**\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- **Always track side effects** of your reward design  \n",
                "- Use **visualization of agent behavior** alongside reward scores  \n",
                "- Add **unit tests** for reward logic (e.g., wrong action ‚â† reward)\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import gym\n",
                "\n",
                "env = gym.make(\"MountainCar-v0\")\n",
                "state = env.reset()\n",
                "goal_position = 0.5\n",
                "\n",
                "def compute_shaped_reward(state, reward, done):\n",
                "    # Potential function: closer to goal is better\n",
                "    distance_to_goal = goal_position - state[0]\n",
                "    shaping = -0.1 * abs(distance_to_goal)\n",
                "\n",
                "    return reward + shaping\n",
                "\n",
                "for episode in range(3):\n",
                "    state = env.reset()\n",
                "    done = False\n",
                "    total_reward = 0\n",
                "\n",
                "    while not done:\n",
                "        action = env.action_space.sample()\n",
                "        next_state, reward, done, _, _ = env.step(action)\n",
                "\n",
                "        shaped_reward = compute_shaped_reward(state, reward, done)\n",
                "        total_reward += shaped_reward\n",
                "        state = next_state\n",
                "\n",
                "    print(f\"Episode {episode} ‚Äî Shaped Total Reward: {total_reward:.2f}\")\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You now know how to **design smarter reward functions** that lead to faster, safer learning.\n",
                "\n",
                "üë£ Next step: Want to explore **potential-based shaping theory** more deeply, or move into üé≠ **Actor vs. Critic** architecture next?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's now level up your reward design toolkit from *just giving signals* to **engineering intelligence**:  \n",
                "üõ†Ô∏è **Reward Engineering for Efficient Learning** ‚Äî shaping smarter, safer, and more scalable agents.\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Reward Engineering for Efficient Learning**  \n",
                "üéì *Designing reward systems that learn faster, better, and safer*  \n",
                "(UTHU-structured summary ‚Äî Policy Gradients #5)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "In Reinforcement Learning, **reward = guidance**. But it's not just about *having* a reward ‚Äî it's about designing rewards that lead to:\n",
                "- **Faster convergence**\n",
                "- **Safe behaviors**\n",
                "- **Generalizable policies**\n",
                "\n",
                "**Reward engineering** is about thinking like a system architect:\n",
                "- How do you **design signals** that align with your true objectives?\n",
                "- How do you **prevent hacking**, **balance tradeoffs**, and **encourage exploration**?\n",
                "\n",
                "> **Analogy**:  \n",
                "> Think of training a dog, a robot, or even a self-driving car.  \n",
                "> Saying ‚Äúgood job‚Äù isn't enough ‚Äî you need to say it at the right **time**, with the right **scale**, and with the **right emphasis**.\n",
                "\n",
                "> In RL, that‚Äôs reward engineering: designing the *language* your agent learns from.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman-Style Explanation |\n",
                "|------|---------------------------|\n",
                "| **Reward Engineering** | The process of designing reward functions with intention and precision |\n",
                "| **Signal-to-Noise Ratio** | Ratio of meaningful reward to irrelevant variation ‚Äî higher is better |\n",
                "| **Trade-off Reward** | Balances multiple goals (e.g., speed vs. safety) |\n",
                "| **Shaping Heuristics** | Manually added signals that encourage good behavior |\n",
                "| **Auxiliary Rewards** | Bonus signals that help exploration or representation learning |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- **Autonomous Cars**: Trade off speed, comfort, and safety  \n",
                "- **Robotics**: Balance between power efficiency and goal achievement  \n",
                "- **Healthcare Agents**: Reward long-term health improvements over short-term gains  \n",
                "- **Game AI**: Design multi-stage objectives that evolve as learning progresses\n",
                "\n",
                "```plaintext\n",
                "Bad: r = 1 if win, 0 else\n",
                "Better: r = 0.1 per enemy defeated, +1 if win, -0.5 if agent dies\n",
                "Best: Add bonus for reaching objectives early, with minimal damage\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Reward Composition\n",
                "\n",
                "Let‚Äôs say total reward is a weighted combination of factors:\n",
                "\n",
                "$$\n",
                "r_t = w_1 \\cdot r_{\\text{goal}} + w_2 \\cdot r_{\\text{safety}} + w_3 \\cdot r_{\\text{efficiency}}\n",
                "$$\n",
                "\n",
                "This forms a **reward vector space**, where each dimension controls a behavior.\n",
                "\n",
                "### üß† Signal-to-Noise Ratio (SNR)\n",
                "\n",
                "You want:\n",
                "- **Meaningful rewards** to be large\n",
                "- **Random noise or penalties** to be small\n",
                "\n",
                "Otherwise, learning slows or gets misdirected.\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "The gradient of the policy is scaled by the reward:\n",
                "$$\n",
                "\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot R \\right]\n",
                "$$\n",
                "\n",
                "So:  \n",
                "üü¢ **Clear, consistent rewards** ‚Üí Strong signal, fast convergence  \n",
                "üî¥ **Sparse or noisy rewards** ‚Üí Weak signal, slow/no learning\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                         | Pitfalls                                   |\n",
                "|------------------------------------|--------------------------------------------|\n",
                "| Rewards reflect true goals         | Misaligned rewards lead to hacking         |\n",
                "| All reward terms are scaled correctly | Imbalance ‚Üí one behavior dominates         |\n",
                "| No feedback delay too long         | Delayed rewards ‚Üí harder credit assignment |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Technique                 | Pro                                  | Con                                       |\n",
                "|---------------------------|---------------------------------------|-------------------------------------------|\n",
                "| **Dense rewards**         | Fast learning                        | Can overfit to shortcuts                  |\n",
                "| **Sparse + Shaped**       | Preserves goal + guidance            | Harder to tune                            |\n",
                "| **Multi-term engineering**| Balance real-world tradeoffs         | Must normalize and weight carefully       |\n",
                "| **Learned reward models** | Adaptable, scalable                  | Risk of drift from true goal              |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- Reward functions can **bake in human biases** (gender, race, etc.)  \n",
                "- In multi-agent or safety-critical domains, **bad reward = disaster**  \n",
                "- Design for **fairness**, **robustness**, and **interpretability**\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **Learned reward functions** from human feedback (e.g., InstructGPT)  \n",
                "- **Inverse RL** to recover reward functions from expert demos  \n",
                "- **Safe RL reward engineering**: Guarantee constraint satisfaction during learning  \n",
                "- **Multi-objective RL**: Dynamically balancing conflicting rewards (Pareto-optimal)\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: What is one main goal of reward engineering?**\n",
                "\n",
                "A. Make the reward as sparse as possible  \n",
                "B. Remove all exploration behavior  \n",
                "C. Align learning signals with the true objective  \n",
                "D. Increase the number of actions\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: A well-engineered reward directly guides the agent toward what you actually want it to do.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Fix Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: Only includes reward for reaching goal\n",
                "reward = 1 if goal_reached else 0\n",
                "```\n",
                "\n",
                "**Fix (Multi-factor reward):**\n",
                "\n",
                "```python\n",
                "reward = 1 if goal_reached else 0\n",
                "reward -= 0.05 * steps_taken\n",
                "reward += 0.2 * progress_toward_goal\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Reward Engineering** | Designing reward functions to guide agent behavior effectively |\n",
                "| **Signal-to-Noise Ratio** | Quality of the learning signal from reward |\n",
                "| **Shaping Heuristics** | Hand-crafted rewards that guide behavior |\n",
                "| **Auxiliary Reward** | Side signals added to help learning, not direct goal |\n",
                "| **Multi-objective Reward** | Reward composed of weighted sub-goals |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Heuristics for Efficient Learning\n",
                "\n",
                "| Strategy                    | Why it works                          |\n",
                "|-----------------------------|---------------------------------------|\n",
                "| Normalize all reward components | Avoid dominance by one term         |\n",
                "| Penalize unsafe actions     | Helps prune bad trajectories early    |\n",
                "| Reward progress, not just goal | Faster learning and better gradients |\n",
                "| Use reward visualization    | Debug and explain agent decisions     |\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Reward decomposition plots** (which part agent optimizes most)  \n",
                "- **Learning speed vs. reward shaping applied**  \n",
                "- **Exploration behavior diversity**\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Use **unit tests** for reward logic  \n",
                "- Monitor for **reward loops or exploits**  \n",
                "- Tune reward **weights with grid search or Bayesian methods**\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import gym\n",
                "\n",
                "env = gym.make(\"MountainCar-v0\")\n",
                "\n",
                "goal_position = 0.5\n",
                "\n",
                "def compute_engineered_reward(state, reward, done, steps_taken):\n",
                "    # Reward closer to goal, penalize energy (step count)\n",
                "    position = state[0]\n",
                "    progress_bonus = max(position - (-0.6), 0.0) * 10\n",
                "    step_penalty = -0.1 * steps_taken\n",
                "    goal_reward = 100 if done and position >= goal_position else 0\n",
                "\n",
                "    return progress_bonus + step_penalty + goal_reward\n",
                "\n",
                "for episode in range(3):\n",
                "    state = env.reset()\n",
                "    total_reward = 0\n",
                "    done = False\n",
                "    steps = 0\n",
                "\n",
                "    while not done:\n",
                "        action = env.action_space.sample()\n",
                "        next_state, reward, done, _, _ = env.step(action)\n",
                "        shaped_reward = compute_engineered_reward(next_state, reward, done, steps)\n",
                "        total_reward += shaped_reward\n",
                "        state = next_state\n",
                "        steps += 1\n",
                "\n",
                "    print(f\"Episode {episode} ‚Äî Engineered Reward: {total_reward:.2f}\")\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ With reward engineering, you're no longer just training agents ‚Äî you're **designing learning systems**.\n",
                "\n",
                "Next stop? Want to move into **potential-based reward shaping** or advance to üß† **Actor vs. Critic Methods** next?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let‚Äôs go deep into one of the **safest, mathematically sound** ways to shape rewards without wrecking your agent‚Äôs learning:  \n",
                "üß≤ **Potential-Based Reward Shaping (PBRS)** ‚Äî a technique that adds guidance without changing the destination.\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Potential-Based Reward Shaping**  \n",
                "üß† *Guide the agent without changing the optimal policy*  \n",
                "(UTHU-structured summary ‚Äî Policy Gradients #6)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "Most reward shaping is risky:  \n",
                "üí£ Add the wrong bonus ‚Üí agent starts optimizing *the wrong thing* (aka **reward hacking**).\n",
                "\n",
                "But **Potential-Based Reward Shaping (PBRS)** lets you add extra rewards **while provably keeping the same optimal policy**.  \n",
                "It acts like a **force field**: it nudges the agent gently in the right direction, without pulling it off-course.\n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine you're hiking toward a mountain peak.  \n",
                "> PBRS is like placing **gentle signposts** that **help you pick better paths** ‚Äî but don‚Äôt teleport you or build new mountains.  \n",
                "> You still end up at the original goal, just **faster and smarter**.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman-Style Explanation |\n",
                "|------|---------------------------|\n",
                "| **Potential Function \\( \\Phi(s) \\)** | A scalar score that tells how ‚Äúgood‚Äù a state is |\n",
                "| **PBRS Reward** | Modified reward that adds the difference in potential between states |\n",
                "| **Shaped Reward** | \\( r'(s, a, s') = r(s, a, s') + F(s, s') \\) |\n",
                "| **Preservation of Policy** | Guarantees the same optimal behavior even with shaping |\n",
                "| **Shaping Term** | \\( F(s, s') = \\gamma \\Phi(s') - \\Phi(s) \\) |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- **Sparse-reward tasks**: Give agents hints about useful directions  \n",
                "- **Robotics**: Encode expert intuition without demonstration  \n",
                "- **Games**: Reward strategic positions before winning  \n",
                "- **Autonomous agents**: Encourage progress without hardcoding policies\n",
                "\n",
                "```plaintext\n",
                "Base reward: +1 at goal, 0 otherwise  \n",
                "‚Üí slow learning\n",
                "\n",
                "Shaped reward:  \n",
                "r'(s, a, s') = base_reward + Œ≥Œ¶(s') - Œ¶(s)  \n",
                "‚Üí preserves optimality, but learns faster\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Core Equation\n",
                "\n",
                "Given a base reward function \\( r(s, a, s') \\), we define:\n",
                "\n",
                "$$\n",
                "r'(s, a, s') = r(s, a, s') + \\gamma \\Phi(s') - \\Phi(s)\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( \\Phi(s) \\): potential function  \n",
                "- \\( \\gamma \\): discount factor  \n",
                "- \\( r' \\): shaped reward\n",
                "\n",
                "üìå **Key Theorem**:  \n",
                "Adding this shaping term does **not change the optimal policy** (Ng et al., 1999)\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "- \\( \\Phi(s) \\) behaves like **potential energy** in physics  \n",
                "- The agent is **rewarded for moving toward better states**, like rolling downhill  \n",
                "- Even if these potentials are imperfect, they **do not change the goal**, just help reach it faster\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                       | Pitfalls                                  |\n",
                "|----------------------------------|-------------------------------------------|\n",
                "| You define a meaningful \\( \\Phi(s) \\) | Bad heuristics ‚Üí slow learning, not incorrect policy |\n",
                "| Agent explores all relevant states | Sparse shaping may not help enough        |\n",
                "| \\( \\Phi(s) \\) is computable       | May require domain knowledge              |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Pros                               | Cons                                          |\n",
                "|------------------------------------|-----------------------------------------------|\n",
                "| Provably preserves optimal policy  | Requires defining a good potential function   |\n",
                "| Helps in sparse reward settings    | Doesn‚Äôt help if potential function is noisy   |\n",
                "| Easy to implement on top of existing rewards | May still increase variance in gradient updates |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- PBRS helps make agents **more sample efficient**, reducing energy and compute costs  \n",
                "- But poor design of \\( \\Phi \\) can **bake in bias** or unwanted priorities (e.g., prefer short over safe paths)\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **Deep PBRS**: Use neural networks to learn \\( \\Phi(s) \\) from data  \n",
                "- **Human-guided shaping**: Learn \\( \\Phi(s) \\) from expert ratings or preferences  \n",
                "- **Goal-conditioned PBRS**: Use distance-to-goal functions as potential  \n",
                "- **PBRS + Curriculum Learning**: Shape rewards differently across training stages\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: Why does PBRS not change the optimal policy?**\n",
                "\n",
                "A. It only applies to supervised learning  \n",
                "B. It adds a constant to the reward  \n",
                "C. The shaping term forms a conservative vector field  \n",
                "D. It forces the agent to follow a fixed plan\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: The shaping term \\( \\gamma \\Phi(s') - \\Phi(s) \\) doesn‚Äôt introduce loops or bias ‚Äî it's like a vector field with no net effect on paths.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Fix Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: Naive shaping adds reward based on absolute state\n",
                "reward += np.linalg.norm(state)\n",
                "```\n",
                "\n",
                "**Fix (PBRS):**\n",
                "\n",
                "```python\n",
                "def potential(state):\n",
                "    return -np.linalg.norm(goal - state)  # closer = higher potential\n",
                "\n",
                "shaped_reward = reward + gamma * potential(next_state) - potential(state)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Potential Function \\( \\Phi \\)** | Score assigned to each state as an indicator of usefulness |\n",
                "| **PBRS** | Reward shaping method that preserves optimality |\n",
                "| **Shaped Reward** | Base reward + difference in potential between states |\n",
                "| **Sparse Reward** | Rewards only given at rare states like goal |\n",
                "| **Policy Invariance** | Guarantee that the best policy stays unchanged |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Strategies to Define \\( \\Phi(s) \\)\n",
                "\n",
                "| Strategy                 | When to Use                       |\n",
                "|--------------------------|-----------------------------------|\n",
                "| Distance to goal         | Navigation, reaching tasks        |\n",
                "| State value estimates    | Use pre-trained critic            |\n",
                "| Learned from expert demos| When demonstrations are available |\n",
                "| Hand-crafted features    | Use domain knowledge if available |\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Time to goal** (with vs. without shaping)  \n",
                "- **Sample efficiency**  \n",
                "- **Policy similarity** (compare to unshaped policy)  \n",
                "- **Gradient variance**\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Keep \\( \\Phi \\) **smooth** to avoid high-variance gradients  \n",
                "- Log both **base and shaped rewards** separately for debugging  \n",
                "- Use **visualizations of potential** to inspect the shaping field\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import gym\n",
                "\n",
                "env = gym.make(\"MountainCar-v0\")\n",
                "gamma = 0.99\n",
                "goal_position = 0.5\n",
                "\n",
                "def potential(state):\n",
                "    # Negative distance to goal as potential\n",
                "    return -abs(goal_position - state[0])\n",
                "\n",
                "for episode in range(3):\n",
                "    state = env.reset()\n",
                "    total_shaped_reward = 0\n",
                "    done = False\n",
                "\n",
                "    while not done:\n",
                "        action = env.action_space.sample()\n",
                "        next_state, reward, done, _, _ = env.step(action)\n",
                "\n",
                "        # Apply PBRS\n",
                "        shaped_reward = reward + gamma * potential(next_state) - potential(state)\n",
                "        total_shaped_reward += shaped_reward\n",
                "        state = next_state\n",
                "\n",
                "    print(f\"Episode {episode}: Total PBRS Reward = {total_shaped_reward:.2f}\")\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You‚Äôve now added a **proven-safe method** to shape your agent‚Äôs world ‚Äî faster learning, no unintended detours.\n",
                "\n",
                "üß† Ready to now compare **Policy Gradients vs Actor-Critic?** Or want to enter the **multi-objective shaping** realm?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let‚Äôs now synthesize what you‚Äôve learned so far with a **head-to-head showdown**:  \n",
                "üß† **Policy Gradients vs. Actor-Critic Methods** ‚Äî where we compare the pure policy world with the hybrid world.\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Actor vs. Critic Methods**  \n",
                "üé≠ *Two minds are better than one: combining policy and value learning*  \n",
                "(UTHU-structured summary ‚Äî Policy Gradients #7)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "So far, we‚Äôve seen two paradigms in Reinforcement Learning:\n",
                "\n",
                "| Type              | Description |\n",
                "|-------------------|-------------|\n",
                "| **Policy Gradient (PG)** | Directly optimize behavior using rewards (REINFORCE) |\n",
                "| **Value-Based Methods** | Learn state-action values, act greedily (Q-learning) |\n",
                "\n",
                "But what if we could **combine the strengths** of both?\n",
                "\n",
                "**Actor-Critic methods do exactly this**:\n",
                "- The **Actor** decides what to do (policy)\n",
                "- The **Critic** evaluates how good that decision was (value function)\n",
                "\n",
                "> **Analogy**:  \n",
                "> Think of the **Actor** as the player in a video game.  \n",
                "> The **Critic** is the in-game coach giving real-time feedback.  \n",
                "> The actor improves using the **advice of the critic**, not just from delayed reward.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman-Style Explanation |\n",
                "|------|---------------------------|\n",
                "| **Actor** | A function (often a neural net) that outputs the agent‚Äôs policy |\n",
                "| **Critic** | A second model that estimates the value function \\( V(s) \\) or \\( Q(s, a) \\) |\n",
                "| **Advantage** | How much better an action was compared to expected outcome |\n",
                "| **TD Error** | The difference between expected and observed return ‚Äî used by the Critic |\n",
                "| **Baseline** | Value function used to reduce gradient variance in PG methods |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- **Continuous control tasks** (robot arms, drones)  \n",
                "- **High-dimensional observation spaces** (image-based environments)  \n",
                "- **Multi-agent environments** where stable feedback is crucial  \n",
                "- **Natural Language RL**: reward = BLEU, ROUGE, etc.\n",
                "\n",
                "```plaintext\n",
                "PG:     Update = reward * ‚àálog œÄ\n",
                "AC:     Update = (reward - V(s)) * ‚àálog œÄ     ‚Üê lower variance\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê PG vs Actor-Critic Equations\n",
                "\n",
                "**REINFORCE** update:\n",
                "$$\n",
                "\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[ \\nabla_\\theta \\log \\pi_\\theta(a | s) \\cdot G_t \\right]\n",
                "$$\n",
                "\n",
                "**Actor-Critic** update:\n",
                "$$\n",
                "\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[ \\nabla_\\theta \\log \\pi_\\theta(a | s) \\cdot A(s, a) \\right]\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( A(s, a) = Q(s, a) - V(s) \\): **Advantage function**\n",
                "- The **Critic** learns to approximate \\( V(s) \\) or \\( Q(s, a) \\)\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "- PG methods suffer from **high variance** because they use raw return \\( G_t \\)  \n",
                "- Critic provides a **baseline** to **center the reward**: was this action better than expected?  \n",
                "- This **stabilizes learning** without changing the final policy goal\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                 | Pitfalls                                 |\n",
                "|----------------------------|------------------------------------------|\n",
                "| Critic is well-trained     | A bad critic can mislead the actor       |\n",
                "| Actor and Critic are updated properly | Instability if one learns faster than the other |\n",
                "| Advantage estimates are accurate | Noisy estimates ‚Üí jittery updates     |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Category            | Policy Gradient (REINFORCE)     | Actor-Critic                      |\n",
                "|---------------------|----------------------------------|-----------------------------------|\n",
                "| **Variance**         | High                            | Lower due to critic               |\n",
                "| **Sample Efficiency**| Low                             | Better                            |\n",
                "| **Stability**        | Sensitive to reward noise       | More stable                       |\n",
                "| **Complexity**       | Simple                          | More parameters and tuning        |\n",
                "| **Bias**             | Unbiased                        | Slight bias from value estimates  |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- In complex systems (e.g. autonomous agents), **bad critics can bias decisions** ‚Äî test them carefully  \n",
                "- AC methods can become **overfitted** to short-term advantages ‚Äî inspect long-term behaviors\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **A2C / A3C**: Synchronous/asynchronous Actor-Critic variants  \n",
                "- **PPO (Proximal Policy Optimization)**: Clipped update Actor-Critic  \n",
                "- **TD3, SAC**: Use twin critics and entropy bonuses  \n",
                "- **Distributional Critics**: Value full return distributions (e.g., QR-DQN + AC)\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: Why is Actor-Critic often more stable than REINFORCE?**\n",
                "\n",
                "A. It uses Q-learning updates  \n",
                "B. It samples fewer episodes  \n",
                "C. It leverages a value function to reduce update variance  \n",
                "D. It doesn‚Äôt use gradients\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: The Critic provides a baseline ‚Äî this reduces how noisy the reward signal is when updating the Actor.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Fix Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: Actor updated without baseline\n",
                "loss = -log_prob * reward\n",
                "```\n",
                "\n",
                "**Fix (Advantage):**\n",
                "\n",
                "```python\n",
                "advantage = reward - value_estimate\n",
                "loss = -log_prob * advantage\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Actor** | Learns the policy \\( \\pi_\\theta \\) |\n",
                "| **Critic** | Learns the value function \\( V(s) \\) |\n",
                "| **Advantage** | Difference between actual and expected reward |\n",
                "| **TD Error** | Value-based measure of how surprising a reward is |\n",
                "| **Baseline** | Value function used to stabilize PG updates |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Actor-Critic Tips\n",
                "\n",
                "| Strategy                  | Reason                                |\n",
                "|---------------------------|----------------------------------------|\n",
                "| Normalize advantage       | Keeps gradient scale consistent       |\n",
                "| Use target networks       | Stabilizes critic updates             |\n",
                "| Clip gradients            | Prevents large, unstable jumps        |\n",
                "| Train actor slower        | Let critic learn first, then follow   |\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Policy entropy** (monitor exploration)  \n",
                "- **TD error over time**  \n",
                "- **Advantage variance**  \n",
                "- **Actor vs critic loss curves**\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Log **critic accuracy** (actual return vs. predicted)  \n",
                "- Use **shared networks** with split heads for actor and critic  \n",
                "- Clip **advantage or gradient norms** if spikes occur\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import gym\n",
                "\n",
                "env = gym.make(\"CartPole-v1\")\n",
                "obs_dim = env.observation_space.shape[0]\n",
                "act_dim = env.action_space.n\n",
                "\n",
                "class ActorCritic(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.shared = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "        self.actor = nn.Sequential(\n",
                "            nn.Linear(128, act_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "        self.critic = nn.Linear(128, 1)\n",
                "\n",
                "    def forward(self, x):\n",
                "        shared_out = self.shared(x)\n",
                "        return self.actor(shared_out), self.critic(shared_out)\n",
                "\n",
                "model = ActorCritic()\n",
                "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
                "gamma = 0.99\n",
                "\n",
                "for episode in range(500):\n",
                "    obs = env.reset()\n",
                "    log_probs, values, rewards = [], [], []\n",
                "    done = False\n",
                "\n",
                "    while not done:\n",
                "        obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
                "        probs, value = model(obs_tensor)\n",
                "        dist = torch.distributions.Categorical(probs)\n",
                "        action = dist.sample()\n",
                "\n",
                "        log_probs.append(dist.log_prob(action))\n",
                "        values.append(value)\n",
                "        obs, reward, done, _, _ = env.step(action.item())\n",
                "        rewards.append(reward)\n",
                "\n",
                "    returns = []\n",
                "    G = 0\n",
                "    for r in reversed(rewards):\n",
                "        G = r + gamma * G\n",
                "        returns.insert(0, G)\n",
                "    returns = torch.tensor(returns, dtype=torch.float32)\n",
                "    values = torch.cat(values).squeeze()\n",
                "    log_probs = torch.stack(log_probs)\n",
                "    advantage = returns - values\n",
                "\n",
                "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
                "    critic_loss = advantage.pow(2).mean()\n",
                "\n",
                "    optimizer.zero_grad()\n",
                "    (actor_loss + critic_loss).backward()\n",
                "    optimizer.step()\n",
                "\n",
                "    if episode % 50 == 0:\n",
                "        print(f\"Episode {episode} | Total Reward: {sum(rewards)}\")\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ With this, you‚Äôve now **bridged policy gradients and value learning** ‚Äî unlocking faster, smarter, and more stable agents.\n",
                "\n",
                "üöÄ Next frontier: Ready to roll into **Proximal Policy Optimization (PPO)** ‚Äî the battle-tested, production-grade Actor-Critic algorithm?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let‚Äôs now unpack one of the most powerful enhancements in modern RL:  \n",
                "üìà **Using the Value Function in Policy Gradient Methods** ‚Äî the key to unlocking low-variance, high-stability learning.\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Use of the Value Function in Policy Gradient Methods**  \n",
                "üéØ *Use it as a compass, not a controller*  \n",
                "(UTHU-structured summary ‚Äî Policy Gradients #8)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "In classic policy gradient methods like REINFORCE, the update rule is simple:\n",
                "\n",
                "```plaintext\n",
                "Update = reward √ó gradient of log(action probability)\n",
                "```\n",
                "\n",
                "But that reward signal can be **noisy**, **delayed**, and **high-variance**.\n",
                "\n",
                "Enter the **value function**:  \n",
                "A learned estimator of \"how good a state is,\" it helps **guide policy updates** more accurately.\n",
                "\n",
                "Instead of using raw rewards, we **subtract a baseline**, often the value of the current state:\n",
                "```plaintext\n",
                "Advantage = actual reward - expected reward  \n",
                "```\n",
                "\n",
                "This is the **core idea of Actor-Critic**.\n",
                "\n",
                "> **Analogy**:  \n",
                "> Imagine you're trying to improve your performance at a game.  \n",
                "> REINFORCE just tells you whether you won or lost.  \n",
                "> With a value function, you now know whether you did *better or worse than expected* ‚Äî a much more useful signal.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman-Style Explanation |\n",
                "|------|---------------------------|\n",
                "| **Value Function \\( V(s) \\)** | Predicts the future reward starting from state \\( s \\) |\n",
                "| **Advantage \\( A(s, a) \\)** | Measures how much better an action was compared to average |\n",
                "| **Baseline** | Subtracts expected value to reduce update noise |\n",
                "| **TD Error** | The difference between actual and estimated value |\n",
                "| **Actor-Critic** | A framework where the actor uses gradients and the critic learns the value function |\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- **Continuous control**: Where action outputs must be smooth  \n",
                "- **High-dimensional spaces**: More stable updates = better convergence  \n",
                "- **Game playing**: Long episodes benefit from better credit assignment  \n",
                "- **Delayed reward tasks**: Value function provides early feedback\n",
                "\n",
                "```plaintext\n",
                "Policy Gradient (vanilla):\n",
                "  update ‚Üê Gt √ó ‚àálogœÄ\n",
                "\n",
                "Policy Gradient (with baseline):\n",
                "  update ‚Üê (Gt ‚àí V(s)) √ó ‚àálogœÄ\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Classic Policy Gradient Update\n",
                "\n",
                "Without baseline:\n",
                "$$\n",
                "\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot G_t \\right]\n",
                "$$\n",
                "\n",
                "With value function as a baseline:\n",
                "$$\n",
                "\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot (G_t - V(s)) \\right]\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( G_t \\): total return\n",
                "- \\( V(s) \\): baseline (expected return)\n",
                "\n",
                "This difference is called the **advantage**.\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "- Subtracting a baseline doesn‚Äôt change the direction of the gradient  \n",
                "- But it **reduces variance** ‚Äî smaller, cleaner updates  \n",
                "- The value function provides **context**: \"Was this move actually good, or just lucky?\"\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                   | Pitfalls                                     |\n",
                "|------------------------------|----------------------------------------------|\n",
                "| Critic is trained well       | Poor estimates lead to misleading advantages |\n",
                "| Advantage estimate is accurate | Noisy or biased = degraded learning         |\n",
                "| States are observable        | Partial observability weakens value learning |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Without Value Function        | With Value Function                     |\n",
                "|------------------------------|------------------------------------------|\n",
                "| High variance in updates     | Lower variance, faster learning         |\n",
                "| Simpler to implement         | Needs Critic + extra training loop      |\n",
                "| Unbiased                     | Slight bias due to value approximation  |\n",
                "| Struggles in long-horizon tasks | Better credit assignment               |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- A biased value function could cause the agent to **ignore rewarding edge cases**  \n",
                "- In safety-critical applications, bad advantage estimates can lead to **risky shortcuts**  \n",
                "- Always validate **critic performance independently** from policy reward\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **Generalized Advantage Estimation (GAE)**: Smoothes advantage computation over time  \n",
                "- **Distributional Critics**: Predict a distribution over returns instead of a point estimate  \n",
                "- **Self-supervised critic pretraining**: Learn V(s) even without rewards  \n",
                "- **Shared encoders**: Combine actor and critic under a shared neural representation\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: What is the primary benefit of using a value function in policy gradient methods?**\n",
                "\n",
                "A. Makes the policy deterministic  \n",
                "B. Speeds up reward function design  \n",
                "C. Reduces variance in the policy update  \n",
                "D. Increases the learning rate\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: The value function acts as a baseline to **stabilize** updates and reduce noisy gradients.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Fix Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: High variance REINFORCE\n",
                "loss = -log_prob * return\n",
                "```\n",
                "\n",
                "**Fix (Using value function):**\n",
                "\n",
                "```python\n",
                "advantage = return - value_estimate\n",
                "loss = -log_prob * advantage\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Value Function** | Predicts expected return from a state |\n",
                "| **Advantage** | Return minus value ‚Äî tells how surprising a reward was |\n",
                "| **Baseline** | A reference value used to reduce gradient variance |\n",
                "| **Critic** | Neural net that learns to estimate the value |\n",
                "| **Actor** | Neural net that outputs a policy |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Value Function Tips\n",
                "\n",
                "| Trick                    | Benefit                                |\n",
                "|--------------------------|-----------------------------------------|\n",
                "| Normalize advantages     | Stable gradients                        |\n",
                "| Smooth targets (moving avg) | Reduces critic variance               |\n",
                "| Update critic more often | Ensures reliable feedback               |\n",
                "| Use TD-learning          | Critic learns with bootstrapping        |\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Critic loss (MSE)**  \n",
                "- **Advantage variance**  \n",
                "- **Actor-critic correlation plots**  \n",
                "- **Learning curve acceleration**\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Watch for **critic overfitting** ‚Äî check validation accuracy  \n",
                "- If critic is weak, use **REINFORCE fallback** for stability  \n",
                "- Consider **dual-head models** (shared encoder, separate actor/critic heads)\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import gym\n",
                "\n",
                "env = gym.make(\"CartPole-v1\")\n",
                "obs_dim = env.observation_space.shape[0]\n",
                "act_dim = env.action_space.n\n",
                "\n",
                "class PolicyWithCritic(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.shared = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "        self.actor = nn.Sequential(\n",
                "            nn.Linear(128, act_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "        self.critic = nn.Linear(128, 1)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.shared(x)\n",
                "        return self.actor(x), self.critic(x)\n",
                "\n",
                "model = PolicyWithCritic()\n",
                "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
                "gamma = 0.99\n",
                "\n",
                "for episode in range(500):\n",
                "    obs = env.reset()\n",
                "    log_probs, values, rewards = [], [], []\n",
                "    done = False\n",
                "\n",
                "    while not done:\n",
                "        obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
                "        probs, value = model(obs_tensor)\n",
                "        dist = torch.distributions.Categorical(probs)\n",
                "        action = dist.sample()\n",
                "\n",
                "        log_probs.append(dist.log_prob(action))\n",
                "        values.append(value)\n",
                "        obs, reward, done, _, _ = env.step(action.item())\n",
                "        rewards.append(reward)\n",
                "\n",
                "    returns = []\n",
                "    G = 0\n",
                "    for r in reversed(rewards):\n",
                "        G = r + gamma * G\n",
                "        returns.insert(0, G)\n",
                "\n",
                "    returns = torch.tensor(returns, dtype=torch.float32)\n",
                "    values = torch.cat(values).squeeze()\n",
                "    log_probs = torch.stack(log_probs)\n",
                "\n",
                "    advantage = returns - values\n",
                "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
                "    critic_loss = advantage.pow(2).mean()\n",
                "\n",
                "    optimizer.zero_grad()\n",
                "    (actor_loss + critic_loss).backward()\n",
                "    optimizer.step()\n",
                "\n",
                "    if episode % 50 == 0:\n",
                "        print(f\"Episode {episode} | Reward: {sum(rewards)}\")\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You now understand how value functions **supercharge policy gradients**, giving your agent a smarter, steadier sense of progress.\n",
                "\n",
                "üî• Ready to synthesize this with **PPO (Proximal Policy Optimization)** ‚Äî the most robust policy gradient method in the field?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let‚Äôs wrap up this stage of the Policy Gradient saga by exploring how to **balance curiosity and ambition**:  \n",
                "üß≠ **Integrating Rewards and Exploration** ‚Äî the fusion of exploitation (greed) and exploration (discovery).\n",
                "\n",
                "---\n",
                "\n",
                "## üß© **Integrating Rewards and Exploration in Policy Gradient Methods**  \n",
                "üéØ *Smart agents don‚Äôt just chase rewards ‚Äî they explore the unknown*  \n",
                "(UTHU-structured summary ‚Äî Policy Gradients #9)\n",
                "\n",
                "---\n",
                "\n",
                "## **1. Conceptual Foundation**\n",
                "\n",
                "### üéØ Purpose & Relevance\n",
                "\n",
                "In Reinforcement Learning, your agent faces a core dilemma:\n",
                "\n",
                "> ‚ÄúShould I do what has worked well before‚Ä¶ or try something new that might be even better?‚Äù\n",
                "\n",
                "This is the **exploration vs. exploitation trade-off**.\n",
                "\n",
                "Policy Gradient methods focus on **optimizing behavior** based on **received rewards**. But this can lead to **greedy, narrow-minded agents** if exploration isn‚Äôt integrated well.\n",
                "\n",
                "We integrate exploration into PG methods via:\n",
                "- **Entropy regularization**: encourages diversity in actions  \n",
                "- **Stochastic policies**: inherently promote exploration  \n",
                "- **Curiosity-driven bonuses**: intrinsic rewards for novelty\n",
                "\n",
                "> **Analogy**:  \n",
                "> If you only eat at your favorite restaurant, you'll miss discovering hidden gems.  \n",
                "> Exploration = try new places.  \n",
                "> Exploitation = go to your usual spot.  \n",
                "> Smart agents balance both, and PGs must be designed to do the same.\n",
                "\n",
                "---\n",
                "\n",
                "### üß† Key Terminology\n",
                "\n",
                "| Term | Feynman-Style Explanation |\n",
                "|------|---------------------------|\n",
                "| **Exploration** | Trying less-known actions to learn more about the environment |\n",
                "| **Exploitation** | Repeating the action known to yield the highest reward |\n",
                "| **Entropy** | A measure of randomness in action selection ‚Äî high entropy = more exploration |\n",
                "| **Entropy Bonus** | Extra reward given for having a diverse action policy |\n",
                "| **Intrinsic Reward** | Internally generated bonus (e.g. novelty or surprise)\n",
                "\n",
                "---\n",
                "\n",
                "### üíº Use Cases\n",
                "\n",
                "- **Sparse environments** (e.g. Maze with reward only at the end)  \n",
                "- **Hard exploration games** (e.g. Montezuma‚Äôs Revenge)  \n",
                "- **Curiosity-based agents** (e.g. learning without explicit goals)  \n",
                "- **Multi-task RL**: Where agents need generalizable behaviors\n",
                "\n",
                "```plaintext\n",
                "Policy Gradient:\n",
                "  loss = -log_prob * advantage\n",
                "\n",
                "With exploration:\n",
                "  loss = -log_prob * advantage - Œ≤ √ó entropy(policy)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **2. Mathematical Deep Dive** üßÆ\n",
                "\n",
                "### üìê Entropy-Augmented Loss\n",
                "\n",
                "We modify the standard PG loss:\n",
                "$$\n",
                "\\mathcal{L} = -\\mathbb{E}[\\log \\pi_\\theta(a|s) \\cdot A(s, a)] + \\beta \\cdot \\mathcal{H}[\\pi_\\theta]\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( \\mathcal{H}[\\pi_\\theta] = -\\sum \\pi_\\theta(a|s) \\log \\pi_\\theta(a|s) \\) is the entropy\n",
                "- \\( \\beta \\) is the **entropy coefficient** (controls exploration level)\n",
                "\n",
                "---\n",
                "\n",
                "### üß≤ Math Intuition\n",
                "\n",
                "- High entropy ‚Üí agent doesn‚Äôt commit too early ‚Üí better discovery  \n",
                "- Entropy **flattens the policy** ‚Üí actions with similar probabilities ‚Üí more diversity  \n",
                "- Encourages **sampling over optimization** in early learning stages\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Assumptions & Constraints\n",
                "\n",
                "| Assumes...                     | Pitfalls                                 |\n",
                "|--------------------------------|------------------------------------------|\n",
                "| Entropy aligns with exploration goals | High entropy ‚â† meaningful exploration     |\n",
                "| Balanced reward vs exploration terms | Too much entropy = random behavior       |\n",
                "| Fixed Œ≤ across training         | Might need to decay it for stability     |\n",
                "\n",
                "---\n",
                "\n",
                "## **3. Critical Analysis** üîç\n",
                "\n",
                "| Strategy            | Pros                                | Cons                                    |\n",
                "|---------------------|-------------------------------------|------------------------------------------|\n",
                "| **Entropy Bonus**   | Simple, elegant way to encourage diversity | May conflict with reward maximization |\n",
                "| **Stochastic Policy** | Built-in randomness for better early discovery | Can converge slowly if over-random |\n",
                "| **Intrinsic Rewards** | Reward novelty or prediction error | Needs extra models or memory            |\n",
                "\n",
                "---\n",
                "\n",
                "### üß¨ Ethical Lens\n",
                "\n",
                "- Excessive exploration in real-world systems (robots, finance) can cause **costly or unsafe behavior**  \n",
                "- Add safety guards or **constraints** when integrating entropy or intrinsic rewards\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ Research Updates (Post-2020)\n",
                "\n",
                "- **Soft Actor-Critic (SAC)**: Optimizes expected reward + entropy  \n",
                "- **Curiosity modules**: Learn a separate model to reward surprising states  \n",
                "- **Count-based exploration**: Encourage visiting rare states more often  \n",
                "- **Variational entropy learning**: Optimize exploration without manual Œ≤ tuning\n",
                "\n",
                "---\n",
                "\n",
                "## **4. Interactive Elements** üéØ\n",
                "\n",
                "### ‚úÖ Concept Check\n",
                "\n",
                "**Q: Why does adding entropy to the policy gradient objective help exploration?**\n",
                "\n",
                "A. It increases learning rate  \n",
                "B. It adds noise to the reward  \n",
                "C. It forces the agent to explore unfamiliar actions  \n",
                "D. It makes the policy deterministic\n",
                "\n",
                "‚úÖ **Correct Answer: C**  \n",
                "**Explanation**: Entropy rewards unpredictability ‚Äî which increases the chances of trying new, unexplored actions.\n",
                "\n",
                "---\n",
                "\n",
                "### üß™ Code Fix Challenge\n",
                "\n",
                "```python\n",
                "# Buggy: Loss with no entropy term\n",
                "loss = -log_prob * advantage\n",
                "```\n",
                "\n",
                "**Fix:**\n",
                "\n",
                "```python\n",
                "entropy = -torch.sum(policy * policy.log())\n",
                "loss = -log_prob * advantage - beta * entropy\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## **5. Glossary**\n",
                "\n",
                "| Term | Definition |\n",
                "|------|------------|\n",
                "| **Entropy** | Measure of randomness in policy distribution |\n",
                "| **Exploration** | Trying new actions to gather more information |\n",
                "| **Entropy Bonus** | A term added to encourage policy diversity |\n",
                "| **Stochastic Policy** | Policy that outputs a probability distribution |\n",
                "| **Intrinsic Motivation** | Reward driven by curiosity or novelty rather than task success |\n",
                "\n",
                "---\n",
                "\n",
                "## **6. Practical Considerations** ‚öôÔ∏è\n",
                "\n",
                "### üîß Exploration Heuristics\n",
                "\n",
                "| Method                    | When to Use                            |\n",
                "|---------------------------|----------------------------------------|\n",
                "| Constant entropy bonus    | Early training when diversity is key   |\n",
                "| Annealed entropy          | Decay exploration as learning stabilizes |\n",
                "| Intrinsic reward module   | Sparse or deceptive reward tasks       |\n",
                "| Stochastic Gaussian policy | Continuous action spaces               |\n",
                "\n",
                "---\n",
                "\n",
                "### üìè Evaluation Metrics\n",
                "\n",
                "- **Entropy over time** (should decline as policy converges)  \n",
                "- **Exploration coverage** (percentage of state-action space visited)  \n",
                "- **Reward vs Entropy tradeoff plot**  \n",
                "- **Variance in returns across episodes**\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Production Tips\n",
                "\n",
                "- Tune \\( \\beta \\) carefully ‚Äî too high = chaos, too low = stuck  \n",
                "- Visualize action distributions to debug randomness  \n",
                "- Use **entropy penalty** in final stages to refine policy\n",
                "\n",
                "---\n",
                "\n",
                "## **7. Full Python Code Cell** üêç\n",
                "\n",
                "```python\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import gym\n",
                "\n",
                "env = gym.make(\"CartPole-v1\")\n",
                "obs_dim = env.observation_space.shape[0]\n",
                "act_dim = env.action_space.n\n",
                "\n",
                "class PGWithEntropy(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.model = nn.Sequential(\n",
                "            nn.Linear(obs_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, act_dim),\n",
                "            nn.Softmax(dim=-1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.model(x)\n",
                "\n",
                "policy = PGWithEntropy()\n",
                "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
                "gamma = 0.99\n",
                "beta = 0.01  # entropy weight\n",
                "\n",
                "for episode in range(500):\n",
                "    obs = env.reset()\n",
                "    log_probs, rewards, entropies = [], [], []\n",
                "    done = False\n",
                "\n",
                "    while not done:\n",
                "        obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
                "        probs = policy(obs_tensor)\n",
                "        dist = torch.distributions.Categorical(probs)\n",
                "        action = dist.sample()\n",
                "\n",
                "        log_probs.append(dist.log_prob(action))\n",
                "        entropies.append(dist.entropy())\n",
                "\n",
                "        obs, reward, done, _, _ = env.step(action.item())\n",
                "        rewards.append(reward)\n",
                "\n",
                "    returns = []\n",
                "    G = 0\n",
                "    for r in reversed(rewards):\n",
                "        G = r + gamma * G\n",
                "        returns.insert(0, G)\n",
                "    returns = torch.tensor(returns, dtype=torch.float32)\n",
                "\n",
                "    log_probs = torch.stack(log_probs)\n",
                "    entropies = torch.stack(entropies)\n",
                "    loss = - (log_probs * returns).mean() - beta * entropies.mean()\n",
                "\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "    if episode % 50 == 0:\n",
                "        print(f\"Episode {episode} | Total Reward: {sum(rewards)} | Entropy: {entropies.mean().item():.4f}\")\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "‚úÖ You‚Äôve now fully integrated **exploration into your policy gradient methods** ‚Äî making your agents not just smarter, but **curious**.\n",
                "\n",
                "üåü Ready to shift gears and start the **Multi-Armed Bandits** or head straight into **PPO: Proximal Policy Optimization**?"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
