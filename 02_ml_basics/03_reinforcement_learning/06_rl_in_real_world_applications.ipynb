{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "üî• That‚Äôs a sharp catch ‚Äî and I totally agree with you.\n",
                "\n",
                "The **decision flow diagram** I used in the **Path Planning & Manipulation Tasks** topic gave that topic a **visual logic boost** ‚Äî made it click like a seatbelt.  \n",
                "But I *totally dropped the visual baton* in the other sections. Let‚Äôs fix that now.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Here‚Äôs what I‚Äôll do:\n",
                "\n",
                "I‚Äôll go back and give you **clean visual decision flows** for the key topics we‚Äôve covered so far.  \n",
                "Each diagram will answer the question:  \n",
                "üß† *‚ÄúWhat‚Äôs the agent deciding at each stage?‚Äù*\n",
                "\n",
                "Ready? Let's start rolling them out below üëá\n",
                "\n",
                "---\n",
                "\n",
                "## 1Ô∏è‚É£ **Sim-to-Real Transfer**\n",
                "\n",
                "```\n",
                "[Train in Simulation]\n",
                "        |\n",
                "        v\n",
                "[Is Policy Robust to Changes?]\n",
                "        |                      \\\n",
                "     Yes                        No\n",
                "     |                            \\\n",
                "[Deploy in Real World]     [Domain Randomization or Adaptation]\n",
                "                                   |\n",
                "                                   v\n",
                "                          [Fine-tune on Real Data]\n",
                "```\n",
                "\n",
                "üéØ Use: Ensures real-world performance by preparing the agent for surprises during training.\n",
                "\n",
                "---\n",
                "\n",
                "## 2Ô∏è‚É£ **Continuous Control in Robotic Arms**\n",
                "\n",
                "```\n",
                "[Observe Joint States & Target]\n",
                "        |\n",
                "        v\n",
                "[Policy Outputs Torques]\n",
                "        |\n",
                "        v\n",
                "[Apply Smooth Motor Commands]\n",
                "        |\n",
                "        v\n",
                "[Evaluate: Error + Stability]\n",
                "        |\n",
                "        v\n",
                "[Update Policy (if training)]\n",
                "```\n",
                "\n",
                "üéØ Use: Real-valued actions require fine control over physical actuators ‚Äî no choppy \"go left\" commands here.\n",
                "\n",
                "---\n",
                "\n",
                "## 3Ô∏è‚É£ **RL for Decision-Making in Autonomous Vehicles**\n",
                "\n",
                "```\n",
                "[Sensor Input: Lidar + Camera + GPS]\n",
                "        |\n",
                "        v\n",
                "[State Encoding (Speed, Distance, Lane)]\n",
                "        |\n",
                "        v\n",
                "[Policy: Action Choice]\n",
                "(Accelerate / Brake / Lane Change / Wait)\n",
                "        |\n",
                "        v\n",
                "[Environment Feedback]\n",
                "(Collision? Smooth Ride? Obstacle?)\n",
                "        |\n",
                "        v\n",
                "[Reward Signal ‚Üí Learn or Adapt]\n",
                "```\n",
                "\n",
                "üéØ Use: Helps self-driving cars *think*, not just react.\n",
                "\n",
                "---\n",
                "\n",
                "## 4Ô∏è‚É£ **Traffic Simulation for Training RL Models**\n",
                "\n",
                "```\n",
                "[Scenario Generator]\n",
                "  |     \\\n",
                "[Pedestrians] [Vehicles]\n",
                "      \\          /\n",
                "     [Multi-Agent Interaction]\n",
                "             |\n",
                "             v\n",
                "[RL Agent Learns Optimal Policy]\n",
                "             |\n",
                "             v\n",
                "[Evaluate in Diverse Sim Conditions]\n",
                "```\n",
                "\n",
                "üéØ Use: Build worlds where RL can safely learn edge-case behavior.\n",
                "\n",
                "---\n",
                "\n",
                "## 5Ô∏è‚É£ **Real-Time Planning & Adaptive Navigation**\n",
                "\n",
                "```\n",
                "[Get Current Sensor Data]\n",
                "        |\n",
                "        v\n",
                "[Predict Environment Changes (cars, lights)]\n",
                "        |\n",
                "        v\n",
                "[Short-Term Trajectory Planning]\n",
                "        |\n",
                "        v\n",
                "[Choose Action Based on Cost Map]\n",
                "        |\n",
                "        v\n",
                "[Act ‚Üí Sense ‚Üí Replan]\n",
                "        |\n",
                "        ‚îî‚îÄ> (Loop every ~100ms)\n",
                "```\n",
                "\n",
                "üéØ Use: Keeps the car responsive in dynamic traffic situations.\n",
                "\n",
                "---\n",
                "\n",
                "## 6Ô∏è‚É£ **RL for Game AI (Chess, Go, Video Games)**\n",
                "\n",
                "```\n",
                "[Observe Game State]\n",
                "        |\n",
                "        v\n",
                "[Policy: Choose Best Move]\n",
                "        |\n",
                "        v\n",
                "[Environment: Return New State + Reward]\n",
                "        |\n",
                "        v\n",
                "[Update Q-values / Policy Network]\n",
                "        |\n",
                "        v\n",
                "[Repeat Until Game Ends]\n",
                "```\n",
                "\n",
                "üéØ Use: Teaches agents strategy without supervision, purely by winning and losing.\n",
                "\n",
                "---\n",
                "\n",
                "## 7Ô∏è‚É£ **Multi-Agent & Competitive RL**\n",
                "\n",
                "```\n",
                "[Agent 1 Observes State]    [Agent 2 Observes State]\n",
                "        |                              |\n",
                "        v                              v\n",
                "[Choose Action A1]            [Choose Action A2]\n",
                "        \\                              /\n",
                "          \\                         /\n",
                "        [Environment Combines A1, A2]\n",
                "                    |\n",
                "                    v\n",
                "         [Both Receive Rewards]\n",
                "                    |\n",
                "                    v\n",
                "            [Policy Update (Both)]\n",
                "```\n",
                "\n",
                "üéØ Use: Models both **cooperation and competition** ‚Äî essential in games and real-world social systems.\n",
                "\n",
                "---\n",
                "\n",
                "## 8Ô∏è‚É£ **AlphaZero**\n",
                "\n",
                "```\n",
                "[Game State]\n",
                "    |\n",
                "    v\n",
                "[Neural Net: Policy & Value Estimation]\n",
                "    |\n",
                "    v\n",
                "[MCTS: Simulate Hundreds of Outcomes]\n",
                "    |\n",
                "    v\n",
                "[Choose Most Promising Move]\n",
                "    |\n",
                "    v\n",
                "[Self-Play ‚Üí Train NN on Results]\n",
                "```\n",
                "\n",
                "üéØ Use: Strategy without human data. Trains via high-quality self-generated experience.\n",
                "\n",
                "---\n",
                "\n",
                "Would you like these **diagrams in image format** too? I can quickly generate visuals so you can use them in slides, docs, or a PDF cheat sheet."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Absolutely! Here's a full structured summary of **\"Path Planning and Manipulation Tasks\"** in **Reinforcement Learning for Robotics**, designed using your framework ‚Äî clear, beginner-friendly, with analogies, and guided by the Feynman technique.\n",
                "\n",
                "---\n",
                "\n",
                "## üß† **1. Conceptual Foundation**\n",
                "\n",
                "### üìå **Purpose & Relevance**  \n",
                "**Why care?**  \n",
                "Imagine you're teaching a robotic arm to pick up a cup of coffee without spilling it ‚Äî sounds easy, right? But for robots, it's like teaching a toddler to play Jenga‚Ä¶ with chopsticks. ü§π\n",
                "\n",
                "**Path planning** is the brain's ability to decide *how* to move, while **manipulation** is about *what actions* to perform once movement starts. Reinforcement Learning (RL) helps robots *learn by trial and error*, optimizing their choices to reach a goal ‚Äî just like training a puppy with treats.\n",
                "\n",
                "üîß **Real-world analogy**:  \n",
                "- **Path planning** = navigating through a crowded room without bumping into anyone  \n",
                "- **Manipulation** = picking up a delicate egg without cracking it  \n",
                "\n",
                "Both require learning what‚Äôs safe, what‚Äôs effective, and how to adapt if the environment changes.\n",
                "\n",
                "---\n",
                "\n",
                "### üîë **Key Terminology**\n",
                "\n",
                "| Term                | Metaphor / Analogy |\n",
                "|---------------------|--------------------|\n",
                "| **Action Space**    | A robot's \"menu\" of possible moves ‚Äî like selecting chess moves each turn. |\n",
                "| **State Space**     | The robot‚Äôs ‚Äúmap‚Äù of current reality ‚Äî like looking at a game board. |\n",
                "| **Policy**          | A rulebook that says, \"If you're here, then do that.\" Like traffic signs. |\n",
                "| **Reward Signal**   | Treats for good behavior. Positive if robot gets closer to the cup, negative if it knocks it over. |\n",
                "| **Trajectory**      | The path taken from start to finish ‚Äî like a GPS route from home to work. |\n",
                "\n",
                "---\n",
                "\n",
                "### üìà **Use Cases**\n",
                "\n",
                "- **Robotic arms in manufacturing**: Welding or assembling parts.\n",
                "- **Service robots**: Delivering objects in cluttered rooms.\n",
                "- **Surgical robots**: Precise, minimal-damage movements.\n",
                "\n",
                "üìç **Decision Flow (Simplified)**:\n",
                "```\n",
                "+-------------+\n",
                "| Start State |\n",
                "+------+------+\n",
                "       |\n",
                "       v\n",
                "+------+------+\n",
                "|  Choose Action  <--+\n",
                "+------+------+\\     |\n",
                "       |       |     |\n",
                "       v       |     |\n",
                "+------+------+/     |\n",
                "|  Next State |      |\n",
                "+------+------+\\     |\n",
                "       |       |     |\n",
                "       v       |     |\n",
                "+------+------+/     |\n",
                "| Evaluate Reward <--+\n",
                "+-------------+\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## üßÆ **2. Mathematical Deep Dive**\n",
                "\n",
                "### üìê **Core Equations**\n",
                "\n",
                "In RL, we often define a **policy** œÄ that maps states (s) to actions (a):\n",
                "\n",
                "$$\n",
                "\\pi(a|s) = P(a \\mid s)\n",
                "$$\n",
                "\n",
                "**Return** \\( G_t \\) is the total future reward:\n",
                "\n",
                "$$\n",
                "G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
                "$$\n",
                "\n",
                "**Objective**: Maximize expected return:\n",
                "\n",
                "$$\n",
                "J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\gamma^t r_t \\right]\n",
                "$$\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è **Math Intuition**  \n",
                "- Think of **policy** as a dynamic decision tree that adapts with experience.\n",
                "- **Discount factor** \\( \\gamma \\) behaves like memory ‚Äî short-term vs. long-term focus.\n",
                "- Maximizing return is like choosing the hiking path with the best view *and* least effort.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è **Assumptions & Constraints**\n",
                "- Environments are often **Markovian**: future only depends on present.\n",
                "- Many methods assume **continuous actions**, which complicates exploration.\n",
                "- **Sim-to-real gap**: what works in simulation may not translate well to reality.\n",
                "\n",
                "---\n",
                "\n",
                "## üîç **3. Critical Analysis**\n",
                "\n",
                "### ‚öñÔ∏è **Strengths vs Weaknesses**\n",
                "\n",
                "| Strengths                        | Weaknesses                        |\n",
                "|----------------------------------|-----------------------------------|\n",
                "| Adapts to complex environments   | Requires tons of training data    |\n",
                "| Works in continuous action spaces| Sim-to-real transfer is tricky    |\n",
                "| Handles high-dimensional inputs  | Sensitive to reward design        |\n",
                "\n",
                "---\n",
                "\n",
                "### üß≠ **Ethical Lens**\n",
                "- **Safety-critical errors**: In manipulation, a wrong move can destroy hardware.\n",
                "- **Bias in training data**: A poorly simulated environment can bias real-world behavior.\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ **Research Updates**\n",
                "- **Residual RL**: Combines classical control with RL (Google Brain, 2020+)\n",
                "- **Diffusion policies for robotics**: Nonlinear trajectory generation via transformers (Stanford, 2022+)\n",
                "- **Domain Randomization**: Improves sim-to-real success rate\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ **4. Interactive Elements**\n",
                "\n",
                "### ‚úÖ **Concept Check**\n",
                "\n",
                "**Q1. Why is reward shaping risky in manipulation tasks?**\n",
                "\n",
                "A. It makes the robot learn faster  \n",
                "B. It can lead to unintended shortcuts or bad habits  \n",
                "C. It ensures better sim-to-real transfer  \n",
                "D. It removes exploration\n",
                "\n",
                "‚úÖ **Answer**: **B** ‚Äì Over-tuned rewards can make the agent ‚Äúcheat‚Äù the task without learning generalizable behavior.\n",
                "\n",
                "---\n",
                "\n",
                "### üß© **Code Exercise**\n",
                "\n",
                "```python\n",
                "def compute_reward(grip_force: float, distance: float) -> float:\n",
                "    # A poorly designed reward function\n",
                "    return grip_force * 2 - distance\n",
                "\n",
                "# Problem: Reward is maximized by squeezing hard even if far away\n",
                "```\n",
                "\n",
                "üîß **Fix**:\n",
                "- Penalize excessive grip force\n",
                "- Add bonus only when close\n",
                "\n",
                "```python\n",
                "def fixed_reward(grip_force: float, distance: float) -> float:\n",
                "    return -distance + (1.0 if distance < 0.1 else 0.0) - 0.5 * max(grip_force - 1.0, 0)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## üìö **5. Glossary**\n",
                "\n",
                "| Term          | Explanation |\n",
                "|---------------|-------------|\n",
                "| **Trajectory** | The full sequence of states and actions the robot goes through. |\n",
                "| **Policy**     | Strategy or function that tells the robot what action to take. |\n",
                "| **State**      | A snapshot of the robot‚Äôs current situation. |\n",
                "| **Reward**     | A numerical score given for doing something right (or wrong). |\n",
                "| **Simulation** | A digital environment where we train robots safely before real-world tests. |\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è **6. Practical Considerations**\n",
                "\n",
                "### üß™ **Hyperparameters**\n",
                "\n",
                "- **Learning rate**: Too high = unstable; too low = slow learning.\n",
                "- **Discount factor**: \\( \\gamma = 0.9 \\) usually good for manipulation (short-term planning).\n",
                "- **Exploration**: Use entropy regularization to prevent premature convergence.\n",
                "\n",
                "---\n",
                "\n",
                "### üìè **Evaluation Metrics**\n",
                "\n",
                "```python\n",
                "# Success rate: percentage of successful grasps\n",
                "success_rate = np.mean([result[\"success\"] for result in test_runs])\n",
                "```\n",
                "\n",
                "- **Smoothness of trajectory**\n",
                "- **Total reward per episode**\n",
                "- **Time to complete task**\n",
                "\n",
                "---\n",
                "\n",
                "### üöÄ **Production Tips**\n",
                "- **Simulation**: Train in randomized environments to reduce sim-to-real gap.\n",
                "- **Sensor noise**: Inject noise during training.\n",
                "- **Latency**: Ensure inference speed < environment step speed.\n",
                "\n",
                "---\n",
                "\n",
                "## üêç **7. Full Python Code Walkthrough**\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def generate_trajectory(start: np.ndarray, goal: np.ndarray, steps: int = 20) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Simulate a simple linear path from start to goal.\n",
                "    \"\"\"\n",
                "    trajectory = np.linspace(start, goal, steps)\n",
                "    return trajectory\n",
                "\n",
                "def visualize_trajectory(traj: np.ndarray) -> None:\n",
                "    \"\"\"\n",
                "    Visualizes 2D trajectory of the robot.\n",
                "    \"\"\"\n",
                "    plt.plot(traj[:, 0], traj[:, 1], marker='o')\n",
                "    plt.title(\"Planned Trajectory\")\n",
                "    plt.xlabel(\"X\")\n",
                "    plt.ylabel(\"Y\")\n",
                "    plt.grid(True)\n",
                "    plt.show()\n",
                "\n",
                "# Example\n",
                "start = np.array([0, 0])\n",
                "goal = np.array([5, 5])\n",
                "traj = generate_trajectory(start, goal)\n",
                "visualize_trajectory(traj)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "Let me know if you‚Äôd like the same breakdown for **Sim-to-Real Transfer** or **Continuous Control** next. ü¶æ"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Perfect ‚Äî let's dive into the **Sim-to-Real Transfer** section now! This is where RL gets out of the lab and into the real world‚Ä¶ hopefully without knocking over a few vases on the way. üèóÔ∏èü§ñ\n",
                "\n",
                "---\n",
                "\n",
                "## üß† **1. Conceptual Foundation**\n",
                "\n",
                "### üìå **Purpose & Relevance**\n",
                "\n",
                "Sim-to-real transfer is like teaching a pilot on a flight simulator and then expecting them to land a real plane in bad weather. üéÆ‚úàÔ∏è\n",
                "\n",
                "In robotics, simulations are **safe**, **fast**, and **cheap**, but reality is **messy**, **noisy**, and **full of surprises**. The challenge is that an RL agent trained in a clean, perfect sim often fails in real life. Why? Because sim environments can‚Äôt capture every quirk of the real world ‚Äî especially physics, friction, sensor noise, or wear-and-tear.\n",
                "\n",
                "So we need techniques to **bridge that gap** between simulated perfection and real-world chaos.\n",
                "\n",
                "---\n",
                "\n",
                "### üîë **Key Terminology**\n",
                "\n",
                "| Term                    | Metaphor / Analogy |\n",
                "|-------------------------|--------------------|\n",
                "| **Domain Gap**          | Like switching from English to French mid-conversation ‚Äî syntax may transfer, but details break. |\n",
                "| **Domain Randomization**| Making your sim ‚Äúweird on purpose‚Äù so your agent gets used to surprises ‚Äî like training in a room with the lights off. |\n",
                "| **Real2Sim**            | Using real-world data to fine-tune the simulation ‚Äî like recreating a driving route in GTA based on dashcam footage. |\n",
                "| **Transferability**     | The ability of a policy to generalize from one world to another. |\n",
                "| **Sim Fidelity**        | How close a sim feels to the real world ‚Äî like a video game with ultra-realistic graphics and physics. |\n",
                "\n",
                "---\n",
                "\n",
                "### üìà **Use Cases**\n",
                "\n",
                "- **Factory robots**: Train safely in sim before deploying on real conveyor belts.\n",
                "- **Autonomous drones**: Practice navigating complex airspaces digitally first.\n",
                "- **Warehouse automation**: Handling packages of various shapes/sizes under uncertainty.\n",
                "\n",
                "---\n",
                "\n",
                "## üßÆ **2. Mathematical Deep Dive**\n",
                "\n",
                "### üìê **Core Equations**\n",
                "\n",
                "Let‚Äôs denote:\n",
                "- \\( M_s = \\text{Simulated MDP} \\)\n",
                "- \\( M_r = \\text{Real-world MDP} \\)\n",
                "\n",
                "A policy \\( \\pi \\) trained on \\( M_s \\) should also perform well on \\( M_r \\). The goal is to minimize:\n",
                "\n",
                "$$\n",
                "\\Delta = |J_{M_s}(\\pi) - J_{M_r}(\\pi)|\n",
                "$$\n",
                "\n",
                "Where \\( J \\) is the expected return of policy \\( \\pi \\).\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è **Math Intuition**\n",
                "\n",
                "This is about **distributional shift**. If your policy only sees perfect data (sim), it won't survive the real world's random potholes. So you try to:\n",
                "- **Expose it to more diversity** in sim (domain randomization)\n",
                "- **Make sim closer to real** (domain adaptation)\n",
                "\n",
                "Imagine training a dog on a clean floor, then taking it outside in the rain. Not the same, right?\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è **Assumptions & Constraints**\n",
                "- Real-world interaction is expensive ‚Äî we can‚Äôt afford millions of trial runs.\n",
                "- Sensors may behave differently in real vs. sim (latency, noise).\n",
                "- Actuator torque and physics may be simulated inaccurately.\n",
                "\n",
                "---\n",
                "\n",
                "## üîç **3. Critical Analysis**\n",
                "\n",
                "### ‚öñÔ∏è **Strengths vs Weaknesses**\n",
                "\n",
                "| Strengths                        | Weaknesses                          |\n",
                "|----------------------------------|-------------------------------------|\n",
                "| Safe training in simulation      | Real-world mismatch (domain gap)    |\n",
                "| Scalable, parallelizable         | Hard to model real noise accurately |\n",
                "| Great for pre-training policies  | Poor sim design = useless policy    |\n",
                "\n",
                "---\n",
                "\n",
                "### üß≠ **Ethical Lens**\n",
                "- Robots trained only in sim might **misbehave or fail catastrophically** in real settings.\n",
                "- Need to test in controlled real-world setups to avoid unintended damage or harm.\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ **Research Updates (Post-2020)**\n",
                "- **OpenAI‚Äôs Rubik‚Äôs Cube robot** (2019‚Äì2020): Used **massive domain randomization**.\n",
                "- **Real2Sim via inverse graphics**: Using real sensor data to fine-tune sim params.\n",
                "- **World Models (DreamerV3)**: Agents learn a sim *inside their heads* ‚Äî works well in low-data settings.\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ **4. Interactive Elements**\n",
                "\n",
                "### ‚úÖ **Concept Check**\n",
                "\n",
                "**Q: Why can overfitting to a perfect sim environment be dangerous in real-world deployment?**\n",
                "\n",
                "A. It improves robustness  \n",
                "B. It makes models more generalizable  \n",
                "C. It reduces training cost  \n",
                "D. It causes poor real-world performance  \n",
                "\n",
                "‚úÖ **Answer**: **D** ‚Äì Overfitting to sim often makes the agent brittle to unexpected real-world conditions.\n",
                "\n",
                "---\n",
                "\n",
                "### üß© **Code Challenge**\n",
                "\n",
                "```python\n",
                "# Simulation training with fixed gravity (bad practice)\n",
                "env.gravity = 9.81\n",
                "for i in range(episodes):\n",
                "    reward = agent.train(env)\n",
                "```\n",
                "\n",
                "üõ†Ô∏è **Fix with domain randomization**:\n",
                "\n",
                "```python\n",
                "# Randomize gravity to mimic real-world variation\n",
                "for i in range(episodes):\n",
                "    env.gravity = np.random.uniform(8.0, 11.0)\n",
                "    reward = agent.train(env)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## üìö **5. Glossary**\n",
                "\n",
                "| Term                | Explanation |\n",
                "|---------------------|-------------|\n",
                "| **Domain Gap**       | Difference between sim and real environments. |\n",
                "| **Randomization**    | Intentionally introducing variation in training. |\n",
                "| **Fidelity**         | How real the sim feels. |\n",
                "| **Transferability**  | How well a trained model performs in new settings. |\n",
                "| **Policy**           | The robot‚Äôs decision-making brain. |\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è **6. Practical Considerations**\n",
                "\n",
                "### üß™ **Hyperparameters**\n",
                "- **Randomization ranges**: How much noise to inject\n",
                "- **Fine-tuning epochs**: Post-sim training on real data\n",
                "- **Curriculum learning**: Start easy, make sim harder gradually\n",
                "\n",
                "---\n",
                "\n",
                "### üìè **Evaluation Metrics**\n",
                "\n",
                "```python\n",
                "# Sim2Real Transfer Score\n",
                "def sim2real_gap(sim_reward, real_reward):\n",
                "    return abs(sim_reward - real_reward)\n",
                "```\n",
                "\n",
                "- **Policy robustness under perturbation**\n",
                "- **Success rate in real environment**\n",
                "- **Time to adapt post-deployment**\n",
                "\n",
                "---\n",
                "\n",
                "### üöÄ **Production Tips**\n",
                "- **Test safely**: Begin with low-power or slow movements.\n",
                "- **Sensor calibration**: Match real sensors as closely as possible.\n",
                "- **Batch RL + real rollouts**: Mix simulated and real experience during training.\n",
                "\n",
                "---\n",
                "\n",
                "## üêç **7. Full Python Code Walkthrough**\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def randomize_env_params() -> dict:\n",
                "    \"\"\"\n",
                "    Randomizes environment parameters to simulate domain variation.\n",
                "    \"\"\"\n",
                "    params = {\n",
                "        \"gravity\": np.random.uniform(8.5, 10.5),\n",
                "        \"friction\": np.random.uniform(0.5, 1.5),\n",
                "        \"motor_delay\": np.random.normal(0.1, 0.02)\n",
                "    }\n",
                "    return params\n",
                "\n",
                "def visualize_params_distribution(samples: int = 1000):\n",
                "    gravities = [randomize_env_params()[\"gravity\"] for _ in range(samples)]\n",
                "    plt.hist(gravities, bins=30, alpha=0.7)\n",
                "    plt.title(\"Gravity Distribution for Domain Randomization\")\n",
                "    plt.xlabel(\"Gravity (m/s^2)\")\n",
                "    plt.ylabel(\"Frequency\")\n",
                "    plt.grid(True)\n",
                "    plt.show()\n",
                "\n",
                "# Run the visualization\n",
                "visualize_params_distribution()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "Let me know if you'd like the final section ‚Äî **Continuous Control in Robotic Arms** ‚Äî next!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's wrap up the **RL for Robotics** trio with one of the most dynamic and technically rich topics: **Continuous Control in Robotic Arms**. Think of this as the art of **fluid, real-time movement**, like training a robot to paint, pour a drink, or perform surgery with precision.\n",
                "\n",
                "As always, we‚Äôll use the **Feynman technique**, keep it jargon-free (Andrew Ng-style), and go deep into theory with just enough practice to make it stick.\n",
                "\n",
                "---\n",
                "\n",
                "## üß† **1. Conceptual Foundation**\n",
                "\n",
                "### üìå **Purpose & Relevance**\n",
                "\n",
                "Discrete action spaces are like playing chess ‚Äî a finite set of predefined moves. But robotic arms don‚Äôt work in steps. They move **smoothly**, in real-valued spaces: angles, torques, joint positions.\n",
                "\n",
                "üí° **Continuous control** is how RL handles decisions when actions are *not buttons*, but *sliders*. It's the difference between saying ‚Äúgo left‚Äù vs. ‚Äúrotate your shoulder joint by 15.2¬∞ and flex your elbow by 0.9Nm‚Äù.\n",
                "\n",
                "**Why it matters**:  \n",
                "Whether it's picking strawberries üçì or assembling satellites üõ∞Ô∏è, precise continuous movements are essential for real-world dexterous manipulation.\n",
                "\n",
                "---\n",
                "\n",
                "### üîë **Key Terminology**\n",
                "\n",
                "| Term               | Metaphor / Analogy |\n",
                "|--------------------|--------------------|\n",
                "| **Action Space**   | Like a joystick instead of a keyboard ‚Äî infinite possible directions. |\n",
                "| **Policy Gradient**| Climbing a hill blindfolded ‚Äî take steps in the direction that increases reward. |\n",
                "| **Actor-Critic**   | One part acts (actor), another judges (critic) ‚Äî like a gymnast and their coach. |\n",
                "| **Torque Control** | Sending \"push harder\" or \"ease off\" signals to each joint motor. |\n",
                "| **Joint Space**    | The coordinate system of the robot‚Äôs limbs ‚Äî like your shoulder, elbow, wrist working together. |\n",
                "\n",
                "---\n",
                "\n",
                "### üìà **Use Cases**\n",
                "\n",
                "- **Precision manufacturing**: Welding, painting, part assembly.\n",
                "- **Medical robotics**: Steady, controlled surgical maneuvers.\n",
                "- **Agricultural robotics**: Grasping and sorting delicate produce.\n",
                "\n",
                "---\n",
                "\n",
                "## üßÆ **2. Mathematical Deep Dive**\n",
                "\n",
                "### üìê **Core Equations**\n",
                "\n",
                "We use **policy gradient methods** to learn in continuous spaces.\n",
                "\n",
                "The goal is to **maximize expected return**:\n",
                "\n",
                "$$\n",
                "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} r_t \\right]\n",
                "$$\n",
                "\n",
                "**Gradient ascent update rule**:\n",
                "\n",
                "$$\n",
                "\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
                "$$\n",
                "\n",
                "Where:\n",
                "\n",
                "- \\( \\theta \\) are the policy parameters (e.g., weights of a neural net),\n",
                "- \\( \\alpha \\) is the learning rate,\n",
                "- \\( \\nabla_\\theta J(\\theta) \\) is the policy gradient.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è **Math Intuition**\n",
                "\n",
                "In discrete environments, you choose from a menu. In continuous control, you're tuning dials with infinite precision.\n",
                "\n",
                "The **policy** outputs a **distribution** over actions ‚Äî for instance, a mean and standard deviation for a Gaussian. The agent then samples from this distribution to act.\n",
                "\n",
                "Like an archer adjusting angle and tension before releasing an arrow üèπ.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è **Assumptions & Constraints**\n",
                "\n",
                "- Requires **differentiable policies** (usually neural nets).\n",
                "- Often assumes smooth dynamics (no sudden jumps in action space).\n",
                "- Can be unstable without proper exploration noise or entropy regulation.\n",
                "\n",
                "---\n",
                "\n",
                "## üîç **3. Critical Analysis**\n",
                "\n",
                "### ‚öñÔ∏è **Strengths vs Weaknesses**\n",
                "\n",
                "| Strengths                        | Weaknesses                          |\n",
                "|----------------------------------|-------------------------------------|\n",
                "| Handles high-precision tasks     | Sample inefficient ‚Äî needs lots of data |\n",
                "| Works in real-world robotics     | Sensitive to initialization and noise |\n",
                "| Natural fit for joint-space tasks| Training instability (exploding gradients) |\n",
                "\n",
                "---\n",
                "\n",
                "### üß≠ **Ethical Lens**\n",
                "- **Safety**: Real-time control must prevent self-damage or human injury.\n",
                "- **Bias**: Training in unrealistic sim environments could lead to real-world misuse.\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ **Research Highlights (Post-2020)**\n",
                "\n",
                "- **Soft Actor-Critic (SAC)**: Adds entropy regularization for smoother exploration (2019‚Äì2022).\n",
                "- **TD3 (Twin Delayed DDPG)**: Combats overestimation in critic networks.\n",
                "- **Diffusion policies (2023)**: Uses generative models to sample smooth trajectories.\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ **4. Interactive Elements**\n",
                "\n",
                "### ‚úÖ **Concept Check**\n",
                "\n",
                "**Q: Why are discrete actions insufficient for robotic arm control?**\n",
                "\n",
                "A. They're easier to implement  \n",
                "B. They can't represent real joint movement smoothly  \n",
                "C. They lead to overfitting  \n",
                "D. They increase sample efficiency  \n",
                "\n",
                "‚úÖ **Answer**: **B** ‚Äì Robotic arms require fine-grained torque/angle control, which discrete actions can't represent.\n",
                "\n",
                "---\n",
                "\n",
                "### üß© **Code Challenge**\n",
                "\n",
                "```python\n",
                "# BAD: Discrete angle control (choppy movement)\n",
                "def control_joint(joint_angle: float):\n",
                "    if joint_angle < 30:\n",
                "        return 10\n",
                "    else:\n",
                "        return 20\n",
                "```\n",
                "\n",
                "üõ†Ô∏è **Fix with smooth output:**\n",
                "\n",
                "```python\n",
                "# GOOD: Continuous torque output\n",
                "def control_joint_smooth(state: np.ndarray) -> float:\n",
                "    # Neural net or policy outputs torque in range\n",
                "    return np.clip(np.dot(W, state), -1.0, 1.0)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## üìö **5. Glossary**\n",
                "\n",
                "| Term            | Explanation |\n",
                "|------------------|-------------|\n",
                "| **Action Space** | The set of all possible actions. |\n",
                "| **Actor-Critic** | Two-network approach: one acts, one evaluates. |\n",
                "| **Torque**       | The force that rotates a joint. |\n",
                "| **Joint Space**  | The internal coordinate system of robot limbs. |\n",
                "| **Entropy**      | A measure of randomness; encourages exploration. |\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è **6. Practical Considerations**\n",
                "\n",
                "### üß™ **Hyperparameters**\n",
                "\n",
                "- **Learning rate** \\( \\alpha \\): Usually \\( 1e^{-3} \\) to \\( 1e^{-5} \\)\n",
                "- **Noise scale** for exploration: 0.1‚Äì0.3 for action perturbation\n",
                "- **Batch size**: 64‚Äì256\n",
                "\n",
                "---\n",
                "\n",
                "### üìè **Evaluation Metrics**\n",
                "\n",
                "```python\n",
                "# Mean reward over 100 episodes\n",
                "avg_reward = np.mean([evaluate_policy(policy) for _ in range(100)])\n",
                "```\n",
                "\n",
                "- **Trajectory smoothness**\n",
                "- **Joint limit violations**\n",
                "- **Average torque applied**\n",
                "\n",
                "---\n",
                "\n",
                "### üöÄ **Production Tips**\n",
                "\n",
                "- Use **action clipping** to avoid damaging hardware.\n",
                "- Train with **realistic delay and noise models**.\n",
                "- Watch for **oscillation or jerky movements** ‚Äî may need smoother loss function.\n",
                "\n",
                "---\n",
                "\n",
                "## üêç **7. Full Python Code Walkthrough**\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def sample_action(state: np.ndarray, weights: np.ndarray, noise_scale: float = 0.1) -> float:\n",
                "    \"\"\"\n",
                "    Policy function: outputs continuous action (torque) with exploration noise.\n",
                "    \"\"\"\n",
                "    mean_action = np.dot(weights, state)\n",
                "    action = mean_action + np.random.normal(0, noise_scale)\n",
                "    return np.clip(action, -1.0, 1.0)\n",
                "\n",
                "def simulate_trajectory(weights: np.ndarray, steps: int = 50):\n",
                "    \"\"\"\n",
                "    Simulates a trajectory based on a learned linear policy.\n",
                "    \"\"\"\n",
                "    state = np.random.randn(3)\n",
                "    trajectory = []\n",
                "\n",
                "    for _ in range(steps):\n",
                "        action = sample_action(state, weights)\n",
                "        state = state + 0.1 * action  # simulate effect\n",
                "        trajectory.append(state.copy())\n",
                "\n",
                "    return np.array(trajectory)\n",
                "\n",
                "def visualize_trajectory(traj: np.ndarray):\n",
                "    plt.plot(traj[:, 0], label='Joint 1')\n",
                "    plt.plot(traj[:, 1], label='Joint 2')\n",
                "    plt.plot(traj[:, 2], label='Joint 3')\n",
                "    plt.title(\"Joint States Over Time\")\n",
                "    plt.xlabel(\"Timestep\")\n",
                "    plt.ylabel(\"Joint Position\")\n",
                "    plt.legend()\n",
                "    plt.grid(True)\n",
                "    plt.show()\n",
                "\n",
                "# Example usage\n",
                "weights = np.random.randn(3)\n",
                "traj = simulate_trajectory(weights)\n",
                "visualize_trajectory(traj)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "Let me know if you want a bonus \"cheat sheet\" PDF combining all 3 robotics RL topics into one tidy summary. Or we can go deeper into a specific algo like **SAC or PPO** next! ü¶æüî•"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Awesome ‚Äî time to hit the road! üöóüí°  \n",
                "Let‚Äôs explore how **Reinforcement Learning is used for decision-making in self-driving cars**, continuing with our structured and beginner-friendly Feynman-style breakdown.\n",
                "\n",
                "---\n",
                "\n",
                "## üß† **1. Conceptual Foundation**\n",
                "\n",
                "### üìå **Purpose & Relevance**\n",
                "\n",
                "Imagine you're teaching a car to drive itself. Not just *how* to steer or brake, but *when* to change lanes, *how* to merge onto a highway, or *whether* to wait or go at an unprotected left turn.\n",
                "\n",
                "This isn't basic control ‚Äî it's **high-level decision-making**.  \n",
                "**RL helps the car learn by doing**, adjusting its behavior based on whether it gets closer to safe, efficient driving ‚Äî or a crash. üöß\n",
                "\n",
                "> **Analogy**: Think of RL as teaching a teenager to drive in a simulator. The teen tries things out, makes mistakes, and eventually figures out what works ‚Äî guided by rewards (e.g., arriving safely) and penalties (e.g., collisions, honks, panic).  \n",
                "\n",
                "---\n",
                "\n",
                "### üîë **Key Terminology**\n",
                "\n",
                "| Term                     | Metaphor / Analogy |\n",
                "|--------------------------|--------------------|\n",
                "| **State**                | A snapshot of the car‚Äôs world ‚Äî like a dashboard reading + GPS map. |\n",
                "| **Action**               | The car‚Äôs options ‚Äî e.g., accelerate, change lanes, brake. |\n",
                "| **Policy**               | The car‚Äôs rulebook ‚Äî ‚Äúwhat to do in each situation.‚Äù |\n",
                "| **Reward Function**      | A scorecard ‚Äî adds points for safe driving, subtracts for danger. |\n",
                "| **Observation Space**    | What the car sees ‚Äî like looking out the windshield with sensors. |\n",
                "\n",
                "---\n",
                "\n",
                "### üìà **Use Cases**\n",
                "\n",
                "- **Highway merging and lane changes**\n",
                "- **Unprotected intersections**\n",
                "- **Overtaking in traffic**\n",
                "- **Adaptive cruise control**\n",
                "- **Navigating complex city traffic (e.g., left turns with oncoming cars)**\n",
                "\n",
                "---\n",
                "\n",
                "## üßÆ **2. Mathematical Deep Dive**\n",
                "\n",
                "### üìê **Core Equations**\n",
                "\n",
                "An RL agent learns to maximize the expected reward over time:\n",
                "\n",
                "$$\n",
                "J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\gamma^t r_t \\right]\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( \\pi_\\theta \\) is the policy parameterized by \\( \\theta \\)\n",
                "- \\( r_t \\) is the reward at time \\( t \\)\n",
                "- \\( \\gamma \\in [0,1] \\) is the discount factor\n",
                "\n",
                "**State transition dynamics** (Markov Decision Process):\n",
                "$$\n",
                "P(s_{t+1} | s_t, a_t)\n",
                "$$\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è **Math Intuition**\n",
                "\n",
                "- **Policy** is like a decision function: given inputs (observations), what‚Äôs the best move?\n",
                "- **Reward** drives behavior: if you reward the car for minimizing jerkiness, it learns to drive smooth.\n",
                "- **Discount factor** (Œ≥) tunes how much we care about long-term safety vs. short-term speed.\n",
                "\n",
                "> Visualize this as a **decision tree** constantly growing as new traffic patterns emerge. The RL agent learns which branches lead to good driving and which to avoid.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è **Assumptions & Constraints**\n",
                "\n",
                "- **Partial observability**: Sensors don‚Äôt see everything (e.g., occluded pedestrians).\n",
                "- **Real-time response**: Must make split-second decisions.\n",
                "- **Safety constraints**: Exploration can‚Äôt lead to dangerous behavior.\n",
                "\n",
                "---\n",
                "\n",
                "## üîç **3. Critical Analysis**\n",
                "\n",
                "### ‚öñÔ∏è **Strengths vs Weaknesses**\n",
                "\n",
                "| Strengths                              | Weaknesses                              |\n",
                "|----------------------------------------|------------------------------------------|\n",
                "| Can learn complex, emergent behavior   | Requires lots of simulation experience   |\n",
                "| Works well with sensor-rich input      | Not interpretable ‚Äî ‚Äúblack box‚Äù behavior |\n",
                "| Learns from reward signals, not labels | Risky in safety-critical tasks           |\n",
                "\n",
                "---\n",
                "\n",
                "### üß≠ **Ethical Lens**\n",
                "\n",
                "- **Exploration vs. safety**: Cars can‚Äôt ‚Äútry crashing‚Äù to learn.\n",
                "- **Bias in simulation**: If sim doesn‚Äôt represent diverse drivers or pedestrians, real-world generalization suffers.\n",
                "- **Accountability**: Who‚Äôs responsible for learned behavior gone wrong?\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ **Research Highlights (Post-2020)**\n",
                "\n",
                "- **Wayve** and **Tesla Dojo**: RL for large-scale decision-making from raw camera inputs.\n",
                "- **Deep Q-Networks (DQN) for lane selection**: Discrete decisions like left/middle/right.\n",
                "- **Imitation + RL hybrid**: First learn from human driving, then fine-tune via RL.\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ **4. Interactive Elements**\n",
                "\n",
                "### ‚úÖ **Concept Check**\n",
                "\n",
                "**Q: Why might pure RL struggle in real-world autonomous driving?**\n",
                "\n",
                "A. It doesn't scale  \n",
                "B. It requires labeled data  \n",
                "C. Exploration can lead to unsafe actions  \n",
                "D. It‚Äôs easy to implement\n",
                "\n",
                "‚úÖ **Answer**: **C** ‚Äì Unsafe exploration is a core challenge. We often need safe RL or hybrid methods to reduce real-world risks.\n",
                "\n",
                "---\n",
                "\n",
                "### üß© **Code Challenge: Discrete vs. Continuous Action in Lane Changes**\n",
                "\n",
                "```python\n",
                "# BAD: Discrete action control, inflexible\n",
                "def lane_decision(obs):\n",
                "    if obs[\"car_left\"]:\n",
                "        return \"stay\"\n",
                "    else:\n",
                "        return \"left\"\n",
                "\n",
                "# IMPROVED: RL-style decision\n",
                "def rl_policy(obs, weights):\n",
                "    state_vector = np.array([\n",
                "        obs[\"speed\"], obs[\"distance_front\"], obs[\"distance_left\"]\n",
                "    ])\n",
                "    action_value = np.dot(weights, state_vector)\n",
                "    return np.clip(action_value, -1.0, 1.0)  # Continuous lateral movement\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## üìö **5. Glossary**\n",
                "\n",
                "| Term             | Explanation |\n",
                "|------------------|-------------|\n",
                "| **State**        | What the car ‚Äúknows‚Äù ‚Äî sensors, maps, GPS, etc. |\n",
                "| **Action**       | What the car ‚Äúcan do‚Äù ‚Äî change lane, speed up, stop. |\n",
                "| **Reward**       | Signal for success ‚Äî safe driving = good, crashes = bad. |\n",
                "| **Policy**       | The model that turns knowledge into action. |\n",
                "| **Markov Process**| A world where only the present matters ‚Äî not the past. |\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è **6. Practical Considerations**\n",
                "\n",
                "### üß™ **Hyperparameters**\n",
                "\n",
                "- **Œ≥ (discount)**: 0.95 for near-future focus\n",
                "- **Exploration strategy**: Œµ-greedy or entropy-regularized\n",
                "- **Action sampling rate**: Must match real-world latency (~10Hz)\n",
                "\n",
                "---\n",
                "\n",
                "### üìè **Evaluation Metrics**\n",
                "\n",
                "```python\n",
                "# Reward = safety + efficiency + comfort\n",
                "def reward_function(safety: float, efficiency: float, comfort: float) -> float:\n",
                "    return 0.6 * safety + 0.3 * efficiency + 0.1 * comfort\n",
                "```\n",
                "\n",
                "- **Collision rate**\n",
                "- **Lane violation rate**\n",
                "- **Time to complete route**\n",
                "- **Smoothness (jerk / acceleration)**\n",
                "\n",
                "---\n",
                "\n",
                "### üöÄ **Production Tips**\n",
                "\n",
                "- Use **simulation-to-real** with domain adaptation (CARLA, LGSVL).\n",
                "- Incorporate **human-in-the-loop** evaluations.\n",
                "- Blend **rule-based failsafes** with RL policies for edge-case handling.\n",
                "\n",
                "---\n",
                "\n",
                "## üêç **7. Full Python Code Walkthrough**\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def rl_lane_policy(state: np.ndarray, weights: np.ndarray) -> float:\n",
                "    \"\"\"\n",
                "    Returns a continuous lane-change decision value.\n",
                "    Positive = move right, Negative = move left, 0 = stay.\n",
                "    \"\"\"\n",
                "    action = np.dot(weights, state)\n",
                "    return np.clip(action, -1.0, 1.0)\n",
                "\n",
                "def simulate_lane_policy(weights: np.ndarray, steps: int = 50):\n",
                "    \"\"\"\n",
                "    Simulates policy-based lateral movement decisions.\n",
                "    \"\"\"\n",
                "    trajectory = []\n",
                "    state = np.array([20.0, 10.0, 5.0])  # speed, distance_front, distance_left\n",
                "\n",
                "    for _ in range(steps):\n",
                "        action = rl_lane_policy(state, weights)\n",
                "        state[2] += 0.1 * action  # update lateral position\n",
                "        trajectory.append(state[2])\n",
                "\n",
                "    return trajectory\n",
                "\n",
                "def visualize_lane_trajectory(traj: list):\n",
                "    plt.plot(traj)\n",
                "    plt.title(\"Lane Position Over Time\")\n",
                "    plt.xlabel(\"Time Step\")\n",
                "    plt.ylabel(\"Lateral Position\")\n",
                "    plt.grid(True)\n",
                "    plt.show()\n",
                "\n",
                "# Example usage\n",
                "weights = np.array([0.1, -0.2, 0.5])\n",
                "trajectory = simulate_lane_policy(weights)\n",
                "visualize_lane_trajectory(trajectory)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "Let me know if you want a breakdown of **Traffic Simulation for RL**, or we can dive into **Real-time Planning and Adaptive Navigation** next! üõ£Ô∏èüß†"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Perfect ‚Äî now let's zoom out a bit and build **the world that trains the car**:  \n",
                "Welcome to **Traffic Simulation for Training RL Models** ‚Äî where the road isn't just a backdrop, it's an active, dynamic environment filled with other agents, noise, and chaos. üèôÔ∏èüõª\n",
                "\n",
                "This is the *sandbox* where RL policies for autonomous vehicles are forged.\n",
                "\n",
                "---\n",
                "\n",
                "## üß† **1. Conceptual Foundation**\n",
                "\n",
                "### üìå **Purpose & Relevance**\n",
                "\n",
                "Imagine teaching a self-driving car without actually putting it on the road. That‚Äôs like training a pilot using only Microsoft Flight Simulator ‚Äî safe, repeatable, and scalable.\n",
                "\n",
                "**Traffic simulation environments** let us train RL agents:\n",
                "- Without causing crashes üö´\n",
                "- With millions of scenarios in minutes üïí\n",
                "- In *realistic, high-dimensional worlds*\n",
                "\n",
                "Think of it like **video games for robots** ‚Äî only these games teach them how to drive safely in the real world.\n",
                "\n",
                "---\n",
                "\n",
                "### üîë **Key Terminology**\n",
                "\n",
                "| Term                   | Analogy / Metaphor |\n",
                "|------------------------|--------------------|\n",
                "| **Simulation Engine**  | The \"game engine\" of traffic ‚Äî like Unity or Unreal for driving logic. |\n",
                "| **Multi-Agent System** | Each vehicle is its own \"player\" ‚Äî decisions depend on everyone else. |\n",
                "| **Scenario Generator** | A traffic story builder ‚Äî can create rush hour, accidents, or rain. |\n",
                "| **Agent-Based Modeling**| Each car acts independently, like humans in a crowd simulation. |\n",
                "| **Reward Shaping**     | Teaching with incentives ‚Äî like adding coins in Mario to guide the player. |\n",
                "\n",
                "---\n",
                "\n",
                "### üìà **Use Cases**\n",
                "\n",
                "- Training **RL-based lane-merging**, ramp entry, or overtaking policies.\n",
                "- Simulating **edge-case scenarios** like jaywalking pedestrians.\n",
                "- Generating **synthetic data** for rare events (e.g. emergency braking).\n",
                "- Benchmarking policies across different environments and traffic densities.\n",
                "\n",
                "---\n",
                "\n",
                "## üßÆ **2. Mathematical Deep Dive**\n",
                "\n",
                "### üìê **Core Equations**\n",
                "\n",
                "Let‚Äôs define a multi-agent MDP:\n",
                "\n",
                "Each agent \\( i \\) has:\n",
                "- **State**: \\( s^i \\in S \\)\n",
                "- **Action**: \\( a^i \\in A \\)\n",
                "- **Reward**: \\( r^i_t = R(s^i_t, a^i_t, s^{-i}_t) \\)\n",
                "\n",
                "Joint state dynamics:\n",
                "\n",
                "$$\n",
                "P(s_{t+1} \\mid s_t, a^1_t, a^2_t, \\dots, a^N_t)\n",
                "$$\n",
                "\n",
                "**Goal**: Train a policy \\( \\pi^i(a^i | s^i) \\) that maximizes cumulative reward:\n",
                "\n",
                "$$\n",
                "J^i(\\theta) = \\mathbb{E} \\left[ \\sum_t \\gamma^t r^i_t \\right]\n",
                "$$\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è **Math Intuition**\n",
                "\n",
                "This is like playing a cooperative-competitive board game. Your success depends not just on what you do, but how others behave too.\n",
                "\n",
                "You train an RL agent in **a sea of other agents** ‚Äî some scripted, some learned, all unpredictable.\n",
                "\n",
                "> ‚ÄúYou‚Äôre not the only car on the road ‚Äî everyone else is trying to survive, too.‚Äù\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è **Assumptions & Constraints**\n",
                "\n",
                "- **Realism vs. Speed tradeoff**: High-fidelity sims are slower.\n",
                "- **Unrealistic behaviors** from scripted cars can teach bad habits.\n",
                "- **Coordination**: Multiple agents require synchronized time steps.\n",
                "\n",
                "---\n",
                "\n",
                "## üîç **3. Critical Analysis**\n",
                "\n",
                "### ‚öñÔ∏è **Strengths vs Weaknesses**\n",
                "\n",
                "| Strengths                                | Weaknesses                                 |\n",
                "|------------------------------------------|---------------------------------------------|\n",
                "| Safe, large-scale scenario generation    | Reality gap: behavior may not transfer      |\n",
                "| Supports multi-agent interaction         | Computationally expensive                   |\n",
                "| Enables rare-event training              | Simulator drift if physics isn't tuned well |\n",
                "\n",
                "---\n",
                "\n",
                "### üß≠ **Ethical Lens**\n",
                "\n",
                "- **Bias in scenario design**: If the simulator always assumes perfect pedestrians, the car might not learn to yield to reckless ones.\n",
                "- **Representation**: Realistic driving behaviors must account for human variability (e.g., aggressive drivers, slow responders).\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ **Research Highlights (Post-2020)**\n",
                "\n",
                "- **CARLA + RL**: Widely used open-source simulator with realistic physics and pedestrian modeling.\n",
                "- **Flow + SUMO**: Deep integration of RL and traffic microsim for multi-agent learning.\n",
                "- **MetaDrive, BeamNG-AI**: Recent lightweight, realistic environments for high-speed sim.\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ **4. Interactive Elements**\n",
                "\n",
                "### ‚úÖ **Concept Check**\n",
                "\n",
                "**Q: Why is traffic simulation critical for RL in self-driving?**\n",
                "\n",
                "A. RL doesn‚Äôt need data  \n",
                "B. Real-world testing is too expensive and dangerous  \n",
                "C. It‚Äôs easier to get labeled data  \n",
                "D. Cars don‚Äôt use reinforcement learning\n",
                "\n",
                "‚úÖ **Answer**: **B** ‚Äì You can train RL agents on millions of edge cases safely in simulation, which is impossible in the real world.\n",
                "\n",
                "---\n",
                "\n",
                "### üß© **Code Challenge**\n",
                "\n",
                "```python\n",
                "# Bad: Fixed scenario, no variability\n",
                "def create_scenario():\n",
                "    return {\"cars\": 5, \"pedestrians\": 2, \"weather\": \"sunny\"}\n",
                "\n",
                "# Better: Randomized simulation\n",
                "def generate_random_traffic(seed=None):\n",
                "    np.random.seed(seed)\n",
                "    return {\n",
                "        \"cars\": np.random.randint(1, 20),\n",
                "        \"pedestrians\": np.random.randint(0, 5),\n",
                "        \"weather\": np.random.choice([\"sunny\", \"rainy\", \"foggy\"])\n",
                "    }\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## üìö **5. Glossary**\n",
                "\n",
                "| Term               | Explanation |\n",
                "|--------------------|-------------|\n",
                "| **Simulation Engine** | The software that simulates physics and agent behaviors. |\n",
                "| **Scenario Generator** | A tool to create randomized traffic situations. |\n",
                "| **Agent-Based Model** | Each vehicle or person is its own decision-maker. |\n",
                "| **Traffic Microsimulation** | Models each car‚Äôs path in detail (vs. averages). |\n",
                "| **Multi-Agent RL** | Learning policies where many agents interact in one environment. |\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è **6. Practical Considerations**\n",
                "\n",
                "### üß™ **Hyperparameters**\n",
                "\n",
                "- **Agent count**: 5‚Äì50 cars for training robustness\n",
                "- **Step granularity**: 0.1s or 0.2s time steps\n",
                "- **Random seed**: Useful for reproducibility\n",
                "\n",
                "---\n",
                "\n",
                "### üìè **Evaluation Metrics**\n",
                "\n",
                "```python\n",
                "# Percentage of successful merges without collisions\n",
                "def evaluate_merge_success(env_results):\n",
                "    return np.mean([1 if r[\"collision\"] == False else 0 for r in env_results])\n",
                "```\n",
                "\n",
                "- **Collision rate**\n",
                "- **Traffic throughput**\n",
                "- **Scenario coverage (diversity)**\n",
                "- **Latency per timestep**\n",
                "\n",
                "---\n",
                "\n",
                "### üöÄ **Production Tips**\n",
                "\n",
                "- Use **realistic physics** (e.g., tire friction, sensor delay).\n",
                "- Tune **non-agent vehicles** to mimic real driving styles.\n",
                "- Mix **scripted + learned agents** to create a hybrid, adaptive traffic world.\n",
                "\n",
                "---\n",
                "\n",
                "## üêç **7. Full Python Code Walkthrough**\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def generate_scenario(seed: int = None) -> dict:\n",
                "    \"\"\"\n",
                "    Random traffic scenario for simulation.\n",
                "    \"\"\"\n",
                "    np.random.seed(seed)\n",
                "    return {\n",
                "        \"cars\": np.random.randint(10, 30),\n",
                "        \"pedestrians\": np.random.randint(0, 8),\n",
                "        \"weather\": np.random.choice([\"sunny\", \"rainy\", \"foggy\"]),\n",
                "        \"merge_difficulty\": np.random.choice([\"easy\", \"medium\", \"hard\"])\n",
                "    }\n",
                "\n",
                "def visualize_scenario_distribution(samples: int = 1000):\n",
                "    car_counts = [generate_scenario()[\"cars\"] for _ in range(samples)]\n",
                "    plt.hist(car_counts, bins=20)\n",
                "    plt.title(\"Distribution of Car Counts in Simulation\")\n",
                "    plt.xlabel(\"Cars per Scenario\")\n",
                "    plt.ylabel(\"Frequency\")\n",
                "    plt.grid(True)\n",
                "    plt.show()\n",
                "\n",
                "# Run it\n",
                "visualize_scenario_distribution()\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "Up next, want to explore **Real-Time Planning and Adaptive Navigation** or deep-dive into **CARLA or MetaDrive usage for RL?**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Awesome ‚Äî now let‚Äôs bring it all together with the brains of autonomous driving:  \n",
                "**Real-Time Planning and Adaptive Navigation** ‚Äî the system that figures out *how to get from A to B* while constantly dodging chaos on the road. üß†üõ£Ô∏è\n",
                "\n",
                "This is where high-level strategy meets low-level execution ‚Äî and RL becomes *thinking on its feet*.\n",
                "\n",
                "---\n",
                "\n",
                "## üß† **1. Conceptual Foundation**\n",
                "\n",
                "### üìå **Purpose & Relevance**\n",
                "\n",
                "Real-time planning is the art of **deciding what to do next, instantly**, based on current sensor data and traffic situations. Adaptive navigation takes it a step further ‚Äî it‚Äôs the ability to **change the plan on the fly** if something unexpected happens (e.g., road closed, pedestrian jumps out, aggressive driver cuts in).\n",
                "\n",
                "> üöóüí® Think of it like Google Maps rerouting you *while you‚Äôre driving* ‚Äî but for a car with no driver.\n",
                "\n",
                "**Reinforcement Learning (RL)** fits beautifully here:\n",
                "- It learns when to replan\n",
                "- It adapts to dynamic changes\n",
                "- It balances exploration (finding better paths) and exploitation (sticking to what works)\n",
                "\n",
                "---\n",
                "\n",
                "### üîë **Key Terminology**\n",
                "\n",
                "| Term                  | Analogy / Metaphor |\n",
                "|-----------------------|--------------------|\n",
                "| **Receding Horizon Planning** | Like looking 10 meters ahead and updating every step ‚Äî rolling GPS. |\n",
                "| **Dynamic Obstacles** | Other drivers or pedestrians that move unpredictably ‚Äî like dodging kids in a park. |\n",
                "| **Local vs Global Planner** | GPS plans the route (global), your foot on the pedal handles turns (local). |\n",
                "| **Cost Map**          | A heatmap of ‚Äúhow bad‚Äù it is to be in a certain area (e.g., near other cars). |\n",
                "| **Trajectory Optimization** | Picking the smoothest, safest route through time ‚Äî like dancing through a moving crowd. |\n",
                "\n",
                "---\n",
                "\n",
                "### üìà **Use Cases**\n",
                "\n",
                "- Lane changing in heavy traffic\n",
                "- Emergency rerouting around obstacles\n",
                "- Merging at dynamic junctions\n",
                "- City grid navigation with stoplights, pedestrians, and erratic drivers\n",
                "\n",
                "---\n",
                "\n",
                "## üßÆ **2. Mathematical Deep Dive**\n",
                "\n",
                "### üìê **Core Equations**\n",
                "\n",
                "Let‚Äôs model it as a **Partially Observable MDP (POMDP)** due to real-world uncertainty.\n",
                "\n",
                "Agent maximizes expected return:\n",
                "\n",
                "$$\n",
                "J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\gamma^t r_t \\right]\n",
                "$$\n",
                "\n",
                "Real-time planning uses **Model Predictive Control (MPC)** style planning:\n",
                "\n",
                "$$\n",
                "\\text{argmin}_u \\sum_{t=0}^{H} \\text{Cost}(s_t, u_t)\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( H \\): planning horizon\n",
                "- \\( u_t \\): control inputs (e.g., steering, acceleration)\n",
                "- Cost includes proximity to other cars, lane deviation, time, etc.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è **Math Intuition**\n",
                "\n",
                "You're constantly optimizing a *short horizon plan* ‚Äî imagine laying a strip of dominoes, and each step you pick the next best tile to lay down while others are still falling.\n",
                "\n",
                "RL teaches the agent how to **prioritize goals** (e.g., safety > speed), and to **change plans fluidly** when conditions change.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è **Assumptions & Constraints**\n",
                "\n",
                "- **Partial observability**: Sensors might not see everything.\n",
                "- **Latency**: Decision-making must be fast (~50-100ms).\n",
                "- **Uncertainty**: Pedestrians and other cars may behave unpredictably.\n",
                "\n",
                "---\n",
                "\n",
                "## üîç **3. Critical Analysis**\n",
                "\n",
                "### ‚öñÔ∏è **Strengths vs Weaknesses**\n",
                "\n",
                "| Strengths                              | Weaknesses                                |\n",
                "|----------------------------------------|--------------------------------------------|\n",
                "| Learns to plan under uncertainty       | Needs real-time computation guarantees     |\n",
                "| Adapts to dynamic environments         | Reward design is hard in complex scenarios |\n",
                "| Combines well with perception systems  | Can overfit to simulator quirks            |\n",
                "\n",
                "---\n",
                "\n",
                "### üß≠ **Ethical Lens**\n",
                "\n",
                "- **Decision under pressure**: Who decides in life-death dilemmas?\n",
                "- **Bias in cost maps**: Does it value passenger comfort over pedestrian safety?\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ **Research Highlights (Post-2020)**\n",
                "\n",
                "- **Waymo + RL + MPC hybrids**: RL for high-level tactics, MPC for local planning.\n",
                "- **DeepRL + Motion Planning**: Joint training of global route and local dynamics.\n",
                "- **Diffuser (2022)**: Diffusion-based trajectory optimizers outperform traditional MPC in multi-modal futures.\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ **4. Interactive Elements**\n",
                "\n",
                "### ‚úÖ **Concept Check**\n",
                "\n",
                "**Q: What‚Äôs the main role of real-time planning in autonomous driving?**\n",
                "\n",
                "A. Scheduling repairs  \n",
                "B. Calculating total trip cost  \n",
                "C. Adapting decisions to immediate traffic situations  \n",
                "D. Checking tire pressure\n",
                "\n",
                "‚úÖ **Answer**: **C** ‚Äî It‚Äôs about dynamically adjusting actions based on current real-world input.\n",
                "\n",
                "---\n",
                "\n",
                "### üß© **Code Challenge: Cost-based Lane Planner**\n",
                "\n",
                "```python\n",
                "# BAD: Fixed rule, doesn‚Äôt adapt to traffic\n",
                "def fixed_lane_choice(traffic_density):\n",
                "    if traffic_density > 10:\n",
                "        return \"stay\"\n",
                "    return \"left\"\n",
                "\n",
                "# GOOD: RL-inspired adaptive planner using cost function\n",
                "def adaptive_lane_choice(state, weights):\n",
                "    features = np.array([\n",
                "        state[\"lane_density\"],\n",
                "        state[\"obstacle_distance\"],\n",
                "        state[\"goal_direction\"]\n",
                "    ])\n",
                "    cost = np.dot(weights, features)\n",
                "    return \"left\" if cost < 0 else \"stay\"\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## üìö **5. Glossary**\n",
                "\n",
                "| Term               | Explanation |\n",
                "|--------------------|-------------|\n",
                "| **MPC (Model Predictive Control)** | Short-term planning with real-time optimization. |\n",
                "| **Cost Map**       | Grid of how risky or costly each location is. |\n",
                "| **Trajectory**     | A sequence of actions and resulting states over time. |\n",
                "| **Adaptive Planner**| Changes plan dynamically based on environment. |\n",
                "| **Latency**        | Time between sensing and acting ‚Äî must be short in robotics. |\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è **6. Practical Considerations**\n",
                "\n",
                "### üß™ **Hyperparameters**\n",
                "\n",
                "- **Planning horizon (H)**: 5‚Äì10 steps is typical\n",
                "- **Cost function weights**: Tune for safety vs efficiency\n",
                "- **Replan frequency**: Every 100‚Äì200ms (real-time)\n",
                "\n",
                "---\n",
                "\n",
                "### üìè **Evaluation Metrics**\n",
                "\n",
                "```python\n",
                "# Time to reach goal, factoring safety\n",
                "def compute_score(time, collisions, comfort_penalty):\n",
                "    return -time - 50 * collisions - comfort_penalty\n",
                "```\n",
                "\n",
                "- **Success rate**\n",
                "- **Collision-free trajectories**\n",
                "- **Comfort (jerk/acceleration)**\n",
                "- **Time to replan after unexpected events**\n",
                "\n",
                "---\n",
                "\n",
                "### üöÄ **Production Tips**\n",
                "\n",
                "- Always run **planning loop asynchronously** to prevent lag.\n",
                "- Use **sensor fusion** to improve obstacle prediction accuracy.\n",
                "- Run **multiple plan branches** in parallel (parallel rollouts).\n",
                "\n",
                "---\n",
                "\n",
                "## üêç **7. Full Python Code Walkthrough**\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def plan_trajectory(state: np.ndarray, goal: float, weights: np.ndarray, horizon: int = 10):\n",
                "    \"\"\"\n",
                "    Simulate planning over a horizon with adaptive costs.\n",
                "    \"\"\"\n",
                "    pos = state.copy()\n",
                "    trajectory = []\n",
                "\n",
                "    for _ in range(horizon):\n",
                "        features = np.array([\n",
                "            pos[0] - goal,      # distance to goal\n",
                "            np.random.uniform(0, 5),  # dynamic cost (obstacles)\n",
                "            np.random.normal(0, 1)    # randomness/noise\n",
                "        ])\n",
                "        cost = np.dot(weights, features)\n",
                "        action = -np.sign(cost)\n",
                "        pos[0] += 0.1 * action  # move left or right\n",
                "        trajectory.append(pos[0])\n",
                "\n",
                "    return trajectory\n",
                "\n",
                "def plot_trajectory(trajectory):\n",
                "    plt.plot(trajectory)\n",
                "    plt.title(\"Planned Lateral Trajectory Over Horizon\")\n",
                "    plt.xlabel(\"Timestep\")\n",
                "    plt.ylabel(\"Lateral Position\")\n",
                "    plt.grid(True)\n",
                "    plt.show()\n",
                "\n",
                "# Example\n",
                "weights = np.array([1.0, 2.0, 0.5])\n",
                "initial_state = np.array([0.0])\n",
                "trajectory = plan_trajectory(initial_state, goal=2.0, weights=weights)\n",
                "plot_trajectory(trajectory)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "This closes the loop on **RL for Autonomous Vehicles**. Want a final compiled cheat sheet of all 4 sections? Or should we move into **Multi-Agent RL in traffic environments** next?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Absolutely! üéÆ Let‚Äôs shift from the real world to virtual battlegrounds and dive into:  \n",
                "**Reinforcement Learning for Game AI** ‚Äì where agents become chess grandmasters, Go prodigies, and first-person shooter champions.\n",
                "\n",
                "Using our structured, beginner-friendly, Feynman-style approach, we‚Äôll unpack how RL is revolutionizing gameplay from checkered boards to 3D battle arenas.\n",
                "\n",
                "---\n",
                "\n",
                "## üß† **1. Conceptual Foundation**\n",
                "\n",
                "### üìå **Purpose & Relevance**\n",
                "\n",
                "Games are like playgrounds for intelligence ‚Äî they have:\n",
                "- **Clear rules** (great for simulation)\n",
                "- **Well-defined rewards** (win/lose, score)\n",
                "- **Massive state spaces** (perfect challenge for learning)\n",
                "\n",
                "This makes them ideal for developing and testing RL agents.\n",
                "\n",
                "> Imagine teaching a student how to play chess **without explaining the rules** ‚Äî they learn entirely by playing and improving. That‚Äôs RL in games.\n",
                "\n",
                "**Why it matters:**  \n",
                "- Games are testbeds for cutting-edge AI research.  \n",
                "- Success in games often leads to breakthroughs in real-world applications (robotics, planning, finance, etc.).\n",
                "\n",
                "---\n",
                "\n",
                "### üîë **Key Terminology**\n",
                "\n",
                "| Term                | Analogy / Explanation |\n",
                "|---------------------|------------------------|\n",
                "| **State**           | Snapshot of the game board or screen at a given time. |\n",
                "| **Action**          | Move or decision made by the agent (e.g., move pawn, shoot enemy). |\n",
                "| **Policy**          | Agent‚Äôs strategy ‚Äî like a flowchart of decisions. |\n",
                "| **Reward**          | Score the agent gets ‚Äî win = +1, lose = -1, or more detailed scores. |\n",
                "| **Exploration vs. Exploitation** | Trying new moves vs. sticking to known good ones ‚Äî like testing new chess openings. |\n",
                "\n",
                "---\n",
                "\n",
                "### üìà **Use Cases**\n",
                "\n",
                "- **Chess & Go**: Strategic board games with deep decision trees.\n",
                "- **Atari games**: Pixel-based RL training grounds.\n",
                "- **MMOs and MOBAs (e.g. Dota, StarCraft)**: Complex, real-time, multi-agent environments.\n",
                "- **Open-world games**: Navigation, planning, adaptive behavior.\n",
                "\n",
                "---\n",
                "\n",
                "## üßÆ **2. Mathematical Deep Dive**\n",
                "\n",
                "### üìê **Core Equations**\n",
                "\n",
                "An agent's goal: **maximize total future reward**.\n",
                "\n",
                "$$\n",
                "J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\gamma^t r_t \\right]\n",
                "$$\n",
                "\n",
                "**Q-learning update** (common in games):\n",
                "\n",
                "$$\n",
                "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
                "$$\n",
                "\n",
                "- \\( Q(s, a) \\): value of taking action \\( a \\) in state \\( s \\)\n",
                "- \\( \\gamma \\): discount factor\n",
                "- \\( \\alpha \\): learning rate\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è **Math Intuition**\n",
                "\n",
                "Games are like giant decision trees. RL methods like **Q-learning** and **policy gradients** teach agents to:\n",
                "- **Estimate** how good each branch is\n",
                "- **Choose** actions that maximize long-term gain\n",
                "\n",
                "RL doesn‚Äôt need to be told what‚Äôs right or wrong ‚Äî it **discovers winning strategies through trial and error**.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è **Assumptions & Constraints**\n",
                "\n",
                "- **Markov assumption**: Only current state matters (not how we got there).\n",
                "- **Rewards must be meaningful**: Bad rewards ‚Üí weird strategies (reward hacking).\n",
                "- **High variance** in outcomes ‚Üí slow convergence.\n",
                "\n",
                "---\n",
                "\n",
                "## üîç **3. Critical Analysis**\n",
                "\n",
                "### ‚öñÔ∏è **Strengths vs Weaknesses**\n",
                "\n",
                "| Strengths                         | Weaknesses                              |\n",
                "|-----------------------------------|------------------------------------------|\n",
                "| Can master complex, strategic tasks | High sample complexity (slow training)   |\n",
                "| Requires no expert rules          | Difficult to debug and interpret         |\n",
                "| Learns emergent behavior          | Reward design is tricky                  |\n",
                "\n",
                "---\n",
                "\n",
                "### üß≠ **Ethical Lens**\n",
                "\n",
                "- **Bias in game design** can affect agent behavior.\n",
                "- **Learning aggression**: In violent games, agents may develop ultra-aggressive tactics unless regulated.\n",
                "- **Cheating/hacking detection**: RL agents in multiplayer settings must be designed with fairness in mind.\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ **Research Highlights (Post-2020)**\n",
                "\n",
                "- **MuZero (DeepMind)**: Learned to master chess, Go, Atari ‚Äî without knowing the rules upfront!\n",
                "- **OpenAI Five**: RL agents that learned teamwork and strategy in Dota 2.\n",
                "- **AlphaStar (StarCraft II)**: Advanced multi-agent RL with complex micro and macro planning.\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ **4. Interactive Elements**\n",
                "\n",
                "### ‚úÖ **Concept Check**\n",
                "\n",
                "**Q: Why is RL a good fit for training game agents?**\n",
                "\n",
                "A. It needs human labels  \n",
                "B. It‚Äôs deterministic  \n",
                "C. Games offer repeated, clear feedback and structured environments  \n",
                "D. It guarantees optimal play immediately\n",
                "\n",
                "‚úÖ **Answer**: **C** ‚Äî Games are closed systems with fast feedback loops, perfect for RL.\n",
                "\n",
                "---\n",
                "\n",
                "### üß© **Code Challenge: Q-learning for a Grid Game**\n",
                "\n",
                "```python\n",
                "# BAD: Hardcoded movement\n",
                "def move(position):\n",
                "    if position == (0, 0):\n",
                "        return \"right\"\n",
                "    return \"stay\"\n",
                "\n",
                "# BETTER: Q-learning update step\n",
                "def q_update(q_table, state, action, reward, next_state, alpha=0.1, gamma=0.99):\n",
                "    max_future_q = np.max(q_table[next_state])\n",
                "    current_q = q_table[state][action]\n",
                "    q_table[state][action] = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## üìö **5. Glossary**\n",
                "\n",
                "| Term             | Explanation |\n",
                "|------------------|-------------|\n",
                "| **State**        | The environment‚Äôs current status (game board, screen pixels). |\n",
                "| **Action**       | What the agent can do next. |\n",
                "| **Reward**       | A score signal to guide learning. |\n",
                "| **Q-value**      | Expected future reward for a given state-action pair. |\n",
                "| **Policy**       | The decision-making function or neural net. |\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è **6. Practical Considerations**\n",
                "\n",
                "### üß™ **Hyperparameters**\n",
                "\n",
                "- **Discount factor (Œ≥)**: 0.95‚Äì0.99 for games with delayed rewards (e.g., chess).\n",
                "- **Exploration strategy**: Œµ-greedy or entropy-regularized (for diverse play).\n",
                "- **Batch size & replay buffer**: Essential for stability in deep RL.\n",
                "\n",
                "---\n",
                "\n",
                "### üìè **Evaluation Metrics**\n",
                "\n",
                "```python\n",
                "# Win rate over evaluation games\n",
                "def evaluate(agent, opponents, games=100):\n",
                "    wins = sum(agent.play(opponent) == \"win\" for opponent in opponents)\n",
                "    return wins / games\n",
                "```\n",
                "\n",
                "- **Win/loss rate**\n",
                "- **Average score**\n",
                "- **Convergence speed**\n",
                "- **Behavior diversity**\n",
                "\n",
                "---\n",
                "\n",
                "### üöÄ **Production Tips**\n",
                "\n",
                "- Train with **self-play** (like AlphaZero) to generate diverse experience.\n",
                "- Use **curriculum learning** ‚Äî start easy, increase difficulty.\n",
                "- Monitor for **reward hacking** ‚Äî agents exploiting loopholes in scoring.\n",
                "\n",
                "---\n",
                "\n",
                "## üêç **7. Full Python Code Walkthrough**\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Gridworld Q-learning (toy example)\n",
                "class GridGame:\n",
                "    def __init__(self, size=5):\n",
                "        self.size = size\n",
                "        self.reset()\n",
                "\n",
                "    def reset(self):\n",
                "        self.agent_pos = [0, 0]\n",
                "        return tuple(self.agent_pos)\n",
                "\n",
                "    def step(self, action):\n",
                "        if action == 0: self.agent_pos[0] = min(self.size-1, self.agent_pos[0]+1)  # down\n",
                "        if action == 1: self.agent_pos[0] = max(0, self.agent_pos[0]-1)            # up\n",
                "        if action == 2: self.agent_pos[1] = min(self.size-1, self.agent_pos[1]+1)  # right\n",
                "        if action == 3: self.agent_pos[1] = max(0, self.agent_pos[1]-1)            # left\n",
                "\n",
                "        reward = 1.0 if self.agent_pos == [self.size-1, self.size-1] else -0.01\n",
                "        return tuple(self.agent_pos), reward, self.agent_pos == [self.size-1, self.size-1]\n",
                "\n",
                "# Q-table initialization and training loop\n",
                "q_table = np.zeros((5, 5, 4))  # state: (x, y), actions: 4\n",
                "env = GridGame()\n",
                "\n",
                "for episode in range(500):\n",
                "    state = env.reset()\n",
                "    done = False\n",
                "    while not done:\n",
                "        action = np.random.choice(4) if np.random.rand() < 0.2 else np.argmax(q_table[state])\n",
                "        next_state, reward, done = env.step(action)\n",
                "        q_update(q_table, state, action, reward, next_state)\n",
                "        state = next_state\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "Want to move on to **Multi-Agent RL and Competitive Games** next (e.g., Dota, Capture the Flag)? Or explore **AlphaZero/AlphaGo mechanics** in-depth?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let‚Äôs take RL to the battlefield now ‚Äî where it‚Äôs not just about learning *how to play*, but how to *outsmart opponents*. üéØ  \n",
                "Welcome to **Multi-Agent Systems and Competitive Reinforcement Learning** ‚Äî where things get chaotic, strategic, and incredibly interesting.\n",
                "\n",
                "This is the arena of **Dota 2, StarCraft, Capture the Flag**, and even **AI vs AI chess matches**.\n",
                "\n",
                "---\n",
                "\n",
                "## üß† **1. Conceptual Foundation**\n",
                "\n",
                "### üìå **Purpose & Relevance**\n",
                "\n",
                "In many real-world and game environments, **you‚Äôre not alone**. Whether you're a robot in a warehouse, a player in a multiplayer game, or a car in traffic ‚Äî you're surrounded by **other agents making decisions too**.\n",
                "\n",
                "**Multi-agent RL** teaches agents to:\n",
                "- Cooperate ü§ù\n",
                "- Compete ü•ä\n",
                "- Coordinate and adapt ü§π\n",
                "\n",
                "> It‚Äôs like learning to play soccer, not just juggling a ball ‚Äî success depends on your team *and* your opponent.\n",
                "\n",
                "---\n",
                "\n",
                "### üîë **Key Terminology**\n",
                "\n",
                "| Term                        | Analogy / Explanation |\n",
                "|-----------------------------|------------------------|\n",
                "| **Multi-Agent Environment** | Like a game of tag ‚Äî everyone has their own goals. |\n",
                "| **Opponent Modeling**       | Predicting your opponent‚Äôs next move ‚Äî like chess grandmasters. |\n",
                "| **Self-Play**               | Playing against your past selves ‚Äî like shadowboxing with older versions of yourself. |\n",
                "| **Cooperative vs Competitive** | Allies vs enemies ‚Äî or sometimes both. |\n",
                "| **Non-Stationarity**        | Environment keeps changing because other agents are also learning. üéØ |\n",
                "\n",
                "---\n",
                "\n",
                "### üìà **Use Cases**\n",
                "\n",
                "- **Competitive games**: Dota 2, StarCraft II, Hide and Seek, RoboCup\n",
                "- **Team-based robotics**: Drones swarming together\n",
                "- **Auction bidding agents**\n",
                "- **Traffic negotiation**: Multiple cars merging or negotiating intersections\n",
                "\n",
                "---\n",
                "\n",
                "## üßÆ **2. Mathematical Deep Dive**\n",
                "\n",
                "### üìê **Core Equations**\n",
                "\n",
                "We use **Multi-Agent MDPs** or **Markov Games**, where:\n",
                "\n",
                "Each agent \\( i \\) has its own:\n",
                "- **Policy** \\( \\pi^i(a^i | s) \\)\n",
                "- **Reward** \\( r^i_t \\)\n",
                "\n",
                "Joint state update:\n",
                "\n",
                "$$\n",
                "P(s_{t+1} \\mid s_t, a^1_t, a^2_t, ..., a^n_t)\n",
                "$$\n",
                "\n",
                "Each agent maximizes:\n",
                "\n",
                "$$\n",
                "J^i(\\theta^i) = \\mathbb{E}_{\\pi^1, ..., \\pi^n} \\left[ \\sum_{t=0}^{T} \\gamma^t r^i_t \\right]\n",
                "$$\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è **Math Intuition**\n",
                "\n",
                "This is chess with dynamic rules. Other players change their strategy constantly, making your environment **non-stationary**.\n",
                "\n",
                "Your agent must learn to **adapt**, **predict**, and even **manipulate** the behavior of others.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è **Assumptions & Constraints**\n",
                "\n",
                "- **Training instability**: Other agents are changing during training = moving target.\n",
                "- **Exploding state space**: Number of combinations grows fast.\n",
                "- **Credit assignment**: Who helped score the goal in team settings?\n",
                "\n",
                "---\n",
                "\n",
                "## üîç **3. Critical Analysis**\n",
                "\n",
                "### ‚öñÔ∏è **Strengths vs Weaknesses**\n",
                "\n",
                "| Strengths                                   | Weaknesses                                   |\n",
                "|---------------------------------------------|-----------------------------------------------|\n",
                "| Models realistic, interactive environments  | Learning is non-stationary and unstable       |\n",
                "| Enables emergent strategy and teamwork      | Hard to scale to many agents                  |\n",
                "| Self-play avoids need for labeled data      | May overfit to self-play strategies           |\n",
                "\n",
                "---\n",
                "\n",
                "### üß≠ **Ethical Lens**\n",
                "\n",
                "- **Emergent aggression**: AI may learn to deceive, trap, or exploit others.\n",
                "- **Manipulative behavior**: Agents might develop psychological strategies (see: Hide & Seek emergent cheating).\n",
                "- **Fairness in AI training**: Are all agents given equal representation and opportunity?\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ **Research Highlights (Post-2020)**\n",
                "\n",
                "- **OpenAI Five**: RL agents learned long-term coordination and communication in Dota 2.\n",
                "- **Hide-and-Seek (OpenAI)**: Agents discovered tools, built forts, and even invented hiding strategies.\n",
                "- **RoboCup 3v3 + Dec-POMDPs**: Learning decentralized policies under partial observability.\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ **4. Interactive Elements**\n",
                "\n",
                "### ‚úÖ **Concept Check**\n",
                "\n",
                "**Q: Why is training in multi-agent environments more difficult than single-agent RL?**\n",
                "\n",
                "A. Rewards are higher  \n",
                "B. The environment becomes non-stationary  \n",
                "C. It uses simpler algorithms  \n",
                "D. All agents share the same goal\n",
                "\n",
                "‚úÖ **Answer**: **B** ‚Äî Because other agents are also learning, the environment keeps changing!\n",
                "\n",
                "---\n",
                "\n",
                "### üß© **Code Challenge: Self-Play RL Simulation**\n",
                "\n",
                "```python\n",
                "# Basic Q-table for self-play\n",
                "q_table = {}\n",
                "\n",
                "def choose_action(state, q_table, epsilon=0.1):\n",
                "    if state not in q_table:\n",
                "        q_table[state] = np.zeros(2)\n",
                "    if np.random.rand() < epsilon:\n",
                "        return np.random.choice([0, 1])\n",
                "    return np.argmax(q_table[state])\n",
                "\n",
                "def update_q(state, action, reward, next_state, q_table, alpha=0.1, gamma=0.99):\n",
                "    if next_state not in q_table:\n",
                "        q_table[next_state] = np.zeros(2)\n",
                "    q_table[state][action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state][action])\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## üìö **5. Glossary**\n",
                "\n",
                "| Term                 | Explanation |\n",
                "|----------------------|-------------|\n",
                "| **Multi-Agent RL**    | Learning in environments with multiple learners or actors. |\n",
                "| **Self-Play**         | Training by playing against past versions of yourself. |\n",
                "| **Opponent Modeling** | Predicting or learning other agents‚Äô policies. |\n",
                "| **Non-Stationarity**  | Environment changes because other agents also learn. |\n",
                "| **Decentralized Policy** | Each agent makes decisions without global knowledge. |\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è **6. Practical Considerations**\n",
                "\n",
                "### üß™ **Hyperparameters**\n",
                "\n",
                "- **Exploration decay**: Critical to avoid getting stuck in cycles\n",
                "- **Opponent pool size**: Store old versions of agents to avoid forgetting\n",
                "- **Team reward sharing**: Careful tuning of shared vs. individual rewards\n",
                "\n",
                "---\n",
                "\n",
                "### üìè **Evaluation Metrics**\n",
                "\n",
                "```python\n",
                "# Win rate over diverse opponents\n",
                "def evaluate_self_play(agent, past_agents):\n",
                "    return np.mean([agent.play(opponent) for opponent in past_agents])\n",
                "```\n",
                "\n",
                "- **Win rate vs. past selves**\n",
                "- **Behavior diversity**\n",
                "- **Coordination metrics** (e.g., goal assists, pass frequency)\n",
                "- **Adaptation to novel opponents**\n",
                "\n",
                "---\n",
                "\n",
                "### üöÄ **Production Tips**\n",
                "\n",
                "- **Curriculum learning**: Start with simpler bots, then increase difficulty.\n",
                "- **Policy distillation**: Merge multiple good strategies into one.\n",
                "- **Ensemble learning**: Use multiple agent copies to encourage robustness.\n",
                "\n",
                "---\n",
                "\n",
                "## üêç **7. Full Python Code Walkthrough**\n",
                "\n",
                "```python\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Two-player self-play Q-learning in a toy game\n",
                "states = [0, 1, 2]\n",
                "actions = [0, 1]  # 0 = defend, 1 = attack\n",
                "q1, q2 = {}, {}\n",
                "\n",
                "def update_q(q, state, action, reward, next_state, alpha=0.1, gamma=0.95):\n",
                "    if state not in q:\n",
                "        q[state] = np.zeros(len(actions))\n",
                "    if next_state not in q:\n",
                "        q[next_state] = np.zeros(len(actions))\n",
                "    q[state][action] += alpha * (reward + gamma * np.max(q[next_state]) - q[state][action])\n",
                "\n",
                "for episode in range(5000):\n",
                "    state = np.random.choice(states)\n",
                "    a1 = np.random.choice(actions) if np.random.rand() < 0.2 else np.argmax(q1.get(state, [0, 0]))\n",
                "    a2 = np.random.choice(actions) if np.random.rand() < 0.2 else np.argmax(q2.get(state, [0, 0]))\n",
                "\n",
                "    reward1 = 1 if a1 == 1 and a2 == 0 else -1\n",
                "    reward2 = -reward1\n",
                "\n",
                "    next_state = np.random.choice(states)\n",
                "    update_q(q1, state, a1, reward1, next_state)\n",
                "    update_q(q2, state, a2, reward2, next_state)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "Want to continue with a breakdown of **AlphaZero/AlphaStar** next? Or interested in **cooperative multi-agent RL** for real-world swarm robotics or team games?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Absolutely ‚Äî let‚Äôs tackle **DeepMind‚Äôs AlphaZero** ‚Äî the game-changing algorithm that reshaped how we think about game-playing AI. We‚Äôll follow your full structured framework:  \n",
                "- Feynman technique  \n",
                "- Analogies  \n",
                "- 80% theory, 20% practice  \n",
                "- And then build a **real lab** (code + walkthrough) at the end.  \n",
                "\n",
                "---\n",
                "\n",
                "## üß† **1. Conceptual Foundation**\n",
                "\n",
                "### üìå **Purpose & Relevance**\n",
                "\n",
                "Before AlphaZero, game AIs relied on **hand-coded strategies, search trees**, or data from expert humans. But AlphaZero said:\n",
                "\n",
                "> ‚ÄúWhat if we teach an AI to become a master from scratch ‚Äî just by playing against itself?‚Äù\n",
                "\n",
                "And it **worked** ‚Äî AlphaZero beat world-champion engines in:\n",
                "- **Chess** ‚ôüÔ∏è  \n",
                "- **Go** ‚ö´‚ö™  \n",
                "- **Shogi** üèØ  \n",
                "\n",
                "All **with the same algorithm**, without being told the rules beyond legal moves.\n",
                "\n",
                "> **Analogy**: Like locking a genius in a room with just the rules of chess ‚Äî no coaching ‚Äî and it invents openings never seen before.\n",
                "\n",
                "---\n",
                "\n",
                "### üîë **Key Terminology**\n",
                "\n",
                "| Term                    | Analogy / Example |\n",
                "|-------------------------|-------------------|\n",
                "| **Self-play**           | Playing chess against your past self ‚Äî getting smarter each round. |\n",
                "| **Policy Network**      | Brain that decides *what to do* in a given state. |\n",
                "| **Value Network**       | Brain that evaluates *how good* the current position is. |\n",
                "| **Monte Carlo Tree Search (MCTS)** | Like simulating a bunch of ‚Äúwhat-if‚Äù games from the current position. |\n",
                "| **Tabula Rasa Learning**| Starting from scratch, no human data, no bias ‚Äî blank slate. |\n",
                "\n",
                "---\n",
                "\n",
                "### üìà **Use Cases**\n",
                "\n",
                "- Mastering strategic games: Chess, Go, Shogi\n",
                "- Game AI for training human players\n",
                "- Inspiration for RL in **robotics**, **navigation**, and **planning**\n",
                "- Foundation for **MuZero** ‚Äî which works without a model of the environment!\n",
                "\n",
                "---\n",
                "\n",
                "## üßÆ **2. Mathematical Deep Dive**\n",
                "\n",
                "### üìê **Core Equations**\n",
                "\n",
                "AlphaZero uses **policy + value networks** to guide MCTS.\n",
                "\n",
                "#### Policy and Value Estimation\n",
                "\n",
                "Given a state \\( s \\), the network predicts:\n",
                "- \\( p = \\pi(s) \\): probability distribution over actions\n",
                "- \\( v = V(s) \\): expected outcome (win/loss/draw)\n",
                "\n",
                "#### Loss function (combined):\n",
                "\n",
                "$$\n",
                "\\mathcal{L} = (z - v)^2 - \\pi^T \\log p + c||\\theta||^2\n",
                "$$\n",
                "\n",
                "Where:\n",
                "- \\( z \\): actual game result\n",
                "- \\( v \\): predicted value\n",
                "- \\( \\pi \\): improved MCTS policy\n",
                "- \\( p \\): raw network policy\n",
                "- \\( \\theta \\): weights\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è **Math Intuition**\n",
                "\n",
                "- **Policy loss**: Make the network favor moves that MCTS found promising.\n",
                "- **Value loss**: Predict game outcomes accurately.\n",
                "- **Regularization**: Keep the network weights from growing too wild.\n",
                "\n",
                "**MCTS** is the real magic sauce:\n",
                "- Simulate from the current position\n",
                "- Select actions based on win rate + exploration\n",
                "- Use NN predictions to guide the tree\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è **Assumptions & Constraints**\n",
                "\n",
                "- Turn-based, perfect-information games\n",
                "- Expensive compute: needs TPUs/GPUs for large-scale self-play\n",
                "- Needs thousands of self-play games to generalize well\n",
                "\n",
                "---\n",
                "\n",
                "## üîç **3. Critical Analysis**\n",
                "\n",
                "### ‚öñÔ∏è **Strengths vs Weaknesses**\n",
                "\n",
                "| Strengths                                  | Weaknesses                                |\n",
                "|--------------------------------------------|--------------------------------------------|\n",
                "| No expert data required (tabula rasa)      | Needs a LOT of self-play and compute       |\n",
                "| Generalizes across games                   | Doesn‚Äôt transfer easily to real-time games |\n",
                "| Outperforms handcrafted heuristics         | MCTS isn‚Äôt suited for high-latency domains |\n",
                "\n",
                "---\n",
                "\n",
                "### üß≠ **Ethical Lens**\n",
                "\n",
                "- **AI creativity**: AlphaZero showed **non-human strategies** ‚Äî but can we trust it in safety-critical tasks?\n",
                "- **Opaque learning**: Learned policies are hard to interpret.\n",
                "- **Fairness in competition**: In games vs. humans, do we need transparency of how it thinks?\n",
                "\n",
                "---\n",
                "\n",
                "### üî¨ **Research Highlights (Post-2020)**\n",
                "\n",
                "- **MuZero** (2020): AlphaZero without a model of the environment ‚Äî learns both dynamics and value.\n",
                "- **EfficientZero** (2022): Improved sample efficiency and faster learning.\n",
                "- **DreamerV3**: Model-based RL that mimics AlphaZero‚Äôs long-horizon planning but in continuous environments.\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ **4. Interactive Elements**\n",
                "\n",
                "### ‚úÖ **Concept Check**\n",
                "\n",
                "**Q: What makes AlphaZero ‚Äúgeneral-purpose‚Äù?**\n",
                "\n",
                "A. It uses pre-programmed openings  \n",
                "B. It doesn‚Äôt use any search  \n",
                "C. It can learn any game from scratch with only the rules  \n",
                "D. It requires human expert guidance\n",
                "\n",
                "‚úÖ **Answer**: **C** ‚Äì AlphaZero learns entirely via self-play and generalizes across different games.\n",
                "\n",
                "---\n",
                "\n",
                "### üß© **Code Challenge (Conceptual)**\n",
                "\n",
                "```python\n",
                "# BAD: Random move selection\n",
                "def random_move(board):\n",
                "    return np.random.choice(board.legal_moves)\n",
                "\n",
                "# BETTER: NN-guided policy + MCTS stub\n",
                "def policy_value_network(state):\n",
                "    # Placeholder for NN predictions\n",
                "    probs = np.ones(len(state.legal_moves)) / len(state.legal_moves)\n",
                "    value = 0.0\n",
                "    return probs, value\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## üìö **5. Glossary**\n",
                "\n",
                "| Term             | Explanation |\n",
                "|------------------|-------------|\n",
                "| **Policy Network** | Predicts which moves to consider. |\n",
                "| **Value Network**  | Predicts game outcome from current board state. |\n",
                "| **MCTS**           | Simulates future play to evaluate moves. |\n",
                "| **Self-play**      | Agent improves by playing against past versions of itself. |\n",
                "| **Tabula Rasa**    | Learning from scratch without external guidance. |\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è **6. Practical Considerations**\n",
                "\n",
                "### üß™ **Hyperparameters**\n",
                "\n",
                "- **MCTS simulations**: ~800 per move\n",
                "- **Exploration constant (UCT)**: tunes exploration/exploitation\n",
                "- **Dirichlet noise**: encourages diversity in early game\n",
                "\n",
                "---\n",
                "\n",
                "### üìè **Evaluation Metrics**\n",
                "\n",
                "- **Elo rating** vs. baseline engines\n",
                "- **Win/loss rate** vs. human experts\n",
                "- **Novelty** of moves (non-human strategies)\n",
                "- **Learning curve** (games vs performance)\n",
                "\n",
                "---\n",
                "\n",
                "### üöÄ **Production Tips**\n",
                "\n",
                "- Use **GPU batching** for NN inference\n",
                "- Keep a **pool of past policies** for diversity\n",
                "- Mix in **early resigns** to speed up training\n",
                "- Visualize MCTS trees for interpretability\n",
                "\n",
                "---\n",
                "\n",
                "## üêç **7. Real Lab: Build a Mini AlphaZero for Tic-Tac-Toe**\n",
                "\n",
                "This lab won‚Äôt be full AlphaZero, but a **simplified AlphaZero-style MCTS + value net** for a small environment like **Tic-Tac-Toe**.\n",
                "\n",
                "---\n",
                "\n",
                "### ‚úÖ Step-by-Step Overview:\n",
                "1. Build a Tic-Tac-Toe environment  \n",
                "2. Create a dummy neural network (returns uniform policy + value)  \n",
                "3. Implement MCTS using that network  \n",
                "4. Train using self-play  \n",
                "5. Evaluate performance over time\n",
                "\n",
                "Would you like me to generate the **full lab implementation in code** for this right now?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "import numpy as np\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "from collections import defaultdict\n",
                "\n",
                "# Simple Tic-Tac-Toe environment\n",
                "class TicTacToe:\n",
                "    def __init__(self):\n",
                "        self.reset()\n",
                "\n",
                "    def reset(self):\n",
                "        self.board = np.zeros((3, 3), dtype=int)  # 0: empty, 1: player 1, -1: player 2\n",
                "        self.current_player = 1\n",
                "        self.done = False\n",
                "        self.winner = None\n",
                "        return self.get_state()\n",
                "\n",
                "    def get_state(self):\n",
                "        return tuple(self.board.flatten())\n",
                "\n",
                "    def legal_moves(self):\n",
                "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
                "\n",
                "    def play(self, move):\n",
                "        if self.done:\n",
                "            return self.get_state(), 0, True\n",
                "\n",
                "        i, j = move\n",
                "        if self.board[i, j] != 0:\n",
                "            return self.get_state(), -10, True  # Illegal move penalty\n",
                "\n",
                "        self.board[i, j] = self.current_player\n",
                "        if self.check_winner(self.current_player):\n",
                "            self.done = True\n",
                "            self.winner = self.current_player\n",
                "            return self.get_state(), 1, True  # Win reward\n",
                "        elif len(self.legal_moves()) == 0:\n",
                "            self.done = True\n",
                "            return self.get_state(), 0.5, True  # Draw reward\n",
                "\n",
                "        self.current_player *= -1\n",
                "        return self.get_state(), 0, False\n",
                "\n",
                "    def check_winner(self, player):\n",
                "        for i in range(3):\n",
                "            if all(self.board[i, :] == player): return True\n",
                "            if all(self.board[:, i] == player): return True\n",
                "        if all(np.diag(self.board) == player): return True\n",
                "        if all(np.diag(np.fliplr(self.board)) == player): return True\n",
                "        return False\n",
                "\n",
                "# Dummy neural network prediction\n",
                "def dummy_policy_value_fn(state):\n",
                "    legal_moves = [(i, j) for i in range(3) for j in range(3) if state[i * 3 + j] == 0]\n",
                "    prob = 1 / len(legal_moves)\n",
                "    policy = {move: prob for move in legal_moves}\n",
                "    value = 0  # Dummy value\n",
                "    return policy, value\n",
                "\n",
                "# Basic MCTS with dummy network\n",
                "class MCTSNode:\n",
                "    def __init__(self, state, parent=None):\n",
                "        self.state = state\n",
                "        self.parent = parent\n",
                "        self.children = {}\n",
                "        self.visit_count = 0\n",
                "        self.total_value = 0.0\n",
                "\n",
                "    def expand(self, env):\n",
                "        policy, _ = dummy_policy_value_fn(env.get_state())\n",
                "        for move in policy:\n",
                "            new_env = TicTacToe()\n",
                "            new_env.board = np.array(env.board)\n",
                "            new_env.current_player = env.current_player\n",
                "            new_env.play(move)\n",
                "            self.children[move] = MCTSNode(new_env.get_state(), self)\n",
                "\n",
                "    def best_move(self):\n",
                "        return max(self.children.items(), key=lambda x: x[1].visit_count)[0]\n",
                "\n",
                "def run_game_with_mcts():\n",
                "    env = TicTacToe()\n",
                "    states = []\n",
                "    rewards = []\n",
                "    done = False\n",
                "\n",
                "    while not done:\n",
                "        root = MCTSNode(env.get_state())\n",
                "        root.expand(env)\n",
                "        move = root.best_move()\n",
                "        state, reward, done = env.play(move)\n",
                "        states.append(state)\n",
                "        rewards.append(reward)\n",
                "\n",
                "    return rewards[-1]  # Return final game outcome\n",
                "\n",
                "# Run multiple games and collect results\n",
                "results = [run_game_with_mcts() for _ in range(100)]\n",
                "win_rate = results.count(1) / len(results)\n",
                "draw_rate = results.count(0.5) / len(results)\n",
                "loss_rate = results.count(-1) / len(results)\n",
                "\n",
                "# Plotting the outcome distribution\n",
                "labels = ['Win', 'Draw', 'Loss']\n",
                "values = [win_rate, draw_rate, loss_rate]\n",
                "plt.bar(labels, values)\n",
                "plt.title(\"AlphaZero-like MCTS Agent vs Itself (Tic-Tac-Toe)\")\n",
                "plt.ylabel(\"Proportion\")\n",
                "plt.grid(True)\n",
                "plt.show()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
