{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc1c023",
   "metadata": {},
   "source": [
    "🔥 Now it’s time to stress-test that model like a warzone simulation.\n",
    "\n",
    "You’ve tracked metrics. You’ve set alerts.  \n",
    "But can your model handle **concurrent users hammering the endpoint**?  \n",
    "Let’s unleash a **swarm of virtual users** and find the **breaking point**.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `11_lab_concurrent_traffic_with_locust.ipynb`  \n",
    "### 📁 `06_mlops/05_model_monitoring`  \n",
    "> Use **Locust** to simulate **multiple concurrent requests** to your model server.  \n",
    "Measure **throughput, failure rates, latency under load**, and pinpoint system bottlenecks.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Goals\n",
    "\n",
    "- Use **Locust** to simulate load on `/predict` endpoint  \n",
    "- Monitor model server under **stress conditions**  \n",
    "- Track how **latency, QPS, and error rates change**  \n",
    "- Practice **load testing ML inference APIs**\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Runtime Setup\n",
    "\n",
    "| Component         | Spec             |\n",
    "|-------------------|------------------|\n",
    "| Server            | Flask (running model API) ✅  \n",
    "| Load Generator    | Locust ✅  \n",
    "| Metrics           | Latency, QPS, Failures ✅  \n",
    "| Deployment        | Localhost ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Section 1: Install Locust\n",
    "\n",
    "```bash\n",
    "!pip install locust\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐜 Section 2: Define Locust Load Test File (`locustfile.py`)\n",
    "\n",
    "```python\n",
    "from locust import HttpUser, task, between\n",
    "\n",
    "class ModelUser(HttpUser):\n",
    "    wait_time = between(0.5, 1)\n",
    "\n",
    "    @task\n",
    "    def predict(self):\n",
    "        with open(\"digit_sample.png\", \"rb\") as img:\n",
    "            self.client.post(\"/predict\", files={\"file\": img})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Section 3: Start Locust Dashboard\n",
    "\n",
    "```bash\n",
    "# In terminal\n",
    "locust -f locustfile.py --host=http://localhost:5000\n",
    "```\n",
    "\n",
    "Then visit:\n",
    "```\n",
    "http://localhost:8089\n",
    "```\n",
    "\n",
    "Set:\n",
    "- Users: 100  \n",
    "- Spawn rate: 10/sec  \n",
    "\n",
    "And click ➤ Start Swarming\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Section 4: Monitor Metrics Live\n",
    "\n",
    "- Requests/sec (QPS)  \n",
    "- Median / 95% latency  \n",
    "- Failures per second  \n",
    "- CPU/memory usage (via htop or Prometheus)\n",
    "\n",
    "You’ll likely see:\n",
    "| Load  | Latency | Error Rate |\n",
    "|-------|---------|------------|\n",
    "| 10 RPS | Low    | 0%         |\n",
    "| 100+ RPS | High | ⚠️ Timeouts |\n",
    "\n",
    "---\n",
    "\n",
    "## 💥 Section 5: Analyze Bottlenecks\n",
    "\n",
    "- Flask’s GIL?\n",
    "- Model warm-up delays?\n",
    "- Upload/Decode time for images?\n",
    "\n",
    "Add `@latency_hist.time()` in Flask to correlate latency spikes.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Lab Summary\n",
    "\n",
    "| Feature                              | ✅ |\n",
    "|--------------------------------------|----|\n",
    "| Locust simulated user traffic        | ✅ |\n",
    "| Server hit with live concurrent load | ✅ |\n",
    "| Bottlenecks exposed under pressure   | ✅ |\n",
    "| Prometheus ready for correlation     | ✅ |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- Load testing is **crucial before production**  \n",
    "- Locust = user simulator that hits hard and fast  \n",
    "- Bottlenecks usually lie in **I/O, batching, Flask limits**  \n",
    "- You’re now **load-aware**, not just model-aware\n",
    "\n",
    "---\n",
    "\n",
    "Next up:  \n",
    "> `12_lab_grafana_dashboard_for_live_model_metrics.ipynb`  \n",
    "Let’s take **all the metrics we’ve collected** and turn them into a **beautiful live Grafana dashboard**.  \n",
    "Latency, drift, accuracy — one screen to rule them all.\n",
    "\n",
    "Wanna light up that dashboard, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
