{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4837e6a",
   "metadata": {},
   "source": [
    "**Boom. Let's break the augmentation myth open.**  \n",
    "Not just what augmentations *are*, but which ones actually help ‚Äî and how much.\n",
    "\n",
    "---\n",
    "\n",
    "# üß™ `08_lab_data_augmentation_comparison.ipynb`  \n",
    "### üìÅ `02_computer_vision`  \n",
    "> Try common image augmentations (flip, rotate, crop, cutout, mixup)  \n",
    "> Compare their **impact on accuracy**, overfitting, and training curves.\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Setup Targets\n",
    "\n",
    "| Spec                    | Design    |\n",
    "|-------------------------|-----------|\n",
    "| Platform                | ‚úÖ Colab / CPU / T4 GPU  \n",
    "| Dataset                 | ‚úÖ CIFAR-10 (small & fast)  \n",
    "| Epochs                  | üîÅ 5‚Äì10 max (quick comparisons)  \n",
    "| Model                   | ‚úÖ Tiny CNN / ResNet18  \n",
    "| Augmentations           | üîÑ TorchVision `transforms`  \n",
    "| Visualization           | ‚úÖ Matplotlib  \n",
    "\n",
    "---\n",
    "\n",
    "## üîß Section 1: Imports & Setup\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è Section 2: Define Augmentation Variants\n",
    "\n",
    "```python\n",
    "basic = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "flip = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "crop = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "cutout = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=1.0, scale=(0.1, 0.2))\n",
    "])\n",
    "\n",
    "all_combo = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5)\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Section 3: Load CIFAR with Different Transforms\n",
    "\n",
    "```python\n",
    "def get_loader(transform):\n",
    "    dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                           download=True, transform=transform)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Section 4: Define a Simple CNN\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üèÉ‚Äç‚ôÇÔ∏è Section 5: Train Function\n",
    "\n",
    "```python\n",
    "def train_model(trainloader):\n",
    "    model = TinyCNN()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(5):\n",
    "        total_loss = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        losses.append(total_loss)\n",
    "    return losses\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Section 6: Compare Augmentations\n",
    "\n",
    "```python\n",
    "transform_sets = {\n",
    "    \"basic\": basic,\n",
    "    \"flip\": flip,\n",
    "    \"crop\": crop,\n",
    "    \"cutout\": cutout,\n",
    "    \"combo\": all_combo\n",
    "}\n",
    "\n",
    "loss_results = {}\n",
    "\n",
    "for name, tf in transform_sets.items():\n",
    "    print(f\"Training with: {name}\")\n",
    "    loader = get_loader(tf)\n",
    "    loss_results[name] = train_model(loader)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Section 7: Plot Loss Curves\n",
    "\n",
    "```python\n",
    "for name, losses in loss_results.items():\n",
    "    plt.plot(losses, label=name)\n",
    "plt.title(\"Training Loss by Augmentation Strategy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Wrap-Up Takeaways\n",
    "\n",
    "| Augmentation | Why It Helps |\n",
    "|--------------|--------------|\n",
    "| Flip         | Avoid overfitting directionally-bias patterns  \n",
    "| Crop         | Makes model robust to spatial shifts  \n",
    "| Cutout       | Simulates occlusion, encourages abstraction  \n",
    "| Combo        | Most generalization power in one pass  \n",
    "\n",
    "---\n",
    "\n",
    "## üß† What You Learned\n",
    "\n",
    "- **Not all augmentations are equal** ‚Äî some help more than others  \n",
    "- **Combo augmentation** can significantly improve generalization  \n",
    "- You now know how to **test, track, and prove** what works\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Colab-Ready Checklist\n",
    "\n",
    "| ‚úÖ Feature                 | Status |\n",
    "|----------------------------|--------|\n",
    "| Low VRAM                  | ‚úÖ      |\n",
    "| Quick training            | ‚úÖ      |\n",
    "| Visualization of results  | ‚úÖ      |\n",
    "| Scalable to any transform | ‚úÖ      |\n",
    "\n",
    "---\n",
    "\n",
    "Ready to jump into the next lab?  \n",
    "`09_lab_finetune_resnet_on_custom_data.ipynb` is next ‚Äî we‚Äôll fine-tune a real ResNet on your own dataset or a subset of CIFAR/flowers. Let‚Äôs go?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
