{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2c0867",
   "metadata": {},
   "source": [
    "🧠🔁 Time to do some **neural soul transfer**.\n",
    "\n",
    "We're distilling knowledge like a sensei into a padawan —  \n",
    "Big model trains a little model by **sharing its output wisdom** instead of just the hard labels.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `09_lab_distill_teacher_student_on_mnist.ipynb`  \n",
    "### 📁 `05_model_optimization`  \n",
    "> Use **Knowledge Distillation** to teach a **small student model** from a **large teacher**.  \n",
    "Compare performance with and without distillation.  \n",
    "Track what’s lost, what’s gained — and how **soft labels change the game**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Goals\n",
    "\n",
    "- Understand **why distillation works**  \n",
    "- Implement distillation loss (KL Divergence + temperature)  \n",
    "- Train student with **teacher soft targets**  \n",
    "- Compare accuracy and generalization\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Runtime Targets\n",
    "\n",
    "| Component            | Spec                 |\n",
    "|----------------------|----------------------|\n",
    "| Dataset              | MNIST ✅  \n",
    "| Teacher              | Big CNN ✅  \n",
    "| Student              | Tiny CNN ✅  \n",
    "| Training Time        | < 3 min ✅  \n",
    "| Device               | CPU / Colab ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Section 1: Imports & Dataset\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "```python\n",
    "transform = transforms.ToTensor()\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 Section 2: Teacher & Student Models\n",
    "\n",
    "```python\n",
    "class Teacher(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Student(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Section 3: Train Teacher\n",
    "\n",
    "```python\n",
    "def train_model(model, loader, epochs=3):\n",
    "    model.train()\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in loader:\n",
    "            out = model(x)\n",
    "            loss = F.cross_entropy(out, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            pred = model(x).argmax(1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "teacher = Teacher()\n",
    "train_model(teacher, train_loader, epochs=5)\n",
    "print(\"Teacher Accuracy:\", evaluate(teacher, test_loader))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Section 4: Distillation Loss\n",
    "\n",
    "```python\n",
    "def distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.7):\n",
    "    soft_loss = F.kl_div(\n",
    "        F.log_softmax(student_logits / T, dim=1),\n",
    "        F.softmax(teacher_logits / T, dim=1),\n",
    "        reduction='batchmean'\n",
    "    ) * (T ** 2)\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "    return alpha * soft_loss + (1 - alpha) * hard_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Section 5: Train Student With Distillation\n",
    "\n",
    "```python\n",
    "student = Student()\n",
    "opt = torch.optim.Adam(student.parameters())\n",
    "T = 2.0  # Temperature\n",
    "alpha = 0.7\n",
    "\n",
    "for epoch in range(5):\n",
    "    student.train()\n",
    "    for x, y in train_loader:\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher(x)\n",
    "        student_logits = student(x)\n",
    "        loss = distillation_loss(student_logits, teacher_logits, y, T=T, alpha=alpha)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Section 6: Compare Accuracies\n",
    "\n",
    "```python\n",
    "acc_student = evaluate(student, test_loader)\n",
    "print(f\"Student Accuracy (Distilled): {acc_student:.4f}\")\n",
    "```\n",
    "\n",
    "Optional: Compare vs student trained **only** on hard labels.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Wrap-Up Summary\n",
    "\n",
    "| Concept                      | ✅ |\n",
    "|------------------------------|----|\n",
    "| Teacher → Student knowledge  | ✅ |\n",
    "| Distillation loss (KL + CE)  | ✅ |\n",
    "| Accuracy comparison          | ✅ |\n",
    "| Colab/laptop safe            | ✅ |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- **Distillation = compression with guidance**  \n",
    "- Teacher’s **soft outputs** contain more info than hard labels  \n",
    "- This technique powers **TinyBERT**, **DistilGPT**, **MobileNetV3**, and more  \n",
    "- You now wield **one of the most powerful tricks** in model optimization\n",
    "\n",
    "---\n",
    "\n",
    "⚡ Let’s shift gears:  \n",
    "Shall we head into **deployment territory** with `06_deployment_and_scaling`?  \n",
    "Next lab: `07_lab_export_pytorch_to_onnx_and_run.ipynb` —  \n",
    "turn your PyTorch model into a portable `.onnx` file and run inference anywhere."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
