{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a849927a",
   "metadata": {},
   "source": [
    "üõ†Ô∏è It‚Äôs time to go **full engineer mode** ‚Äî your model is now a **web service.**  \n",
    "Let‚Äôs wrap it in Flask, containerize it with Docker, and serve it like a **real backend**.\n",
    "\n",
    "---\n",
    "\n",
    "# üß™ `08_lab_dockerize_and_test_flask_model_server.ipynb`  \n",
    "### üìÅ `06_deployment_and_scaling`  \n",
    "> Create a **Flask API** for a PyTorch/ONNX model  \n",
    "> Package it into a **Docker container**  \n",
    "> Test it with live POST requests ‚Äî as if you‚Äôre building an **ML-powered microservice**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Goals\n",
    "\n",
    "- Build a **Flask app** that serves predictions  \n",
    "- Accept **image uploads via POST**  \n",
    "- Serve using **Docker containerization**  \n",
    "- Test using **curl or Python requests**  \n",
    "- Build awareness for **latency, batching, I/O handling**\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Runtime Design\n",
    "\n",
    "| Component      | Spec               |\n",
    "|----------------|--------------------|\n",
    "| Model          | `mlp_mnist.onnx` or PyTorch ‚úÖ  \n",
    "| API Server     | Flask ‚úÖ  \n",
    "| Container      | Dockerfile ‚úÖ  \n",
    "| Testing Tool   | requests / curl ‚úÖ  \n",
    "| Platform       | Local / Colab + ngrok (if needed) ‚úÖ  \n",
    "\n",
    "---\n",
    "\n",
    "## üß† Section 1: Flask App `app.py`\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "app = Flask(__name__)\n",
    "session = ort.InferenceSession(\"mlp_mnist.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.unsqueeze(0).numpy().astype(np.float32))\n",
    "])\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({\"error\": \"No image uploaded\"}), 400\n",
    "    img = Image.open(request.files['file']).convert(\"RGB\")\n",
    "    img_tensor = transform(img)\n",
    "    out = session.run(None, {input_name: img_tensor})\n",
    "    pred = int(np.argmax(out[0]))\n",
    "    return jsonify({\"prediction\": pred})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Section 2: Dockerfile\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.10\n",
    "\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "\n",
    "RUN pip install flask onnxruntime pillow torchvision\n",
    "\n",
    "EXPOSE 5000\n",
    "\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Section 3: Build & Run Docker\n",
    "\n",
    "### ‚úÖ Build\n",
    "```bash\n",
    "docker build -t mnist-flask-app .\n",
    "```\n",
    "\n",
    "### ‚úÖ Run\n",
    "```bash\n",
    "docker run -p 5000:5000 mnist-flask-app\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Section 4: Test the API\n",
    "\n",
    "### üì§ Test with Python\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "files = {'file': open('digit_sample.png', 'rb')}\n",
    "res = requests.post(\"http://localhost:5000/predict\", files=files)\n",
    "print(res.json())\n",
    "```\n",
    "\n",
    "Or use `curl`:\n",
    "```bash\n",
    "curl -X POST -F \"file=@digit_sample.png\" http://localhost:5000/predict\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Wrap-Up Summary\n",
    "\n",
    "| Feature                           | ‚úÖ |\n",
    "|------------------------------------|----|\n",
    "| Flask API from model              | ‚úÖ |\n",
    "| ONNX inference inside endpoint     | ‚úÖ |\n",
    "| Docker container builds cleanly    | ‚úÖ |\n",
    "| RESTful POST request support       | ‚úÖ |\n",
    "| Local + Colab-compatible (ngrok)   | ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What You Learned\n",
    "\n",
    "- Models ‚â† products ‚Äî this is how you **turn ML into APIs**  \n",
    "- Docker ensures **portability and reproducibility**  \n",
    "- You now know how to **ship** an inference pipeline to production  \n",
    "\n",
    "---\n",
    "\n",
    "Wanna go beast mode and hit `09_lab_k8s_microservice_mock_deploy.ipynb` next?  \n",
    "It‚Äôs time to throw this container into a **Kubernetes cluster**, mock a deployment, and simulate **real microservice orchestration**.  \n",
    "\n",
    "Ready, Captain?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
