{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f5516e",
   "metadata": {},
   "source": [
    "🔎📚 **Professor, this is the advanced dial on your RAG toolkit** — we’re not just retrieving by “what it means” anymore…  \n",
    "Now we retrieve based on **who said it, when they said it, and what section it belongs to**.\n",
    "\n",
    "Welcome to **hybrid retrieval** — combining **semantic search** + **metadata filtering**.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 `09_lab_metadata_filtering_in_retrieval.ipynb`  \n",
    "### 📁 `05_llm_engineering/03_rag_systems`  \n",
    "> Add metadata (source, date, tags) to each document chunk  \n",
    "→ Retrieve not just semantically relevant chunks, but **filter by tags**  \n",
    "→ Simulate how tools like **Perplexity AI**, **ChatPDF**, or **Notion AI** prioritize trust, recency, and context\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Goals\n",
    "\n",
    "- Understand how metadata helps improve relevance and traceability  \n",
    "- Attach structured fields to vector chunks  \n",
    "- Apply filters during retrieval (e.g. `\"source\": \"FAQ\"`, `\"author\": \"Einstein\"`)  \n",
    "- Use ChromaDB’s `where` clause to simulate **structured semantic filtering**\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Runtime Specs\n",
    "\n",
    "| Feature         | Spec                     |\n",
    "|------------------|--------------------------|\n",
    "| Vector Search    | ChromaDB ✅  \n",
    "| Metadata Support | Dict-style metadata ✅  \n",
    "| Embeddings       | SentenceTransformer ✅  \n",
    "| Platform         | Colab-ready ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Section 1: Setup and Chunk as Before\n",
    "\n",
    "```python\n",
    "!pip install chromadb sentence-transformers langchain\n",
    "```\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "loader = TextLoader(\"sample_docs.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=40)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🏷️ Section 2: Enrich with Metadata\n",
    "\n",
    "Let’s say each chunk came from a source document with:\n",
    "\n",
    "- Author\n",
    "- Date\n",
    "- Type (FAQ, article, report)\n",
    "\n",
    "```python\n",
    "import random\n",
    "sources = [\"faq\", \"article\", \"policy\"]\n",
    "authors = [\"Alice\", \"Bob\", \"Professor\"]\n",
    "\n",
    "texts = [c.page_content for c in chunks]\n",
    "metas = [\n",
    "    {\"type\": random.choice(sources), \"author\": random.choice(authors), \"index\": i}\n",
    "    for i in range(len(texts))\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Section 3: Store in ChromaDB with Metadata\n",
    "\n",
    "```python\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "client = chromadb.Client(Settings(allow_reset=True))\n",
    "collection = client.get_or_create_collection(\"hybrid_rag\")\n",
    "\n",
    "embeddings = embedder.encode(texts)\n",
    "\n",
    "for i, (text, emb, meta) in enumerate(zip(texts, embeddings, metas)):\n",
    "    collection.add(\n",
    "        ids=[f\"id_{i}\"],\n",
    "        documents=[text],\n",
    "        embeddings=[emb.tolist()],\n",
    "        metadatas=[meta]\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Section 4: Semantic + Metadata Query\n",
    "\n",
    "```python\n",
    "def hybrid_query(prompt, filters=None, k=3):\n",
    "    q_embed = embedder.encode(prompt).tolist()\n",
    "    return collection.query(\n",
    "        query_embeddings=[q_embed],\n",
    "        n_results=k,\n",
    "        where=filters or {}\n",
    "    )[\"documents\"][0]\n",
    "\n",
    "# Example: Only retrieve chunks by \"Alice\" from \"faq\" sources\n",
    "results = hybrid_query(\"What is the policy on returns?\", filters={\"author\": \"Alice\", \"type\": \"faq\"})\n",
    "\n",
    "for i, chunk in enumerate(results):\n",
    "    print(f\"\\nFiltered Chunk {i+1}:\\n{chunk}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Lab Wrap-Up\n",
    "\n",
    "| Feature                          | ✅ |\n",
    "|----------------------------------|----|\n",
    "| Added metadata to each document  | ✅  \n",
    "| Performed filtered semantic search | ✅  \n",
    "| Simulated hybrid structured+vector retrieval | ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What You Learned\n",
    "\n",
    "- Metadata boosts **retrieval control** — vital in legal, medical, or academic AI  \n",
    "- You can ask **\"Give me answers from Alice-written FAQs only\"**  \n",
    "- This is the foundation of **enterprise-grade RAG**  \n",
    "- Filters can be extended to **recency, reliability scores, labels**, and more\n",
    "\n",
    "---\n",
    "\n",
    "✅ That closes out the **RAG Lab Series**:\n",
    "- 🔲 Chunks  \n",
    "- 🧠 Embeds  \n",
    "- 📦 Vector DB  \n",
    "- 🧠💼 Metadata filters  \n",
    "\n",
    "Up next, we deploy this firepower 💣\n",
    "\n",
    "> 🚀 `07_lab_vllm_vs_tgi_latency_comparison.ipynb`  \n",
    "Benchmark and compare **vLLM** and **TGI** (Text Generation Inference)  \n",
    "→ Which one serves LLMs *faster, cheaper, stronger*?\n",
    "\n",
    "You ready to test LLM inference **like an ML systems engineer**?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
