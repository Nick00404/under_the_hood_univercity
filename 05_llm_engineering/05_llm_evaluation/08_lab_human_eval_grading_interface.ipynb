{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f2fd04",
   "metadata": {},
   "source": [
    "🧠 Let’s move into **Lab 08** — this one’s about **human evaluation**, the gold standard in LLM testing. We’ll build a clean UI for **rating generations**, plug in **GPT-4 as a reviewer**, and even prep for **crowdsourcing** if needed.\n",
    "\n",
    "---\n",
    "\n",
    "# 📒 `08_lab_human_eval_grading_interface.ipynb`  \n",
    "## 📁 `05_llm_engineering/05_llm_evaluation`\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Notebook Goals**\n",
    "\n",
    "- Create a **streamlit-style rating UI**\n",
    "- Let humans grade:\n",
    "  - Fluency ✅\n",
    "  - Factual accuracy ✅\n",
    "  - Relevance ✅\n",
    "- Bonus: Plug in GPT-4 to simulate **automated human eval**\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 1. Sample Dataset: References vs Generations\n",
    "\n",
    "```python\n",
    "samples = [\n",
    "    {\n",
    "        \"prompt\": \"What is photosynthesis?\",\n",
    "        \"reference\": \"Photosynthesis is the process by which green plants use sunlight to synthesize food.\",\n",
    "        \"generation\": \"Photosynthesis is how plants turn light into energy for survival.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Where is the Eiffel Tower?\",\n",
    "        \"reference\": \"The Eiffel Tower is in Paris, France.\",\n",
    "        \"generation\": \"The Eiffel Tower is located in Berlin.\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 2. Build Grading Form (Colab Friendly)\n",
    "\n",
    "```python\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import widgets\n",
    "\n",
    "def create_grading_interface(sample):\n",
    "    display(Markdown(f\"### 🧪 Prompt:\\n{sample['prompt']}\"))\n",
    "    display(Markdown(f\"**📖 Reference:** {sample['reference']}\"))\n",
    "    display(Markdown(f\"**🤖 LLM Output:** {sample['generation']}\"))\n",
    "\n",
    "    fluency = widgets.IntSlider(description=\"Fluency\", min=1, max=5, value=3)\n",
    "    factual = widgets.IntSlider(description=\"Factuality\", min=1, max=5, value=3)\n",
    "    relevance = widgets.IntSlider(description=\"Relevance\", min=1, max=5, value=3)\n",
    "    submit = widgets.Button(description=\"Submit Score\")\n",
    "\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def on_submit_clicked(_):\n",
    "        with output:\n",
    "            display(Markdown(\n",
    "                f\"✅ **Scores Submitted:** Fluency: {fluency.value}, \"\n",
    "                f\"Factuality: {factual.value}, Relevance: {relevance.value}\"\n",
    "            ))\n",
    "\n",
    "    submit.on_click(on_submit_clicked)\n",
    "\n",
    "    display(fluency, factual, relevance, submit, output)\n",
    "\n",
    "for sample in samples:\n",
    "    create_grading_interface(sample)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 3. Simulate GPT-4 as Human Evaluator\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "def gpt_evaluator(prompt, reference, generation):\n",
    "    system_prompt = \"You are a helpful evaluator who rates LLM outputs. Rate the fluency, factuality, and relevance on a scale from 1 to 5.\"\n",
    "    user_input = f\"\"\"\n",
    "    Prompt: {prompt}\n",
    "    Reference: {reference}\n",
    "    Output: {generation}\n",
    "    Rate each dimension (fluency, factuality, relevance) from 1 to 5.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "# Example\n",
    "#gpt_evaluator(**samples[0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ What You Built\n",
    "\n",
    "| Feature         | Role |\n",
    "|------------------|------|\n",
    "| Rating UI        | Collect human scores |\n",
    "| GPT-4 Auto Rater | Optional evaluator (for cost/time saving) |\n",
    "| Manual + Automated | ✅ Combined pipeline |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Wrap-Up\n",
    "\n",
    "| Task                          | ✅ |\n",
    "|-------------------------------|----|\n",
    "| Built LLM rating interface     | ✅ |\n",
    "| Collected fluency/factuality  | ✅ |\n",
    "| Plugged in GPT-4 eval optional| ✅ |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔮 Next Lab\n",
    "\n",
    "📒 `09_lab_bias_and_toxicity_metrics_demo.ipynb`  \n",
    "Let’s analyze **bias and toxicity** in LLM outputs — using open-source tools to detect stereotypes, slurs, or harmful completions.\n",
    "\n",
    "Ready to go ethical, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
